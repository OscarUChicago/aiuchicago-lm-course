{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Sbo6OdLLgch"
   },
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqc09zO-CZq7"
   },
   "source": [
    "Welcome! This notebook is a compilation of all of the code I prepared for the AI@UChicago's LM course. AI@UChicago's LM course is a combination of theory & applications. I wrote all of the relevant pieces of code listed below in Sections 4 & 5. Additionally, I prepared both the theory & applications for Section 6; as such, I list this section in its entirety below. Additionally, I collaborated with Arjun Sohur & Hung Le in order to prepare some of the theory in other portions of the notebook (mainly, Section 5); you can find our complete LM course textbook [here](https://github.com/AI-UChicago/Language-Models-Course).\n",
    "\n",
    "---\n",
    "\n",
    "*Note: Provided below is a list of all the resources which were utilized in the creation of this notebook. If you have any more specific questions concerning these citations, please consult Oscar Barnes.*\n",
    "\n",
    "*Resources:*\n",
    "\n",
    "$\\: \\: \\:$ [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n",
    "$\\: \\: \\:$ [UvA: Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "\n",
    "$\\: \\: \\:$ [\"HuggingFace NLP Course\"](https://huggingface.co/learn/nlp-course/chapter1/1)\n",
    "\n",
    "$\\: \\: \\:$ [\"Probabilistic Machine Learning: An Introduction\"](https://probml.github.io/pml-book/book1.html)\n",
    "\n",
    "$\\: \\: \\:$ [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCV7t1NtLUxI"
   },
   "source": [
    "# **Section 4 (Code Only)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LgtTYDJBwnu"
   },
   "source": [
    "## **Section 4.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vt_DNccPI6_"
   },
   "source": [
    "Below, I implement a Multi-Head Attention module (MHA) we will be using later in our complete Transformer. Beyond $MultiHeadAttention$, I also build out a $MaskingMechanism$ module which contains some important static masking methods. See comments and docstrings for additional context below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAl3nWznBtaa"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "import typing\n",
    "from typing import Union, Optional, Tuple\n",
    "\n",
    "\n",
    "\n",
    "# This class defines every method we will need for working with masking in our MHA block.\n",
    "class MaskingMechanism:\n",
    "  @staticmethod\n",
    "  def decoder_mask(size : int) -> torch.Tensor:\n",
    "    '''Mask for the decoder. Size refers to the token sequence length of the input to the decoder.'''\n",
    "    attention_shape = (1, size, size)\n",
    "    mask = torch.tril(torch.ones((1,size,size)), diagonal=0)\n",
    "    return mask\n",
    "\n",
    "  @staticmethod\n",
    "  def expand_mask(mask : torch.Tensor) -> torch.Tensor:\n",
    "    '''Expand mask to 4 dimensions. This method is modified to deal with both 2d and 3d masks.'''\n",
    "    assert mask.ndim >= 2\n",
    "    if mask.ndim == 3:  # Assuming we just need to add a 'num_heads' dim.\n",
    "      mask = mask.unsqueeze(1)\n",
    "    while mask.ndim < 4:\n",
    "      mask = mask.unsqueeze(0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "# This class is our MHA block.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embed_dim : int, num_heads : int, dropout : float = 0.1) -> None:\n",
    "    '''Initialization of the Multi-Head Attention module.'''\n",
    "    super().__init__()\n",
    "    assert embed_dim % num_heads == 0\n",
    "\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = embed_dim // num_heads\n",
    "\n",
    "    self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(embed_dim, embed_dim)) for _ in range(4)])\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    self._initialize_parameters()\n",
    "\n",
    "  def _initialize_parameters(self) -> None:\n",
    "    '''Initialize the weights within each of the linear layers.'''\n",
    "    for module in self.linears:\n",
    "      nn.init.xavier_uniform_(module.weight)\n",
    "      module.bias.data.fill_(0)\n",
    "\n",
    "  def _scaled_dot_product(self, query : torch.Tensor, key : torch.Tensor, value : torch.Tensor, dropout : Optional[nn.Dropout] = None, mask : Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''Define scaled dot product attention.'''\n",
    "    d_k = query.size(-1)\n",
    "    attn_logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "      attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    if dropout is not None:\n",
    "      attention = dropout(attention)\n",
    "    values = torch.matmul(attention, value)\n",
    "    return values, attention\n",
    "\n",
    "  def forward(self, query : torch.Tensor, key : torch.Tensor, value : torch.Tensor, return_attention : bool = False, mask : Optional[torch.Tensor] = None) -> Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "    '''Forward step of the Multi-Head Attention module.'''\n",
    "    nbatches = query.size(0)\n",
    "\n",
    "    if mask is not None:\n",
    "      mask = MaskingMechanism.expand_mask(mask)\n",
    "\n",
    "    query, key, value = [\n",
    "          lin(x).view(nbatches, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "          for lin, x in zip(self.linears, (query, key, value))\n",
    "      ]\n",
    "\n",
    "    values, attention = self._scaled_dot_product(query, key, value, dropout = self.dropout, mask = mask)\n",
    "    values = (values.transpose(1,2).contiguous().view(nbatches, -1, self.embed_dim))\n",
    "\n",
    "    del query, key, value\n",
    "\n",
    "    output = self.linears[-1](values)\n",
    "\n",
    "    if return_attention:\n",
    "      return output, attention\n",
    "    else:\n",
    "      return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in3pnsnQLZaz"
   },
   "source": [
    "# **Section 5 (Code Only)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "491aC1_xOZ65"
   },
   "source": [
    "## **Section 5.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx8QE37oP7Ae"
   },
   "source": [
    "In this section, we look at various elements of the Transformer -- positional encoding, layer normalization, feed-forward networks (FFNs) & the final Transformer output generator, and some additional components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7BidMs8B1-y"
   },
   "source": [
    "### **Section 5.2.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e892rWMQHzg"
   },
   "source": [
    "$PositionalEncoding$ will be used in our Transformer for positionally encoding the model's inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2b-mSk6BlYu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import typing\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model: int, dropout: float, max_len: int = 5000) -> None:\n",
    "    '''\n",
    "    Inputs:\n",
    "      d_model - Hidden dimensionality of the model.\n",
    "      dropout - Dropout probability.\n",
    "      max_len - Maximum length of a sequence to expect.\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0) # Unsqueeze the batch dimension -- now our pe matrix is of dimensionality: [BatchDim, SeqLen, HiddenDim]\n",
    "\n",
    "    # While not a parameter, we want our positional encoding to be part of the module's state_dict\n",
    "    # -- hence, we use the register_buffer method of our parent class. Our module's state_dict is\n",
    "    # simply a Python dictionary object which maps each layer to its parameter tensor.\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # Since our input sequence length may not be as long as max_len, we curtail our pe matrix.\n",
    "    x = x + self.pe[:, :x.size(1)]\n",
    "    return self.dropout(x)\n",
    "\n",
    "# Check our Module's state_dict\n",
    "positional_encoding = PositionalEncoding(500, 0.1) # random values\n",
    "print(\"\\n\\033[1mPositionalEncoding state_dict:\\033[0m\\n\", positional_encoding.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pXM13OdQaCA"
   },
   "source": [
    "Additionally, I've produced some plots below which highlight some of the properties of our positional encoding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXDPgIwUBq6m"
   },
   "outputs": [],
   "source": [
    "# Relevant plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('RdGy')\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "# For our positional encoding visualization, we will ignore the effects of dropout.\n",
    "encod_block = PositionalEncoding(d_model=48, dropout=0.0, max_len=96)\n",
    "# We remove the BatchDim from our positional encoding matrix before converting it\n",
    "# into a NumPy array.\n",
    "pe = encod_block.pe.squeeze().cpu().numpy()\n",
    "\n",
    "# Plotting our positional encodings\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,8))  # Adjusted figure size\n",
    "pos = ax.imshow(pe, cmap=\"RdGy\", aspect='auto')\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_ylabel(\"Position in sequence\")\n",
    "ax.set_xlabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_yticks(range(0, 96, 10))\n",
    "ax.set_xticks(range(0, 48, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8YVHzyHB8lF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming we already have our positional encoding matrix 'pe'\n",
    "# If not, we would need to generate it using the PositionalEncoding class\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set_theme()\n",
    "\n",
    "# Create a 2x2 grid of subplots\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "# Flatten the 2x2 grid into a 1D array for easier iteration\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Colors for each subplot\n",
    "colors = ['C0', 'C1', 'C2', 'C3']\n",
    "\n",
    "for i in range(4):\n",
    "    # Plot the encoding for the i-th dimension across the first 16 sequence positions\n",
    "    ax[i].plot(np.arange(16), pe[:16, i], color=colors[i], marker=\"o\",\n",
    "               markersize=6, markeredgecolor=\"black\")\n",
    "    ax[i].set_title(f\"Encoding in hidden dimension {i}\")\n",
    "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
    "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
    "    ax[i].set_xticks(np.arange(0, 16))\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
    "    ax[i].set_ylim(-1.2, 1.2)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1sck3GSB_i7"
   },
   "source": [
    "### **Section 5.2.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMDShyuEQmbV"
   },
   "source": [
    "$LayerNorm$ will be used to apply layer normalization to our Transformer input as it passes through the various sub-layers & layers of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBwD-V9ICCm7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import typing\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, feature_size: tuple, eps: float = 1e-6, initialize_parameters : bool = True) -> None:\n",
    "    '''\n",
    "    Inputs:\n",
    "      feature_size - Size of tensor passed to the forward method.\n",
    "      eps - Small value to prevent division by zero.  Default is 1e-6.\n",
    "    '''\n",
    "    super().__init__()\n",
    "    # We use nn.Parameter() to introduce trainable params α and β into our layer normalization\n",
    "    self.alpha = nn.Parameter(torch.ones(feature_size))\n",
    "    self.beta = nn.Parameter(torch.zeros(feature_size))\n",
    "    self.eps = eps\n",
    "\n",
    "    if initialize_parameters:\n",
    "      self._initialize_parameters()\n",
    "\n",
    "  def _initialize_parameters(self) -> None:\n",
    "    nn.init.xavier_uniform_(self.alpha)\n",
    "    nn.init.xavier_uniform_(self.beta)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # In Layer Normalization, we want to normalize over the HiddenDim dimension\n",
    "    mean = x.mean(-1, keepdim=True)\n",
    "    std = x.std(-1, keepdim=True)\n",
    "    return self.alpha * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmdlbhg_Qvrg"
   },
   "source": [
    "Below, we look at the state_dict associated with our $LayerNorm$ module before using a simple example to demonstrate the baseline output of layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCKJdACaCG6z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"First, let's look at the state_dict for our LayerNorm module.\\n\")\n",
    "x = torch.randn(1,50,512)\n",
    "layer_norm = LayerNorm(feature_size=tuple(x.size()), initialize_parameters = False)\n",
    "print(layer_norm.state_dict())\n",
    "\n",
    "print(\"\\nNow, let's send in an example tensor to see how Layer Normalization works.\\n\")\n",
    "\n",
    "print(\"\\033[1mTensor (Before Normalization):\\033[0m\\n\", x)\n",
    "x = layer_norm.forward(x)\n",
    "print(\"\\n\\033[1mTensor (After Normalization):\\033[0m\\n\", x.data)\n",
    "\n",
    "x = x.data.squeeze(0)\n",
    "x_mean, x_std = [], []\n",
    "for index in range(x.size(0)):\n",
    "  x_mean.append(x[index,:].mean().tolist())\n",
    "  x_std.append(x[index,:].std().tolist())\n",
    "\n",
    "print(\"\\nAs seen across each row in our tensor, we've normalized each layer to have a mean of approximately 0 and a standard deviation of approximately 1.\")\n",
    "print(\"\\n\\033[1mAverage Mean Across Sequence Entries:\\033[0m\", sum(x_mean) / len(x_mean))\n",
    "print(\"\\n\\033[1mAverage Standard Deviation Across Sequence Entries:\\033[0m\", sum(x_std) / len(x_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnYBBj5eCJ0x"
   },
   "source": [
    "### **Section 5.2.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtHjF5MZcSoU"
   },
   "source": [
    "Here, we build out the FFN portion of our Transformer architecture using the $FeedForwardNetwork$ class. Note that we use Kaiming initialization for the weights of our FFN. Additionally, we do a brief test of our FFN at the end of the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vf6WM9q-CLLM"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import typing\n",
    "from types import SimpleNamespace\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "  def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1) -> None:\n",
    "    '''\n",
    "    Inputs:\n",
    "      d_model - Hidden dimensionality of the input (& output!) of the FFN.\n",
    "      d_ff - Hidden dimensionality of the FFN itself.\n",
    "      dropout - Dropout probability.\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.test = nn.Linear(d_model, d_ff)\n",
    "    self.modules = SimpleNamespace(w_1 = nn.Linear(d_model, d_ff),\n",
    "                                   w_2 = nn.Linear(d_ff, d_model),\n",
    "                                   dropout = nn.Dropout(dropout))\n",
    "    self._init_params()\n",
    "\n",
    "  # For ReLU-based linear neural networks, it is recommended to use Kaiming initialization (either\n",
    "  # nn.init.kaiming_normal_ or nn.init.kaiming_uniform_) for the weights.\n",
    "  def _init_params(self) -> None:\n",
    "    for module in self.modules.__dict__:\n",
    "      if isinstance(self.modules.__dict__[module], nn.Linear):\n",
    "        nn.init.kaiming_normal_(self.modules.__dict__[module].weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.modules.__dict__[module].bias)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.modules.w_2(self.modules.dropout(self.modules.w_1(x).relu()))\n",
    "\n",
    "# Demonstration\n",
    "test = torch.randn(1, 50, 512)\n",
    "ffn = FeedForwardNetwork(512, 2048)\n",
    "print(\"\\033[1mLet us test out our FFN on a randomly initialized tensor of size (1, 50, 512):\\033[0m\\n\\n\", test)\n",
    "print(\"\\n\\033[1mHere's our tensor, once passed through our FFN:\\033[0m\\n\\n\", ffn.forward(test).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt7mWrsmceTU"
   },
   "source": [
    "The $OutputGenerator$ class is used to generate the output probabilities of our Transformer architecture. This layer is a combination of the \"Linear\" & \"Softmax\" layers as seen in the canonical diagram of [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "Additionally, we run a brief demonstration below where we simulate passing in 512-dimensional logits representing 50 tokens of a single batch to our final $OutputGenerator$ layer in order to generate a set of next token predictions for each (relevant) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq2Hj7HtCOp1"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import typing\n",
    "\n",
    "class OutputGenerator(nn.Module):\n",
    "  def __init__(self, d_model: int, vocab: int) -> None:\n",
    "    '''\n",
    "    Inputs:\n",
    "      d_model - Hidden dimensionality of the output of the final FFN.\n",
    "      vocab - Size of our vocabulary.\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.projection = nn.Linear(d_model, vocab)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.functional.log_softmax(self.projection(x), dim=-1)\n",
    "\n",
    "# Demonstration\n",
    "print(\"Now, let's imagine we're passing an input through the final stages of our Transformer architecture. \\n Here, we'll pass an example tensor of size (1,50,512) through our FFN and our final output generator (we ignore the \\\"Add & Norm\\\" step for simplicity).\\n We can think of our example tensor as a batch containing a single, 50-token input with an embedding dimension of size 512. Let's state that the size of our vocabulary \\n is 100 tokens.\\n\")\n",
    "ffn = FeedForwardNetwork(512, 2048)\n",
    "generator = OutputGenerator(512, 100)\n",
    "test = torch.randn(1, 50, 512)\n",
    "ffn.forward(test).data\n",
    "output = generator.forward(test).data\n",
    "output = output[:, :(output.size(1) - 1)]\n",
    "print(\"Assuming our final token in the sequence we passed in is an [EOS] token, we strip that index from our output as the calculated Log Softmax probabilities are meaningless.\\n\")\n",
    "print(\"\\033[1mOutput of our generator:\\n\\033[0m\", \"\\033[1mSize: \\033[0m\", tuple(output.size()), \"\\n\\033[1mTensor: \\033[0m\", output)\n",
    "print(\"\\nNow, we take the argmax over our Log Softmax probabilities and return the vocabulary index of the most likely token for each sequence entry. Note that in real-life \\n applications, we usually take the argmax over our logits instead of our softmax values, for numerical stability reasons.\\n\")\n",
    "for index in range(output.size(1)):\n",
    "  print(f\"\\033[1mOur prediction for sequence token {index+1}:\\033[0m \", output.argmax(dim=-1).squeeze()[index].item())\n",
    "print(\"\\nIn a real-life example (assuming we've trained our model), most of the predictions produced here would be equivalent to the vocab index \\n associated with the subsequent token (assuming we're doing next token prediction).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLsSD2duCRNn"
   },
   "source": [
    "### **Section 5.2.4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAD_9lYsdmiN"
   },
   "source": [
    "When passing inputs to our Transformer, we typically pass in a tensor of size (*batch_size*, *num_tokens*). Before our model can add positional encodings to our inputs and properly attend to our model inputs, though, we need to embed these tokens in our model's hidden dimensionspace (e.g., 128, 512). We accomplish this through our $InputEmbeddings$ class, which is primarily powered by PyTorch's $torch.nn.Embedding$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9639Me_lCSu_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "  '''\n",
    "  This class is used to embed your encoder & decoder inputs into the model's hidden dimensionspace using nn.Embedding,\n",
    "  a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "\n",
    "  For example, if you have a tokenized input sequence length of 50 tokens, you would pass in 'input_size = 50' and\n",
    "  'd_model = <model hiden dimensionality>' when instantiating this class.\n",
    "  '''\n",
    "\n",
    "  def __init__(self, d_model : int, input_size : int) -> None:\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(input_size, d_model)\n",
    "    self.d_model = d_model\n",
    "\n",
    "  def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "    return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_c0TezceiKE"
   },
   "source": [
    "Our $SublayerConnection$ module will function the same as the $Add \\; \\& \\; Norm$ layer in the canonical Transformer architecture -- we just provide this layer with a different name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4nKMEl2CUtY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "  \"\"\"Residual connection followed by a layer normalization.\"\"\"\n",
    "\n",
    "  def __init__(self, feature_size : tuple, dropout : float = 0.0) -> None:\n",
    "    super().__init__()\n",
    "    self.layer_norm = LayerNorm(feature_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x : torch.Tensor, sublayer : nn.Module) -> torch.Tensor:\n",
    "    x = x + self.dropout(sublayer(x))\n",
    "    x = self.layer_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuEpODxtOhZn"
   },
   "source": [
    "## **Section 5.2(b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlVEPkHCe32I"
   },
   "source": [
    "In this section, we finally build the Transformer architecture. We start with our Encoder and Decoder blocks before combining the two into our complete Transformer under our $Transformer$ module. Additionally, in **Section 5.2(b).3.1** we construct a useful function which can be quickly used to initialize our model; we'll be making heavy use of this function in the various Transformer example applications provided in Sections 5.3-5.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JOIp__OCobv"
   },
   "source": [
    "### **Section 5.2(b).1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siE_tdf7fc2l"
   },
   "source": [
    "Here, our $EncoderLayer$ module functions as a single layer of the Transformer Encoder block. We use our $Encoder$ module in order to create multiple (deep copy!) instances of our $EncoderLayer$ module which feed into each other in a sequential fashion -- in short, our Encoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEw9ZgjmCp_T"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "from typing import Optional, Union\n",
    "\n",
    "# Modules\n",
    "class EncoderLayer(nn.Module):\n",
    "    '''This module defines a single layer of the Encoder.'''\n",
    "\n",
    "    def __init__(self, self_attn : MultiHeadAttention, feed_forward : FeedForwardNetwork, feature_size : tuple, dropout : float = 0.0) -> None:\n",
    "        '''\n",
    "        Inputs:\n",
    "            self_attn - A MHA self-attention module. This module must be an instance or subclass of MultiHeadAttention, or must otherwise have the same signature in its dunder __init__ and forward method.\n",
    "            feed_forward - A FFN module. This module must be an instance or subclass of FeedForwardNetwork, or must otherwise have the same signature in its dunder __init__ and forward method.\n",
    "            feature_size - A tuple containing the dimensions of an input tensor.\n",
    "            dropout - Dropout probability.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.modules = SimpleNamespace(self_attn = self_attn,\n",
    "                                       feed_forward = feed_forward,\n",
    "                                       sublayers = nn.ModuleList([copy.deepcopy(SublayerConnection(feature_size = feature_size, dropout = dropout)) for _ in range(2)]))\n",
    "\n",
    "    def forward(self, x : torch.Tensor, mask : Optional[Union[torch.Tensor, MaskingMechanism]] = None) -> torch.Tensor:\n",
    "        '''The forward step of our encoder layer. You must include an input tensor. Optionally, you can pass in an attention mask.'''\n",
    "        x = self.modules.sublayers[0].forward(x, lambda x: self.modules.self_attn(x, x, x, mask = mask))\n",
    "        x = self.modules.sublayers[1].forward(x, lambda x: self.modules.feed_forward(x))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''This module instantiates the Encoder.'''\n",
    "\n",
    "    def __init__(self, layer : EncoderLayer, num_layers : int) -> None:\n",
    "        '''\n",
    "        Inputs:\n",
    "            layer - A single instance of an encoder layer. This module must be an instance or subclass of EncoderLayer, or must otherwise have the same signature in its dunder __init__ and forward method.\n",
    "            num_layers - The number of layers in the encoder.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x : torch.Tensor, mask : Optional[Union[torch.Tensor, MaskingMechanism]] = None) -> torch.Tensor:\n",
    "        '''The forward step of our encoder. You must include an input tensor. Optionally, you can include an attention mask.'''\n",
    "        for layer in self.layers:\n",
    "            x = layer(x = x, mask = mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tfv3plE0CrCu"
   },
   "source": [
    "### **Section 5.2(b).2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dEZL1xuftDn"
   },
   "source": [
    "Here, we apply the same process as in **Section 5.2(b).1** to build out our Transformer's Decoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSu1y8q8CsFc"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn\n",
    "import copy\n",
    "from types import SimpleNamespace\n",
    "from typing import Optional, Union\n",
    "\n",
    "# Modules\n",
    "class DecoderLayer(nn.Module):\n",
    "    '''This module defines a single layer of the Decoder.'''\n",
    "\n",
    "    def __init__(self, self_attn : MultiHeadAttention, encoder_attn : MultiHeadAttention, feed_forward : FeedForwardNetwork, feature_size : tuple, dropout : float = 0.0) -> None:\n",
    "        '''\n",
    "        Inputs:\n",
    "            self_attn - A MHA self-attention module. This module must be an instance or subclass of MultiHeadAttention, or must otherwise have the same signature in its dunder __init__ and forward method.\n",
    "            encoder_attn - A MHA module which takes as query and value the outputs of the Encoder. With the exception of the optional mask, this module should be configured in the exact same manner as the self_attn module. This module must be an instance or subclass of MultiHeadAttention, or must otherwise have the same signature in its dunder __init__ and forward method.\n",
    "            feed_forward - A FFN module. This module must be an instance or subclass of FeedForwardNetwork, or must otherwise have the same signature in its dunder __init__ and forward method.\n",
    "            feature_size - A tuple containing the dimensions of an input tensor.\n",
    "            dropout - Dropout probability.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.modules = SimpleNamespace(self_attn = self_attn,\n",
    "                                       encoder_attn = encoder_attn,\n",
    "                                       feed_forward = feed_forward,\n",
    "                                       sublayers = nn.ModuleList([copy.deepcopy(SublayerConnection(feature_size = feature_size, dropout = dropout)) for _ in range(3)]))\n",
    "\n",
    "    def forward(self, x : torch.Tensor, encoder_output : torch.Tensor, encoder_mask : Optional[Union[torch.Tensor, MaskingMechanism]] = None, decoder_mask : Optional[Union[torch.Tensor, MaskingMechanism]] = None) -> torch.Tensor:\n",
    "        '''The forward step of our decoder layer. You must input an input tensor as well as the output of the encoder (encoder_output). Optionally, you can provide a mask for the self_attn module and a mask for the encoder_attn module.'''\n",
    "        x = self.modules.sublayers[0].forward(x, lambda x: self.modules.self_attn(x, x, x, mask = decoder_mask))\n",
    "\n",
    "        # If you are using a decoder-only model, then set encoder_output equivalent to x.\n",
    "        x = self.modules.sublayers[1].forward(x, lambda x: self.modules.encoder_attn(x, encoder_output, encoder_output, mask = decoder_mask))\n",
    "\n",
    "        x = self.modules.sublayers[2].forward(x, lambda x: self.modules.feed_forward(x))\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''This module instantiates the Decoder.'''\n",
    "\n",
    "    def __init__(self, layer : DecoderLayer, num_layers : int) -> None:\n",
    "        '''\n",
    "        Inputs:\n",
    "            layer - A single instance of a decoder layer. This module must be an instance or subclass of DecoderLayer, or must otherwise have the same signature in its dunder __init__ and forward method.\n",
    "            num_layers - The number of layers in the decoder.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x : torch.Tensor, encoder_output: torch.Tensor, encoder_mask : Optional[Union[torch.Tensor, MaskingMechanism]] = None, decoder_mask : Optional[Union[torch.Tensor, MaskingMechanism]] = None) -> torch.Tensor:\n",
    "        '''The forward step of our decoder. You must input an input tensor as well as the output of the encoder (encoder_output). Optionally, you can provide a mask for the self_attn module and a mask for the encoder_attn module.'''\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, encoder_mask, decoder_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA7aZVQGCwGJ"
   },
   "source": [
    "### **Section 5.2(b).3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADsAeTvAf5vW"
   },
   "source": [
    "In this section, we build our Transformer architecture. Note that via the `model_construct` class initialization input, we can specify which type of Transformer we'd like to initialize: Encoder-Only, Decoder-Only, and Complete. Our `forward` class method is pre-configured depending on which argument one passes to `model_construct`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLQdVkHXCyFa"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union\n",
    "\n",
    "# Module\n",
    "class Transformer(nn.Module):\n",
    "  '''This module can be used to instantiate and make a forward pass through a Transformer.'''\n",
    "\n",
    "  def __init__(self, encoder : Encoder, decoder : Decoder, encoder_embed : nn.Sequential, decoder_embed : nn.Sequential, output_generator : OutputGenerator, model_construct : Union[str(\"encoder\"), str(\"decoder\"), str(\"encoder-decoder\")]) -> None:\n",
    "    '''\n",
    "    Inputs:\n",
    "      encoder - This is an instance of the Encoder class. Make sure to initialize this class with all of the required parameters.\n",
    "      decoder - This is an instance of the Decoder class. Make sure to initialize this class with all of the required parameters.\n",
    "      encoder_embed - This is an instance of the nn.Sequential class whose inputs are instances of the InputEmbeddings and PositionalEncoding classes. Make sure to initialize this class with all of the required parameters.\n",
    "      decoder_embed - This is an instance of the nn.Sequential class whose inputs are instances of the InputEmbeddings and PositionalEncoding classes. Make sure to initialize this class with all of the required parameters.\n",
    "      output_generator - This is an instance of the OutputGenerator class. Make sure to initialize this class with all of the required parameters.\n",
    "      model_construct - Pass in an argument specifying what instance of the Transformer you would like to return: encoder-only, decoder-only, or the complete encoder-decoder model. See the signature to this method for the list of acceptable inputs.\n",
    "    '''\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.encoder_embed = encoder_embed\n",
    "    self.decoder_embed = decoder_embed\n",
    "    self.output_generator = output_generator\n",
    "    self.model_construct = str(model_construct)\n",
    "\n",
    "  def forward(self, encoder_input : torch.Tensor = None, encoder_output : torch.Tensor = None, decoder_input : torch.Tensor = None, encoder_mask : Optional[Union[torch.Tensor, MaskingMechanism]] = None, decoder_mask : Optional[Union[torch.Tensor, MaskingMechanism]] = None) -> torch.Tensor:\n",
    "    '''\n",
    "    Inputs:\n",
    "      encoder_input - This is the input sequence passed to the encoder. This method maps the hidden dimensionality of the input to the embed_dim; otherwise, you must make sure tokenization has been completed, alongside any other required pre-processing methods.\n",
    "      encoder_output - This is the output of the encoder, passed into the second MHA block within the Decoder. If you are using a decoder-only model, simply leave this argument as None.\n",
    "      decoder_input - This is the input sequence passed to the decoder. This method maps the hidden dimensionality of the input to the embed_dim; otherwise, you must make sure tokenization has been completed, alongside any other required pre-processing methods.\n",
    "      encoder_mask - An optional input. Pass in a tensor mask for your input to the encoder.\n",
    "      decoder_mask - An optional input. Pass in a tensor mask for your input to the decoder.\n",
    "    '''\n",
    "    if self.model_construct == \"encoder\":\n",
    "      return self.encoder(self.encoder_embed(encoder_input), encoder_mask)\n",
    "    elif self.model_construct == \"decoder\":\n",
    "      if encoder_output is not None:\n",
    "        return self.decoder(self.decoder_embed(decoder_input), encoder_output, encoder_mask, decoder_mask)\n",
    "      else:\n",
    "        return self.decoder(self.decoder_embed(decoder_input), self.decoder_embed(decoder_input), encoder_mask, decoder_mask)\n",
    "    elif self.model_construct == \"encoder-decoder\":\n",
    "      return self.decoder(self.decoder_embed(decoder_input), self.encoder(self.encoder_embed(encoder_input), encoder_mask), encoder_mask, decoder_mask)\n",
    "    else:\n",
    "      print(\"This is an invalid input.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCVdk0z2C79D"
   },
   "source": [
    "#### **Section 5.2(b).3.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI490nEbgg43"
   },
   "source": [
    "In this section, we provide a simple `model_constructor` function which can be used to quickly initialize an instance of our Transformer model. Instead of passing in a variety of different modules (each with their own `__init__` arguments) to our $Transformer$ module, we can instead pass in the appropriate sequence of strings, integers, and floating-point numbers to easily initialize a $Transformer$ module.\n",
    "\n",
    "This function may present some limitations when initializing a Transformer model as compared to initializing the model architecture via the $Transformer$ module directly, depending on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H75TaCCxC7fk"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import copy\n",
    "\n",
    "# Function\n",
    "def model_constructor(model_construct : Union[str(\"encoder\"), str(\"decoder\"), str(\"encoder-decoder\")], feature_size : tuple, encoder_input_size : int, decoder_input_size : int, vocab_size : int, num_layers : int, embed_dim : int = 512, d_ff : int = 2048, num_heads : int = 8, dropout : float = 0.1, pos_encoding_max_length : int = 5000):\n",
    "  '''\n",
    "  This function can be used to initialize the Transformer module.\n",
    "\n",
    "  Inputs:\n",
    "    model_construct - Pass in an argument specifying what instance of the Transformer you would like to return: encoder-only, decoder-only, or the complete encoder-decoder model. See the signature to this method for the list of acceptable inputs.\n",
    "    feature_size - A tuple containing the dimensions of an input tensor.\n",
    "    encoder_input_size - The size of the vocabulary of the input sequence passed to the encoder. For example, an input sequence of 10 randomly generated integers ranging from 0 to 100 would correspond to a vocabulary size of 101.\n",
    "    decoder_input_size - The size of the vocabulary of the input sequence passed to the decoder.\n",
    "    vocab_size - The number of labels you have in your vocabulary for your output generator.\n",
    "    num_layers - The number of layers in your Transformer. If you initialize a Transformer with both an encoder and a decoder, both blocks will have the same number of layers.\n",
    "    embed_dim - The size of the hidden dimensions of the input(s) to the model.\n",
    "    d_ff - The size of the hidden dimensionas of the feed forward network.\n",
    "    num_heads - The number of heads in the MHA blocks. Make sure that embed_dim % num_heads == 0.\n",
    "    dropout - Dropout probability.\n",
    "    pos_encoding_max_length - The maximum length of the positional encoding.\n",
    "  '''\n",
    "\n",
    "  positional_encoding = PositionalEncoding(embed_dim, dropout, pos_encoding_max_length)\n",
    "  multihead_attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "  ffn = FeedForwardNetwork(embed_dim, d_ff, dropout)\n",
    "  output_generator = OutputGenerator(embed_dim, vocab_size)\n",
    "\n",
    "  encoder = Encoder(layer = EncoderLayer(copy.deepcopy(multihead_attn), copy.deepcopy(ffn), feature_size, dropout), num_layers = num_layers)\n",
    "  decoder = Decoder(layer = DecoderLayer(copy.deepcopy(multihead_attn), copy.deepcopy(multihead_attn), copy.deepcopy(ffn), feature_size, dropout), num_layers = num_layers)\n",
    "  encoder_embed = nn.Sequential(InputEmbeddings(embed_dim, encoder_input_size), copy.deepcopy(positional_encoding))\n",
    "  decoder_embed = nn.Sequential(InputEmbeddings(embed_dim, decoder_input_size), copy.deepcopy(positional_encoding))\n",
    "\n",
    "  model = Transformer(\n",
    "      encoder,\n",
    "      decoder,\n",
    "      encoder_embed,\n",
    "      decoder_embed,\n",
    "      copy.deepcopy(output_generator),\n",
    "      model_construct\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06X9EVnNDCkU"
   },
   "source": [
    "## **Section 5.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpt_dla1Tr1n"
   },
   "source": [
    "Below, we highlight a toy example involving use of an Encoder-only Transformer. In this example, we are tasking a Transformer Encoder with reversing a sequence of random integers fed to the model as input. In particular, we will be feeding our model with a single batch with 15 tokens of hidden dimension 1; these 15 tokens are pulled from a set of 10 randomly integers ranging from 0 to 9. Since we haven't trained our Transformer, the outputs of our model for this example will be non-sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zo3gT2mDEO5"
   },
   "outputs": [],
   "source": [
    "## Toy Encoder Example\n",
    "# Imports\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, num_different_numbers : int, seq_len : int, batch_size : int) -> None:\n",
    "    '''\n",
    "    Inputs:\n",
    "      num_different_numbers - We will be using torch.randint in order to generate our batch of sequences of random numbers. The low by default is 0. The highest possible integer generated is the value assigned to this variable.\n",
    "      seq_len - The length of each sequence of random integers.\n",
    "      batch_size - This determines the size of the batch of the sequence of random integers.\n",
    "    '''\n",
    "\n",
    "    super().__init__()\n",
    "    self.parameters = SimpleNamespace(num_different_numbers = num_different_numbers,\n",
    "                                      seq_len = seq_len,\n",
    "                                      batch_size = batch_size)\n",
    "\n",
    "    self.data = torch.randint(self.parameters.num_different_numbers, size=(self.parameters.batch_size, self.parameters.seq_len))\n",
    "\n",
    "  # To define our Dataset class, we'll need to define two methods: __len__ and __getitem__.\n",
    "  # __len__ returns the size of the dataset.\n",
    "  def __len__(self) -> torch.Tensor.size:\n",
    "    return self.size\n",
    "\n",
    "  # __getitem__ returns the idx'th batch entry.\n",
    "\n",
    "  # IMPORTANT: Our Dataset class is currently constructed such that it only produces a single batch. If we want to have a dataset which can handle multiple\n",
    "  # batches, we'd need to (a) pass in an num_batches parameter to the __init__ method, and (b) pass in an additional batch_idx parameter to this method.\n",
    "  def __getitem__(self, idx : int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    input_data = self.data[idx]\n",
    "    labels = torch.flip(input_data, dims=(0,))\n",
    "    return input_data, labels\n",
    "\n",
    "\n",
    "\n",
    "# This small function can be used to instantiate our Dataset class with the desired initialization configuration described above.\n",
    "def initialize_data():\n",
    "  temp_dataset = partial(Dataset, num_different_numbers = 10, seq_len = 15, batch_size = 1)\n",
    "  working_dataset = copy.deepcopy(temp_dataset())\n",
    "\n",
    "  del temp_dataset\n",
    "\n",
    "  return working_dataset\n",
    "\n",
    "\n",
    "\n",
    "# We'll call this function to return our model prediction, among other values.\n",
    "def inference():\n",
    "  '''\n",
    "  Outputs:\n",
    "    encoder_input - This is the input sequence passed to the encoder. In our case, this is a single batch containing 15 tokens.\n",
    "    labels - This is the label sequence corresponding to the input sequence. In our case, this is simply a flipped version of the encoder_input.\n",
    "    model_prediction - These are the predictions for our model. If trained, our model should be able to predict the labels from the encoder_input.\n",
    "    loss - This is our model's cross entropy loss.\n",
    "    accuracy - This is our model's accuracy (between 0 and 1).\n",
    "  '''\n",
    "\n",
    "  # Instantiate our model. One quirk of the model_constructor function is that we need to specify 'decoder_input_size', even though this information isn't relevant if we are using the encoder-only implementation of the Transformer.\n",
    "  test_model = model_constructor(model_construct = \"encoder\", feature_size = (1, 15, 1), encoder_input_size = 10, decoder_input_size = 0, vocab_size = 10, num_layers = 2)\n",
    "  # Putting our model into evaluation mode (instead of training mode)\n",
    "  test_model.eval()\n",
    "\n",
    "  data = initialize_data()\n",
    "  labels = data[0][1]\n",
    "  encoder_input = data[0][0].unsqueeze(0)\n",
    "\n",
    "  # To store our predictions, we're going to instantiate here a tensor of size (1,1) containing the input 0.\n",
    "  model_prediction = torch.empty(1, 1).fill_(0)\n",
    "  # We're going to store all of our model's output probabilities into this empty tensor.\n",
    "  probabilities_reference = torch.empty(15, 10).requires_grad_(False)\n",
    "\n",
    "  for i in range(15):\n",
    "    encoder_output = test_model(encoder_input = encoder_input)\n",
    "    probabilities = test_model.output_generator(encoder_output[:, -1])\n",
    "    probabilities_reference[i, :] = probabilities\n",
    "    _, next_integer = torch.max(probabilities, dim = 1)\n",
    "    # Assume that our next predicted integer is 6. This step converts next_integer from tensor([6]) to tensor(6), which is necessary for concatenating all of our predictions together into a single tensor.\n",
    "    next_integer = next_integer[0]\n",
    "    if i == 0:          # For our first prediction, we want to replace the 0 input in the model_prediction tensor.\n",
    "      model_prediction[:, 0] = next_integer\n",
    "    else:         # Following our first prediction, we want to concatenate our predictions to the model_prediction tensor.\n",
    "      model_prediction = torch.cat([model_prediction, torch.empty(1,1).fill_(next_integer)], dim = 1).type_as(labels)\n",
    "\n",
    "  loss = F.cross_entropy(probabilities_reference, labels)\n",
    "  accuracy = (probabilities_reference.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "  return encoder_input, labels, model_prediction, loss, accuracy\n",
    "\n",
    "\n",
    "# Outputs\n",
    "results = inference()\n",
    "print(\"\\033[1mThis is the input to our Transformer:\\033[0m \", results[0].data)\n",
    "print(\"\\n\\033[1mThese are the labels for our input:\\033[0m \", results[1].data)\n",
    "print(\"\\n\\033[1mThis is our model's (untrained) prediction:\\033[0m \", results[2].data)\n",
    "print(\"\\n\\033[1mThis is our model's cross entropy loss:\\033[0m \", results[3].data)\n",
    "print(\"\\n\\033[1mThis is our model's accuracy:\\033[0m \", results[4].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNEEf4cHOpoY"
   },
   "source": [
    "## **Section 5.4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6uoW5x8DJs8"
   },
   "source": [
    "### **Section 5.4.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohSC_ydfUIc7"
   },
   "source": [
    "Here, we highlight a toy example involving the utilization of a Decoder-only Transformer. In this example, we are tasking a Transformer Decoder with predicting a sequence of Fibonacci numbers. The Transformer is tasked with generating n Fibonacci sequence entries, starting from the i'th entry; the first token in our masked input will correspond to the integer i. In particular, we will be asking our Transformer to generate a single batch of 15 Fibonacci sequence entries, starting from the 10th entry. Since we haven't trained our Transformer, the outputs of our model for this example will be non-sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f970BQkiDLrH"
   },
   "outputs": [],
   "source": [
    "## Toy Decoder Example\n",
    "# Imports\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate a Fibonacci number\n",
    "def fibonacci_sequence(fibonacci_num):\n",
    "  '''Function to generate the fibonacci_num'th Fibonacci sequence entry.'''\n",
    "  if fibonacci_num < 2:\n",
    "    return int(fibonacci_num)\n",
    "  else:\n",
    "    return fibonacci_sequence(fibonacci_num - 1) + fibonacci_sequence(fibonacci_num - 2)\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, fibonacci_start : int, num_fibonacci_numbers : int, batch_size : int) -> None:\n",
    "    '''\n",
    "    Inputs:\n",
    "      fibonacci_start - The i'th Fibonacci sequence entry. For example, fibonacci_start = 10 corresponds to the 10th Fibonacci sequence entry: 55.\n",
    "      num_fibonacci_numbers - The number of Fibonacci sequence entries to generate, starting from fibonacci_start.\n",
    "      batch_size - The number of input sequences to generate (that is, the size of a single batch).\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    # The first token in our input sequence will be an integer indicating the first Fibonacci sequence entry to generate.\n",
    "    self.starting_token = torch.empty(1,1).fill_(fibonacci_start)\n",
    "    self.fibonacci_numbers = torch.LongTensor([fibonacci_sequence(fibonacci_start + i) for i in range(num_fibonacci_numbers)])\n",
    "\n",
    "    self.data = torch.cat([self.starting_token, self.fibonacci_numbers.unsqueeze(0)], dim=1).type_as(self.fibonacci_numbers)\n",
    "\n",
    "  def __len__(self) -> torch.Tensor.size:\n",
    "    return self.size\n",
    "\n",
    "  def __getitem__(self, idx : int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    input_data = self.data[idx][:-1]\n",
    "    labels = self.data[idx][1:]\n",
    "    return input_data, labels\n",
    "\n",
    "\n",
    "\n",
    "# Function to initialize our data\n",
    "def initialize_data():\n",
    "  temp_dataset = partial(Dataset, fibonacci_start = 10, num_fibonacci_numbers = 15, batch_size = 1)\n",
    "  working_dataset = copy.deepcopy(temp_dataset())\n",
    "\n",
    "  del temp_dataset\n",
    "\n",
    "  return working_dataset\n",
    "\n",
    "\n",
    "\n",
    "def inference():\n",
    "  # The size of our vocabulary must be large enough to have indices for all of the integers from 0 to the largest Fibonacci number we generate.\n",
    "  max_fibonacci_start = 10\n",
    "  max_num_fibonacci_numbers = 15\n",
    "  vocab_size = fibonacci_sequence(max_fibonacci_start + max_num_fibonacci_numbers - 1) + 1\n",
    "\n",
    "  # One quirk of the model_constructor function is that we need to specify 'encoder_input_size', even though this information isn't relevant if we are using the decoder-only implementation of the Transformer.\n",
    "  test_model = model_constructor(\"decoder\", feature_size = (1, 15, 1), encoder_input_size = 0, decoder_input_size = vocab_size, vocab_size = vocab_size, num_layers = 2)\n",
    "  test_model.eval()\n",
    "\n",
    "  # Initialize data\n",
    "  data = initialize_data()\n",
    "  labels = data[0][1]\n",
    "  decoder_inputs = data[0][0].unsqueeze(0)\n",
    "  decoder_mask = MaskingMechanism.decoder_mask(size = 15)\n",
    "\n",
    "  # Tensors used to store our model predictions and probabilities\n",
    "  model_prediction = torch.empty(1, 1).fill_(0)\n",
    "  probabilities_reference = torch.empty(15, vocab_size).requires_grad_(False)\n",
    "\n",
    "  # Inference loop\n",
    "  for i in range(15):\n",
    "    decoder_output = test_model(decoder_input = decoder_inputs, decoder_mask = decoder_mask)\n",
    "    probabilities = test_model.output_generator(decoder_output[:, -1])\n",
    "    probabilities_reference[i, :] = probabilities\n",
    "    _, next_fibonacci = torch.max(probabilities, dim = 1)\n",
    "    next_fibonacci = next_fibonacci[0]\n",
    "\n",
    "    if i == 0:\n",
    "      model_prediction[:, 0] = next_fibonacci\n",
    "    else:\n",
    "      model_prediction = torch.cat([model_prediction, torch.empty(1,1).fill_(next_fibonacci)], dim = 1).type_as(labels)\n",
    "\n",
    "  # Metric calculations\n",
    "  loss = F.cross_entropy(probabilities_reference, labels)\n",
    "  accuracy = (probabilities_reference.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "  return decoder_inputs, labels, model_prediction, loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Output\n",
    "results = inference()\n",
    "print(\"\\033[1mThis is the input to our Transformer:\\033[0m \", f\"\\n{results[0].data}\")\n",
    "print(\"\\n\\033[1mThese are the labels for our input:\\033[0m \", f\"\\n{results[1].data}\")\n",
    "print(\"\\n\\033[1mThis is our model's (untrained) prediction:\\033[0m \", f\"\\n{results[2].data}\")\n",
    "print(\"\\n\\033[1mThis is our model's cross entropy loss:\\033[0m \", results[3].data)\n",
    "print(\"\\n\\033[1mThis is our model's accuracy:\\033[0m \", results[4].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3Zm7yoyDQH7"
   },
   "source": [
    "## **Section 5.5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WteWPTLvUSC2"
   },
   "source": [
    "Here, we highlight a toy example involving the use of a complete Encoder & Decoder Transformer. In this example, we are tasking a Transformer with a simple language task: Integers-to-Binary language translation. We will be feeding a set of randomly-generated integers (a single batch of 15 tokens ranging in value from 0 to 10) to our encoder before tasking our decoder with translating these integers into their respective binary representation. Note that Integers-to-Binary language translation is a formal language translation task (a much simpler task compared to natural language processing, or NLP). Since we haven't trained our Transformer, the outputs of our model for this example will be non-sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAwA2bRMDRft"
   },
   "outputs": [],
   "source": [
    "## Toy Encoder-Decoder Example\n",
    "# Description\n",
    "'''\n",
    "In this example, we are tasking a complete Transformer model with a simple language task: Integers-to-Binary language\n",
    "translation. We will be feeding a set of randomly generated integers to our encoder before tasking our decoder with\n",
    "translating these integers into their binary representation.\n",
    "'''\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "from typing import Tuple\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "# Integers-to-Binary Translation Function\n",
    "def int_to_binary(num):\n",
    "  '''Given an integer, this function returns its binary representation. We use [2:] to remove the '0b' prefix.'''\n",
    "  return bin(num)[2:]\n",
    "\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, vocab : int, num_random_integers : int, batch_size : int) -> None:\n",
    "    super().__init__()\n",
    "    self.data = torch.randint(vocab, size=(batch_size, num_random_integers))\n",
    "\n",
    "  def __len__(self) -> torch.Tensor.size:\n",
    "    return self.size\n",
    "\n",
    "  def __getitem__(self, idx : int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    encoder_input = self.data[idx]\n",
    "\n",
    "    # We append a [CLS] 0-token to the start of the decoder_input. This [CLS] token tells our Decoder to predict the\n",
    "    # binary representation of the first integer in the sequence we've passed to the Encoder.\n",
    "    decoder_input = torch.cat([torch.empty(1).fill_(0), torch.LongTensor([int(int_to_binary(input)) for input in encoder_input[:-1]])], dim = 0)\n",
    "\n",
    "    labels = torch.LongTensor([int(int_to_binary(input)) for input in encoder_input])\n",
    "    return encoder_input, decoder_input, labels\n",
    "\n",
    "\n",
    "\n",
    "# Data initialization function.\n",
    "def initialize_data():\n",
    "  temp_dataset = partial(Dataset, vocab = 11, num_random_integers = 15, batch_size = 1)\n",
    "  working_dataset = copy.deepcopy(temp_dataset())\n",
    "\n",
    "  del temp_dataset\n",
    "\n",
    "  return working_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Inference function\n",
    "def inference():\n",
    "  # The binary representation of the integer 10 is '1010'; as such, we need a Decoder and Output Generator vocabulary which can represent all integers from 0 to 1010.\n",
    "  test_model = model_constructor(\"encoder-decoder\", feature_size = (1,15,1), encoder_input_size = 11, decoder_input_size = 1011, vocab_size = 1011, num_layers = 2)\n",
    "  test_model.eval()\n",
    "\n",
    "  data = initialize_data()\n",
    "  labels = data[0][2]\n",
    "  encoder_input = data[0][0].unsqueeze(0)\n",
    "  decoder_input = data[0][1].unsqueeze(0).type_as(encoder_input)\n",
    "  decoder_mask = MaskingMechanism.decoder_mask(size = 15)\n",
    "\n",
    "  model_prediction = torch.empty(1, 1).fill_(0)\n",
    "  probabilities_reference = torch.empty(15, 1011).requires_grad_(False)\n",
    "\n",
    "  # Inference loop\n",
    "  for i in range(15):\n",
    "    transformer_ouput = test_model(encoder_input = encoder_input, decoder_input = decoder_input, decoder_mask = decoder_mask)\n",
    "    probabilities = test_model.output_generator(transformer_ouput[:, -1])\n",
    "    probabilities_reference[i, :] = probabilities\n",
    "    _, next_binary = torch.max(probabilities, dim = 1)\n",
    "    next_binary = next_binary[0]\n",
    "    if i == 0:\n",
    "      model_prediction[:, 0] = next_binary\n",
    "    else:\n",
    "      model_prediction = torch.cat([model_prediction, torch.empty(1,1).fill_(next_binary)], dim = 1).type_as(labels)\n",
    "\n",
    "  loss = F.cross_entropy(probabilities_reference, labels)\n",
    "  accuracy = (probabilities_reference.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "  return encoder_input, decoder_input, labels, model_prediction, loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Outputs\n",
    "results = inference()\n",
    "print(\"\\033[1mThis is the Encoder input to our Transformer:\\033[0m \", f\"\\n{results[0].data}\")\n",
    "print(\"\\033[1mThis is the Decoder input to our Transformer:\\033[0m \", f\"\\n{results[1].data}\")\n",
    "print(\"\\n\\033[1mThese are the labels for our input:\\033[0m \", f\"\\n{results[2].data}\")\n",
    "print(\"\\n\\033[1mThis is our model's (untrained) prediction:\\033[0m \", f\"\\n{results[3].data}\")\n",
    "print(\"\\n\\033[1mThis is our model's cross entropy loss:\\033[0m \", results[4].data)\n",
    "print(\"\\n\\033[1mThis is our model's accuracy:\\033[0m \", results[5].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAvoiJCbYTeL"
   },
   "source": [
    "# **Section 6: Introduction to 🤗**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqKCKJNPVfI8",
    "outputId": "f09167d2-8803-414d-d489-ec3af872e840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Initial Imports\n",
    "!pip install --quiet datasets evaluate transformers[sentencepiece]\n",
    "!pip install --quiet zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "N5uCG9qjkZME",
    "outputId": "17f5a874-72fc-4609-a792-4a28b18b952b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/huggingface_folder'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change cwd to 'huggingface_folder' -- this is where we want to store any files we download via !wget\n",
    "import os\n",
    "\n",
    "os.mkdir(\"/content/huggingface_folder\")\n",
    "os.chdir(\"/content/huggingface_folder\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_VZt76SQXa0"
   },
   "source": [
    "As the title suggests, Section 6 presents a rigorous introduction to the 🤗 ecosystem; in particular, we will provide readers with a strong foundation in 3 major HF libraries: 🤗 Datasets, 🤗 Tokenizers, and 🤗 Transformers.\n",
    "\n",
    "If you already have experience with 🤗, you may find yourself skimming portions of this section. Note that besides covering the practical applications of the 🤗 system, we also cover certain theoretical components of the HF architecture (e.g. memory-mapped files with Apache Arrow data formatting) and LLM architecture in general (e.g. WordPiece, one prominent tokenization algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG8CErgAyg3N"
   },
   "source": [
    "## **6.1: 🤗 Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UA0txoOSO4Q5"
   },
   "source": [
    "$\\textbf{Section Introduction:}$\n",
    "\n",
    "**Source:** [The 🤗 Datasets Library](https://huggingface.co/learn/nlp-course/chapter5/1?fw=pt)\n",
    "\n",
    "In this section, we will cover basic functionality with 🤗 Datasets. In particular, we will look at some of the prominent practical applications of the HF Datasets library when it comes to loading in and working with Dataset & DatasetDict objects before turning towards a theoretical discussion of memory-mapping files with Apache Arrows (memory save) and streaming in datasets (disk space save)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ArfPpiD0vJr"
   },
   "source": [
    "### **6.1.1: Loading local and remote datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6ljbfIqPQWY"
   },
   "source": [
    "$\\textbf{Introduction:}$\n",
    "\n",
    "**Source:** [What if my dataset isn’t on the Hub?](https://huggingface.co/learn/nlp-course/chapter5/2?fw=pt)\n",
    "\n",
    "In order to load in locally-stored and remotely-stored datasets into our HF ecosystem, we will use the HF Datasets' `load_dataset()` function.\n",
    "\n",
    "You can read a detailed documentation of this function [here](https://huggingface.co/docs/datasets/v2.21.0/en/package_reference/loading_methods#datasets.load_dataset). In short, the 2 main arguments which this function takes are:\n",
    "\n",
    "1.   `path`: We use this argument to specify the type of loading script we want to use. The main options are: CSV & TSV (<code>\"csv\"</code>), text files (<code>\"text\"</code>), JSON & JSON Lines (<code>\"json\"</code>), and pickled DataFrames (<code>\"pandas\"</code>).\n",
    "2.   `data_files`: This argument specifies the path to one or more files.\n",
    "\n",
    "Additionally, another common argument you might be working with is the `field` argument. For example, if you're working with a nested JSON object such as the following:\n",
    "\n",
    "<pre><code>{\"version\": \"0.1.0\",\n",
    "   \"data\": [{\"a\": 1, \"b\": 2.0, \"c\": \"foo\", \"d\": false},\n",
    "          {\"a\": 4, \"b\": -5.5, \"c\": null, \"d\": true}]\n",
    "}\n",
    "</code></pre>\n",
    "\n",
    "then you'd want to specify `field = \"data\"` when loading in the above JSON object as a dataset.\n",
    "\n",
    "Below, we have a table listing all of the common data formats which `load_dataset()` supports:\n",
    "\n",
    "\\\n",
    "\n",
    "\\begin{array}{ccc}\n",
    "  \\textbf{Data format} & \\textbf{Loading script} & \\textbf{Example} \\\\\n",
    "  \\hline\n",
    "  CSV \\; \\& \\; TSV & \\small{\"csv\"} & \\scriptsize{load\\_dataset(\"csv\", \\; data\\_files=\"my\\_file.csv\")} \\\\\n",
    "  \\hline\n",
    "  Text \\; files & \\small{\"text\"} & \\scriptsize{load\\_dataset(\"text\", \\; data\\_files=\"my\\_file.txt\")} \\\\\n",
    "  \\hline\n",
    "  JSON \\; \\& \\; JSON \\ Lines & \\small{\"json\"} & \\scriptsize{load\\_dataset(\"json\", \\; data\\_files=\"my\\_file.jsonl\")} \\\\\n",
    "  \\hline\n",
    "  Pickled \\; DataFrames & \\small{\"pandas\"} & \\scriptsize{load\\_dataset(\"pandas\", \\; data\\_files=\"my\\_dataframe.pkl\")}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2qEczkmU0DA"
   },
   "source": [
    "$\\textbf{Example:}$\n",
    "\n",
    "As a simple example for working with local and remote datasets, we'll be working with the [SQuAD-it dataset](https://github.com/crux82/squad-it/), a large-scale dataset utilized for QA in Italian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqvCDPnZVJHs",
    "outputId": "46fc4049-e362-4276-a218-a278678da085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-11 04:29:47--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz [following]\n",
      "--2024-09-11 04:29:47--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7725286 (7.4M) [application/octet-stream]\n",
      "Saving to: ‘SQuAD_it-train.json.gz’\n",
      "\n",
      "SQuAD_it-train.json 100%[===================>]   7.37M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-09-11 04:29:47 (78.6 MB/s) - ‘SQuAD_it-train.json.gz’ saved [7725286/7725286]\n",
      "\n",
      "--2024-09-11 04:29:48--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz [following]\n",
      "--2024-09-11 04:29:48--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1051245 (1.0M) [application/octet-stream]\n",
      "Saving to: ‘SQuAD_it-test.json.gz’\n",
      "\n",
      "SQuAD_it-test.json. 100%[===================>]   1.00M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2024-09-11 04:29:48 (17.8 MB/s) - ‘SQuAD_it-test.json.gz’ saved [1051245/1051245]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's load in the SQuAD-it training and test splits locally using a simple wget command\n",
    "\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TreVvFknVSxl",
    "outputId": "4a9987ff-e476-4b7e-df38-d328de090e45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# HF Datasets can automatically decompress GZIP, ZIP, TAR, and other files of a similar format.\n",
    "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
    "  # You can also pass in a list of files. If you do so, by default HF will try to combine all the files into a single split,\n",
    "  # which is by default \"train\".\n",
    "\n",
    "  # Additionally, you can glob files that match a specified pattern according to the rules used by the Unix shell.\n",
    "    # Example: ' data_files=\"*.json\" ' will glob all the JSON files in a directory as a single split.\n",
    "\n",
    "\n",
    "# Loading in our dataset\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "\n",
    "# Final result\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvZd99wgWIFI"
   },
   "source": [
    "As we see, our loaded-in dataset is a `DatasetDict` object, with our specified \"train\" and \"test\" splits. A `DatasetDict` object is an object with split names (e.g. \"train\", \"test\") as keys and `Dataset` objects as values. The main advantage of `DatasetDict` objects is that we can use dataset transform methods (Section 6.1.2), like map or filter, to process all the splits at once.\n",
    "\n",
    "In the HF ecosystem, the `Dataset` object is a base class backed by an Arrow table (Section 6.1.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZDOIKK4XfRJ",
    "outputId": "c8b29782-cb6f-42cd-c32e-250ed0352855",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"ABC (Stati Uniti d'America)\",\n",
       " 'paragraphs': [{'context': \"L' American Broadcasting Company (ABC) (stlized nel suo logo come abc dal 1957) è una rete televisiva commerciale americana trasmissione televisiva che è di proprietà del Disney-ABC Television Group, una controllata della divisione Disney Media Networks di The Walt Disney Company. La rete fa parte delle grandi reti televisive Big Three. La rete ha sede a Columbus Avenue e West 66th Street a Manhattan, con ulteriori uffici e stabilimenti di produzione a New York City, Los Angeles e Burbank, California.\",\n",
       "   'qas': [{'answers': [{'answer_start': 257,\n",
       "       'text': 'The Walt Disney Company'},\n",
       "      {'answer_start': 171, 'text': 'Disney-ABC Television Group'},\n",
       "      {'answer_start': 171, 'text': 'Disney-ABC Television Group'}],\n",
       "     'id': '57267b755951b619008f7433',\n",
       "     'question': \"Quale società possiede l' American Broadcasting Company?\"},\n",
       "    {'answers': [{'answer_start': 74, 'text': '1957'},\n",
       "      {'answer_start': 74, 'text': '1957'},\n",
       "      {'answer_start': 74, 'text': '1957'}],\n",
       "     'id': '57267b755951b619008f7434',\n",
       "     'question': 'In quale anno ABC ha stilizzato il suo logo come abc?'},\n",
       "    {'answers': [{'answer_start': 394, 'text': 'Manhattan'},\n",
       "      {'answer_start': 394, 'text': 'Manhattan'},\n",
       "      {'answer_start': 394, 'text': 'Manhattan'}],\n",
       "     'id': '57267b755951b619008f7435',\n",
       "     'question': 'In quale quartiere di New York City ha sede ABC?'}]},\n",
       "  {'context': \"ABC originariamente lanciata il 12 ottobre 1943 come rete radio, separata da e come successore della NBC Blue Network, acquistata da Edward J. Noble. Nel 1948 ha esteso le sue attività alla televisione, seguendo le orme delle reti televisive consolidate CBS e NBC. A metà degli anni Cinquanta, ABC si è fusa con United Paramount Theatres, una catena di sale cinematografiche che un tempo operava come consociata di Paramount Pictures. Leonard Goldenson, che era stato il capo di UPT, ha reso redditizia la nuova rete televisiva contribuendo a sviluppare e greenlight molte serie di successo. Negli anni' 80, dopo aver acquisito una partecipazione dell' 80% nel canale sportivo via cavo ESPN, la società madre della rete si è fusa con Capital Cities Communications, proprietaria di diverse pubblicazioni cartacee, e con emittenti televisive e radiofoniche. Nel 1996, la maggior parte delle attività di Capital Cities/ABC sono state acquistate da The Walt Disney Company.\",\n",
       "   'qas': [{'answers': [{'answer_start': 32, 'text': '12 ottobre 1943'},\n",
       "      {'answer_start': 32, 'text': '12 ottobre 1943'},\n",
       "      {'answer_start': 32, 'text': '12 ottobre 1943'}],\n",
       "     'id': '57267ca75951b619008f7469',\n",
       "     'question': 'Quando ha iniziato ABC?'},\n",
       "    {'answers': [{'answer_start': 53, 'text': 'rete radio'},\n",
       "      {'answer_start': 58, 'text': 'radio'},\n",
       "      {'answer_start': 53, 'text': 'rete radio'}],\n",
       "     'id': '57267ca75951b619008f746a',\n",
       "     'question': \"Che tipo di rete era ABC all' inizio?\"},\n",
       "    {'answers': [{'answer_start': 154, 'text': '1948'},\n",
       "      {'answer_start': 154, 'text': '1948'},\n",
       "      {'answer_start': 154, 'text': '1948'}],\n",
       "     'id': '57267ca75951b619008f746b',\n",
       "     'question': 'Quando è entrato per la prima volta ABC nelle trasmissioni televisive?'},\n",
       "    {'answers': [{'answer_start': 686, 'text': 'ESPN'},\n",
       "      {'answer_start': 686, 'text': 'ESPN'},\n",
       "      {'answer_start': 686, 'text': 'ESPN'}],\n",
       "     'id': '57267ca75951b619008f746c',\n",
       "     'question': \"Negli anni' 80, quale canale sportivo via cavo ha acquistato ABC?\"},\n",
       "    {'answers': [{'answer_start': 734,\n",
       "       'text': 'Capital Cities Communications'},\n",
       "      {'answer_start': 734, 'text': 'Capital Cities Communications'},\n",
       "      {'answer_start': 734, 'text': 'Capital Cities Communications'}],\n",
       "     'id': '57267ca75951b619008f746d',\n",
       "     'question': \"Con quale società si è fusa negli anni' 80 la società madre della rete ABC?\"}]},\n",
       "  {'context': \"La rete televisiva ha otto stazioni televisive di proprietà e gestite e oltre 232 affiliate in tutti gli Stati Uniti e nei suoi territori. La maggior parte dei canadesi ha accesso ad almeno un' affiliata ABC con sede negli Stati Uniti, sia in via aerea (nelle zone situate in prossimità del confine Canada-Stati Uniti) che via cavo, satellite o IPTV, anche se la maggior parte dei programmi ABC sono soggetti a norme di sostituzione simultanea imposte dalla Commissione canadese per la radiotelevisione e le telecomunicazioni che consentono ai fornitori di televisione a pagamento di sostituire il segnale di una stazione americana con quello di un' emittente canadese. ABC News fornisce notizie e contenuti per alcune emittenti radiofoniche selezionate di proprietà di Citadel Broadcasting, che nel 2007 ha acquistato le proprietà di ABC Radio.\",\n",
       "   'qas': [{'answers': [{'answer_start': 78, 'text': '232'},\n",
       "      {'answer_start': 78, 'text': '232'},\n",
       "      {'answer_start': 72, 'text': 'oltre 232'}],\n",
       "     'id': '57267f1cdd62a815002e8740',\n",
       "     'question': 'Quante stazioni affiliate ABC ha attualmente ABC?'},\n",
       "    {'answers': [{'answer_start': 770, 'text': 'Citadel Broadcasting'},\n",
       "      {'answer_start': 770, 'text': 'Citadel Broadcasting'},\n",
       "      {'answer_start': 770, 'text': 'Citadel Broadcasting'}],\n",
       "     'id': '57267f1cdd62a815002e8741',\n",
       "     'question': 'Nel 2007, quale società ha acquistato gli immobili di ABC Radio?'},\n",
       "    {'answers': [{'answer_start': 22, 'text': 'otto'},\n",
       "      {'answer_start': 22, 'text': 'otto'},\n",
       "      {'answer_start': 22, 'text': 'otto'}],\n",
       "     'id': '57267f1cdd62a815002e8742',\n",
       "     'question': 'Quante stazioni della rete sono di proprietà e gestite?'},\n",
       "    {'answers': [{'answer_start': 458,\n",
       "       'text': 'Commissione canadese per la radiotelevisione e le telecomunicazioni'},\n",
       "      {'answer_start': 458,\n",
       "       'text': 'Commissione canadese per la radiotelevisione e le telecomunicazioni'},\n",
       "      {'answer_start': 458,\n",
       "       'text': 'Commissione canadese per la radiotelevisione e le telecomunicazioni'}],\n",
       "     'id': '57267f1cdd62a815002e8743',\n",
       "     'question': 'Quale entità in Canada gestisce i regolamenti di sostituzione per le trasmissioni televisive?'},\n",
       "    {'answers': [{'answer_start': 770, 'text': 'Citadel Broadcasting'},\n",
       "      {'answer_start': 770, 'text': 'Citadel Broadcasting'},\n",
       "      {'answer_start': 770, 'text': 'Citadel Broadcasting'}],\n",
       "     'id': '57267f1cdd62a815002e8744',\n",
       "     'question': \"ABC news fornisce contenuti per le emittenti radiofoniche di cui è proprietaria l' azienda?\"}]},\n",
       "  {'context': \"Negli anni' 30 negli Stati Uniti la radio era dominata da tre società: il Columbia Broadcasting System (CBS), il Mutual Broadcasting System e la National Broadcasting Company (NBC). Quest' ultima era di proprietà del produttore di elettronica Radio Corporation of America (RCA), che possedeva due reti radio che gestivano ciascuna diverse varietà di programmi, NBC Blue e NBC Red. Il NBC Blue Network è stato creato nel 1927 con lo scopo primario di testare nuovi programmi su mercati di minore importanza rispetto a quelli serviti da NBC Red, che serviva le principali città, e di testare serie teatrali.\",\n",
       "   'qas': [{'answers': [{'answer_start': 243,\n",
       "       'text': 'Radio Corporation of America'},\n",
       "      {'answer_start': 243, 'text': 'Radio Corporation of America (RCA)'},\n",
       "      {'answer_start': 243, 'text': 'Radio Corporation of America'}],\n",
       "     'id': '5726808bdd62a815002e8776',\n",
       "     'question': \"Quale società possedeva NBC negli anni' 30?\"},\n",
       "    {'answers': [{'answer_start': 361, 'text': 'NBC Blue'},\n",
       "      {'answer_start': 384, 'text': 'NBC Blue Network'},\n",
       "      {'answer_start': 384, 'text': 'NBC Blue Network'}],\n",
       "     'id': '5726808bdd62a815002e877a',\n",
       "     'question': 'Quale rete radio NBC è stata incaricata di testare nuovi programmi?'}]},\n",
       "  {'context': \"Nel 1934, Mutual ha presentato un reclamo alla Federal Communications Commission (FCC) in merito alle sue difficoltà nell' installazione di nuove stazioni, in un mercato radiofonico già saturo da NBC e CBS. Nel 1938, la FCC iniziò una serie di indagini sulle pratiche delle reti radiofoniche e nel 1940 pubblicò il suo rapporto sulla diffusione dei programmi radiofonici di rete. Il rapporto raccomandava all' RCA di rinunciare al controllo di NBC Red o NBC Blue. All' epoca, la NBC Red Network era la principale rete radiofonica degli Stati Uniti e, secondo la FCC, RCA utilizzava NBC Blue per eliminare qualsiasi accenno di concorrenza. Non disponendo di alcun potere sulle reti stesse, la FCC ha stabilito un regolamento che vieta il rilascio di licenze per le stazioni radio se queste sono affiliate a una rete già proprietaria di reti multiple che forniscono contenuti di interesse pubblico.\",\n",
       "   'qas': [{'answers': [{'answer_start': 10, 'text': 'Mutual'},\n",
       "      {'answer_start': 10, 'text': 'Mutual'}],\n",
       "     'id': '572681ab708984140094c85d',\n",
       "     'question': 'Qual è la società che ha presentato un reclamo alla FCC nel 1934 in merito a problemi per la creazione di nuove stazioni?'},\n",
       "    {'answers': [{'answer_start': 211, 'text': '1938'},\n",
       "      {'answer_start': 211, 'text': '1938'},\n",
       "      {'answer_start': 211, 'text': '1938'}],\n",
       "     'id': '572681ab708984140094c85e',\n",
       "     'question': \"In quale anno ha fatto la FCC ha iniziato un' indagine per il funzionamento delle reti radio in America?\"},\n",
       "    {'answers': [{'answer_start': 298, 'text': '1940'},\n",
       "      {'answer_start': 298, 'text': '1940'},\n",
       "      {'answer_start': 298, 'text': '1940'}],\n",
       "     'id': '572681ab708984140094c85f',\n",
       "     'question': \"Qual è stata la principale rete radio negli anni' 40 in America?\"},\n",
       "    {'answers': [{'answer_start': 479, 'text': 'NBC Red Network'},\n",
       "      {'answer_start': 479, 'text': 'NBC Red Network'},\n",
       "      {'answer_start': 479, 'text': 'NBC Red Network'}],\n",
       "     'id': '572681ab708984140094c860',\n",
       "     'question': 'Quale rete radio utilizzava RCA per eliminare la concorrenza nel 1940?'}]},\n",
       "  {'context': 'Una volta respinti i ricorsi di Mutual contro la FCC, RCA decise di vendere NBC Blue nel 1941, e diede il mandato di farlo a Mark Woods. L\\' 8 gennaio 1942 RCA ha convertito la NBC Blue Network in una controllata indipendente, divorzando formalmente le attività di NBC Red e NBC Blue l\\' 8 gennaio 1942, e la Blue Network viene chiamata \"Blue\" o \"Blue Network\". I neo separati NBC Red e NBC Blue hanno diviso le rispettive attività societarie. Tra il 1942 e il 1943, Woods offrì di vendere l\\' intero NBC Blue Network, un pacchetto che comprendeva locazioni di rete fissa, tre licenze televisive in sospeso (WJZ-TV a New York City, KGO-TV a San Francisco e WENR-TV a Chicago), 60 filiali, quattro strutture operative (a New York City, Chicago, Los Angeles e Washington D. C.), contratti con attori e il marchio associato al Blue Network. La società di investimento Dillon, Read & Co. (acquistata successivamente dalla Swiss Bank Corporation nel 1997) ha offerto 7,5 milioni di dollari per l\\' acquisto della rete, ma l\\' offerta è stata respinta da Woods e dal presidente della RCA David Sarnoff.',\n",
       "   'qas': [{'answers': [{'answer_start': 125, 'text': 'Mark Woods'},\n",
       "      {'answer_start': 125, 'text': 'Mark Woods'}],\n",
       "     'id': '572684f5dd62a815002e87fc',\n",
       "     'question': 'A chi la RCA ha dato mandato alla vendita di NBC blu nel 1941?'},\n",
       "    {'answers': [{'answer_start': 176, 'text': 'NBC Blue Network'},\n",
       "      {'answer_start': 176, 'text': 'NBC Blue Network'},\n",
       "      {'answer_start': 176, 'text': 'NBC Blue Network'}],\n",
       "     'id': '572684f5dd62a815002e87fd',\n",
       "     'question': 'Quale rete è stata trasformata da RCA in filiale indipendente nel 1942?'},\n",
       "    {'answers': [{'answer_start': 862, 'text': 'Dillon, Read & Co'},\n",
       "      {'answer_start': 862, 'text': 'Dillon, Read & Co'},\n",
       "      {'answer_start': 862, 'text': 'Dillon, Read & Co'}],\n",
       "     'id': '572684f5dd62a815002e87fe',\n",
       "     'question': 'Quale società di investimento si è offerta di acquistare la rete NBC Blue da Mark Woods?'},\n",
       "    {'answers': [{'answer_start': 1077, 'text': 'David Sarnoff'},\n",
       "      {'answer_start': 1077, 'text': 'David Sarnoff'},\n",
       "      {'answer_start': 1077, 'text': 'David Sarnoff'}],\n",
       "     'id': '572684f5dd62a815002e87ff',\n",
       "     'question': 'Chi era presidente della RCA nel 1942?'},\n",
       "    {'answers': [{'answer_start': 959, 'text': '7,5 milioni di dollari'},\n",
       "      {'answer_start': 959, 'text': '7,5 milioni di dollari'},\n",
       "      {'answer_start': 959, 'text': '7,5 milioni di dollari'}],\n",
       "     'id': '572684f5dd62a815002e8800',\n",
       "     'question': 'Quanto denaro ha offerto Dillon, Read & Co a Mark Woods per NBC Blue?'}]},\n",
       "  {'context': 'Edward John Noble, proprietario di Life Savers caramelle, catena di farmacie Rexall e stazione radio di New York City WMCA, ha acquistato la rete per 8 milioni di dollari. A causa delle regole di proprietà dell\\' FCC, la transazione, che doveva includere l\\' acquisto di tre stazioni RCA da parte di Noble, gli avrebbe richiesto di rivendere la sua stazione con l\\' approvazione della FCC. La Commissione ha autorizzato l\\' operazione il 12 ottobre 1943. Poco dopo, la Blue Network è stata acquistata dalla nuova società Noble fondata, l\\' American Broadcasting System. Nel 1944 Noble acquistò successivamente i diritti della \"American Broadcasting Company\" da George B. Storer nel 1944; la sua società madre adottò la denominazione sociale American Broadcasting Companies, Inc. Woods mantenne la sua carica di presidente e CEO di ABC fino al dicembre 1949, e fu successivamente promossa a vice-presidente del consiglio di amministrazione prima di lasciare completamente ABC il 30 giugno 1951.',\n",
       "   'qas': [{'answers': [{'answer_start': 35, 'text': 'Life Savers'},\n",
       "      {'answer_start': 35, 'text': 'Life Savers'}],\n",
       "     'id': '572685d1f1498d1400e8e29e',\n",
       "     'question': 'Quale società di caramelle ha posseduto Edward John Noble?'},\n",
       "    {'answers': [{'answer_start': 434, 'text': '12 ottobre 1943'},\n",
       "      {'answer_start': 434, 'text': '12 ottobre 1943'},\n",
       "      {'answer_start': 434, 'text': '12 ottobre 1943'}],\n",
       "     'id': '572685d1f1498d1400e8e29f',\n",
       "     'question': 'Quando è stata autorizzata la vendita di NBC Blue a Edward John Noble?'},\n",
       "    {'answers': [{'answer_start': 656, 'text': 'George B. Storer'},\n",
       "      {'answer_start': 656, 'text': 'George B. Storer'}],\n",
       "     'id': '572685d1f1498d1400e8e2a0',\n",
       "     'question': 'Da chi Noble ha acquisito i diritti della società televisiva americana?'},\n",
       "    {'answers': [{'answer_start': 806, 'text': 'presidente e CEO'},\n",
       "      {'answer_start': 806, 'text': 'presidente e CEO'},\n",
       "      {'answer_start': 806, 'text': 'presidente'}],\n",
       "     'id': '572685d1f1498d1400e8e2a1',\n",
       "     'question': 'Quale posizione ha assunto Mark Woods presso la nuova emittente televisiva americana?'},\n",
       "    {'answers': [{'answer_start': 973, 'text': '30 giugno 1951'},\n",
       "      {'answer_start': 983, 'text': '1951'},\n",
       "      {'answer_start': 973, 'text': '30 giugno 1951'}],\n",
       "     'id': '572685d1f1498d1400e8e2a2',\n",
       "     'question': 'Quando Mark Woods ha lasciato ABC?'}]},\n",
       "  {'context': \"ABC è diventato un concorrente aggressivo di NBC e CBS quando, continuando le tradizioni di servizio pubblico di NBC Blue, ha trasmesso spettacoli sinfonici diretti da Paul Whiteman, spettacoli di Metropolitan Opera e concerti jazz in onda nell' ambito della sua trasmissione di The Chamber Music Society of Lower Basin Street annunciata da Milton Cross. La rete è anche diventato noto per drammi sospensivi come Sherlock Holmes, Gang Busters e Counterspy, così come diversi programmi pomeridiani a metà pomeriggio orientata ai giovani. Tuttavia, ABC si è fatta un nome per se stessa utilizzando la pratica di controprogrammazione, con la quale ha spesso messo le proprie mostre contro le offerte di NBC e CBS, adottando l' uso del registratore a nastro Magnetophon, portato negli Stati Uniti dalla Germania nazista dopo la sua conquista, per pre-registrare la sua programmazione. Con l' aiuto del Magnetophon, ABC è stata in grado di fornire alle sue stelle una maggiore libertà in termini di tempo, e di attrarre anche diversi grandi nomi, come Bing Crosby in un momento in cui NBC e CBS non ha permesso pre-natturati spettacoli. Magnetofon registratore a nastro.\",\n",
       "   'qas': [{'answers': [{'answer_start': 1132, 'text': 'Magnetofon'},\n",
       "      {'answer_start': 1132, 'text': 'Magnetofon registratore a nastro'}],\n",
       "     'id': '57268739708984140094c8ed',\n",
       "     'question': 'Quale nuova tecnologia ha permesso ad ABC di preregistrare i suoi spettacoli?'},\n",
       "    {'answers': [{'answer_start': 0, 'text': 'ABC'},\n",
       "      {'answer_start': 0, 'text': 'ABC'},\n",
       "      {'answer_start': 168, 'text': 'Paul Whiteman'}],\n",
       "     'id': '57268739708984140094c8ee',\n",
       "     'question': 'Quale rete era nota per drammi come Sherlock Holmes?'},\n",
       "    {'answers': [{'answer_start': 1047, 'text': 'Bing Crosby'},\n",
       "      {'answer_start': 1047, 'text': 'Bing Crosby'},\n",
       "      {'answer_start': 1047, 'text': 'Bing Crosby'}],\n",
       "     'id': '57268739708984140094c8ef',\n",
       "     'question': 'Chi era una grande stella che ABC era in grado di attrarre a causa della tecnologia Magnetophon?'},\n",
       "    {'answers': [{'answer_start': 92, 'text': 'servizio pubblico'},\n",
       "      {'answer_start': 92, 'text': 'servizio pubblico'},\n",
       "      {'answer_start': 92, 'text': 'servizio pubblico'}],\n",
       "     'id': '57268739708984140094c8f0',\n",
       "     'question': 'ABC ha continuato la tradizione della NBC Blue di cosa?'}]},\n",
       "  {'context': \"Un secondo periodo di espansione internazionale è legato a quello della rete ESPN negli anni' 90, e alle politiche varate nel 2000 dalla Disney Media Networks (che includeva l' espansione di alcune delle reti via cavo statunitensi della società, tra cui Disney Channel e le sue spin-off Toon Disney, Playhouse Disney e Jetix; sebbene nel giugno 2000 Disney abbia venduto anche la sua partecipazione del 33% nel canale sportivo europeo Eurosport per 155 milioni di dollari USA. A differenza degli altri canali di Disney, ABC è trasmesso negli Stati Uniti, anche se la programmazione della rete è sindacato in molti paesi. La politica relativa alle reti internazionali interamente possedute è stata rilanciata nel 2004 quando, il 27 settembre di quell' anno, ABC ha annunciato il lancio di ABC1, un canale in chiaro nel Regno Unito di proprietà del Gruppo ABC. Tuttavia, l' 8 settembre 2007, Disney ha annunciato che avrebbe interrotto ABC1 citando l' incapacità del canale di raggiungere audience sostenibile. Con la chiusura di ABC1 in ottobre, il tentativo dell' azienda di sviluppare ABC International è stato interrotto.\",\n",
       "   'qas': [{'answers': [{'answer_start': 449,\n",
       "       'text': '155 milioni di dollari'},\n",
       "      {'answer_start': 449, 'text': '155 milioni di dollari'},\n",
       "      {'answer_start': 449, 'text': '155 milioni di dollari'}],\n",
       "     'id': '57269260dd62a815002e89ea',\n",
       "     'question': 'Quanto ha venduto Disney la sua partecipazione in Eurosport nel 2000?'},\n",
       "    {'answers': [{'answer_start': 788, 'text': 'ABC1'},\n",
       "      {'answer_start': 788, 'text': 'ABC1'},\n",
       "      {'answer_start': 788, 'text': 'ABC1'}],\n",
       "     'id': '57269260dd62a815002e89eb',\n",
       "     'question': 'Quale rete è stata lanciata da ABC nel 2004?'},\n",
       "    {'answers': [{'answer_start': 872, 'text': '8 settembre 2007'},\n",
       "      {'answer_start': 872, 'text': '8 settembre 2007'},\n",
       "      {'answer_start': 872, 'text': '8 settembre 2007'}],\n",
       "     'id': '57269260dd62a815002e89ec',\n",
       "     'question': \"Quando è stato interrotto l' ABC1 a causa della scarsa visibilità?\"},\n",
       "    {'answers': [{'answer_start': 1086, 'text': 'ABC International'},\n",
       "      {'answer_start': 1086, 'text': 'ABC International'},\n",
       "      {'answer_start': 1086, 'text': 'ABC International'}],\n",
       "     'id': '57269260dd62a815002e89ed',\n",
       "     'question': 'Quale rete di sviluppo è stata interrotta dopo la chiusura di ABC1?'},\n",
       "    {'answers': [{'answer_start': 542, 'text': 'Stati Uniti'},\n",
       "      {'answer_start': 542, 'text': 'Stati Uniti'},\n",
       "      {'answer_start': 542, 'text': 'Stati Uniti'}],\n",
       "     'id': '57269260dd62a815002e89ee',\n",
       "     'question': 'In quale paese sta trasmettendo ABC, contrariamente agli altri canali di Disney?'}]},\n",
       "  {'context': \"L' idea era quella di creare una rete di canali interamente e parzialmente di proprietà e affiliati per ritrasmettere i programmi della rete. Nel 1959, questa attività viene completata con la sindacazione del programma, con la vendita di programmi ABC Films a reti non di proprietà di ABC. L' arrivo della televisione satellitare ha posto fine alla necessità per ABC di detenere interessi in altri paesi; molti governi volevano anche aumentare la loro indipendenza e rafforzare la legislazione per limitare la proprietà straniera delle proprietà di trasmissione radiotelevisiva. Di conseguenza, ABC è stata costretta a cedere tutte le sue partecipazioni in reti internazionali, principalmente in Giappone e in America Latina, negli anni' 70.\",\n",
       "   'qas': [{'answers': [{'answer_start': 146, 'text': '1959'},\n",
       "      {'answer_start': 146, 'text': '1959'},\n",
       "      {'answer_start': 146, 'text': '1959'}],\n",
       "     'id': '57269344f1498d1400e8e43e',\n",
       "     'question': 'Quando ABC Films ha iniziato a vendere programmi ad altre reti?'},\n",
       "    {'answers': [{'answer_start': 306, 'text': 'televisione satellitare'},\n",
       "      {'answer_start': 290, 'text': \"L' arrivo della televisione satellitare\"},\n",
       "      {'answer_start': 306, 'text': 'televisione satellitare'}],\n",
       "     'id': '57269344f1498d1400e8e43f',\n",
       "     'question': 'Che cosa ha posto fine alla necessità che ABC mantenesse gli interessi in altri paesi?'},\n",
       "    {'answers': [{'answer_start': 409,\n",
       "       'text': 'i governi volevano anche aumentare la loro indipendenza e rafforzare la legislazione'}],\n",
       "     'id': '57269344f1498d1400e8e441',\n",
       "     'question': \"Perchè negli anni' 70 ABC è stata costretta a vendere i suoi interessi nelle reti internazionali?\"}]},\n",
       "  {'context': \"I primi tentativi di internazionalizzare il network televisivo ABC risalgono agli anni Cinquanta, dopo che Leonard Goldenson, seguendo il modello United Paramount Theatres, cercò di utilizzare su ABC le stesse strategie che aveva messo in atto per espandere l' attività teatrale di UPT al mercato internazionale. Leonard Goldenson ha detto che la prima attività internazionale di ABC stava trasmettendo l' incoronazione della regina Elisabetta II nel giugno 1953; CBS e NBC non sono stati in grado di coprire l' incoronazione dal vivo a causa di problemi con problemi tecnici e ritardi di volo. L' aereo della NBC è atterrato in America Latina[dove? che porta ABC a conoscere le filiali in quella regione. Goldenson ha tentato di investire a livello internazionale, avendo ABC investito nel mercato latinoamericano, acquisendo il 51% di una rete che copre l' America Centrale. Goldenson ha citato anche l' interesse in Giappone nei primi anni Cinquanta, acquisendo una partecipazione del 5% in due nuove reti domestiche, il Mainichi Broadcasting System nel 1951 e Nihon Educational Television nel 1957. A metà degli anni Sessanta Goldenson ha anche investito in proprietà di trasmissione a Beirut.\",\n",
       "   'qas': [{'answers': [{'answer_start': 406,\n",
       "       'text': 'incoronazione della regina Elisabetta II'},\n",
       "      {'answer_start': 403,\n",
       "       'text': \"l' incoronazione della regina Elisabetta II\"},\n",
       "      {'answer_start': 403,\n",
       "       'text': \"l' incoronazione della regina Elisabetta II\"}],\n",
       "     'id': '5726caaaf1498d1400e8eb5c',\n",
       "     'question': 'Qual è stato il primo evento internazionale trasmesso da ABC?'},\n",
       "    {'answers': [{'answer_start': 1190, 'text': 'Beirut'},\n",
       "      {'answer_start': 1190, 'text': 'Beirut'},\n",
       "      {'answer_start': 1190, 'text': 'Beirut'}],\n",
       "     'id': '5726caaaf1498d1400e8eb5d',\n",
       "     'question': \"In quale paese ABC si è espansa verso la metà degli anni' 60?\"},\n",
       "    {'answers': [{'answer_start': 1024,\n",
       "       'text': 'Mainichi Broadcasting System'},\n",
       "      {'answer_start': 1024, 'text': 'Mainichi Broadcasting System'},\n",
       "      {'answer_start': 1024, 'text': 'Mainichi Broadcasting System'}],\n",
       "     'id': '5726caaaf1498d1400e8eb5e',\n",
       "     'question': 'Quale rete giapponese ABC ha acquistato una partecipazione nel 1951?'},\n",
       "    {'answers': [{'answer_start': 578, 'text': 'ritardi di volo'}],\n",
       "     'id': '5726caaaf1498d1400e8eb5f',\n",
       "     'question': \"Perchè la NBC non è stata in grado di trasmettere l' incoronazione della regina Elisabetta II?\"},\n",
       "    {'answers': [{'answer_start': 559, 'text': 'problemi tecnici'}],\n",
       "     'id': '5726caaaf1498d1400e8eb60',\n",
       "     'question': \"Perchè la CBS non è stata in grado di trasmettere l' incoronazione della regina Elisabetta II?\"}]},\n",
       "  {'context': \"ABC detiene attualmente i diritti di trasmissione degli Academy Awards, Emmy Awards (che ruotano su tutti e quattro i principali network su base annuale), American Music Awards, Disney Parks Christmas Day Parade, Tournament of Roses Parade, Country Music Association Awards e il CMA Music Festival. Dal 2000, ABC possiede anche i diritti televisivi per la maggior parte degli speciali televisivi delle arachidi, avendo acquisito i diritti di trasmissione da CBS, che ha dato origine agli speciali nel 1965 con l' esordio di A Charlie Brown Christmas (altri speciali delle arachidi trasmessi annualmente da ABC, tra cui A Charlie Brown Christmas, includono It's the Great Pumpkin, Charlie Brown e A Charlie Brown Ringraziamento).\",\n",
       "   'qas': [{'answers': [{'answer_start': 402, 'text': 'arachidi'},\n",
       "      {'answer_start': 402, 'text': 'arachidi'},\n",
       "      {'answer_start': 402, 'text': 'arachidi'}],\n",
       "     'id': '5726e5ac708984140094d51b',\n",
       "     'question': 'Quali sono state le offerte speciali televisive che ABC ha acquisito nel 2000?'},\n",
       "    {'answers': [{'answer_start': 72, 'text': 'Emmy Awards'},\n",
       "      {'answer_start': 72, 'text': 'Emmy Awards'},\n",
       "      {'answer_start': 72, 'text': 'Emmy Awards'}],\n",
       "     'id': '5726e5ac708984140094d51c',\n",
       "     'question': 'Quale riconoscimento ha i suoi diritti ruotato ogni anno tra le quattro principali reti?'},\n",
       "    {'answers': [{'answer_start': 501, 'text': '1965'},\n",
       "      {'answer_start': 501, 'text': '1965'},\n",
       "      {'answer_start': 501, 'text': '1965'}],\n",
       "     'id': '5726e5ac708984140094d51d',\n",
       "     'question': 'Quando ha fatto un debutto di Natale Charlie Brown?'},\n",
       "    {'answers': [{'answer_start': 52, 'text': 'gli Academy Awards'},\n",
       "      {'answer_start': 56, 'text': 'Academy Awards'},\n",
       "      {'answer_start': 56, 'text': 'Academy Awards, Emmy Awards'}],\n",
       "     'id': '5726e5ac708984140094d51e',\n",
       "     'question': 'Quali sono i premi cinematografici che ABC detiene attualmente?'},\n",
       "    {'answers': [{'answer_start': 656, 'text': \"It's the Great Pumpkin\"},\n",
       "      {'answer_start': 656, 'text': \"It's the Great Pumpkin, Charlie Brown\"},\n",
       "      {'answer_start': 656, 'text': \"It's the Great Pumpkin\"}],\n",
       "     'id': '5726e5ac708984140094d51f',\n",
       "     'question': 'Quali sono le arachidi speciali a tema Halloween?'}]},\n",
       "  {'context': \"Dal 1974, ABC ha in genere trasmesso il Capodanno Rockin' Eva di Dick Clark a Capodanno (ospitato prima dal suo creatore Dick Clark, e poi dal suo successore Ryan Seacrest); l' unica eccezione è stata nel 1999, quando ABC ha messo su un anno di pausa per fornire copertura delle feste del millennio internazionale, anche se il conto alla rovescia tradizionale di Clark da Times Square era ancora presente in scena. ABC ha anche trasmesso in onda la sfilata Miss America dal 1954 al 1956,1997-2005 (con i diritti televisivi assunti dal canale via cavo TLC nel 2006, quando la sfilata si è trasferita dalla sua lunga sede a Atlantic City a Las Vegas, prima di tornare a Atlantic City nel 2013) e dal 2011. Con l' attuale contratto con la Miss America Organization, ABC continuerà a trasmettere il concorso fino al 2016.\",\n",
       "   'qas': [{'answers': [{'answer_start': 4, 'text': '1974'},\n",
       "      {'answer_start': 4, 'text': '1974'}],\n",
       "     'id': '5726e5b1f1498d1400e8ef30',\n",
       "     'question': \"Quando ha iniziato a trasmettere il Capodanno Rockin' Eve di Dick Clark?\"},\n",
       "    {'answers': [{'answer_start': 158, 'text': 'Ryan Seacrest'},\n",
       "      {'answer_start': 158, 'text': 'Ryan Seacrest'},\n",
       "      {'answer_start': 158, 'text': 'Ryan Seacrest'}],\n",
       "     'id': '5726e5b1f1498d1400e8ef31',\n",
       "     'question': 'Chi è succeduto a Dick Clark sulle trasmissioni del Capodanno di ABC?'},\n",
       "    {'answers': [{'answer_start': 474, 'text': '1954'},\n",
       "      {'answer_start': 474, 'text': '1954'},\n",
       "      {'answer_start': 474, 'text': '1954'}],\n",
       "     'id': '5726e5b1f1498d1400e8ef32',\n",
       "     'question': 'Quando ha iniziato la trasmissione di Miss America Pageant?'},\n",
       "    {'answers': [{'answer_start': 372, 'text': 'Times Square'},\n",
       "      {'answer_start': 372, 'text': 'Times Square'},\n",
       "      {'answer_start': 40, 'text': 'Capodanno'}],\n",
       "     'id': '5726e5b1f1498d1400e8ef33',\n",
       "     'question': \"Dove era ospitato il Capodanno Rockin' Eve?\"},\n",
       "    {'answers': [{'answer_start': 551, 'text': 'TLC'},\n",
       "      {'answer_start': 551, 'text': 'TLC'},\n",
       "      {'answer_start': 10, 'text': 'ABC'}],\n",
       "     'id': '5726e5b1f1498d1400e8ef34',\n",
       "     'question': 'Quale rete ha assunto i diritti di Miss America Pageant nel 2006?'}]},\n",
       "  {'context': \"Il programma diurno di ABC attualmente prevede talk show The View e The Chew, e il soap opera General Hospital, quest' ultimo è il più lungo programma di intrattenimento nella storia della rete televisiva ABC, in onda dal 1963. L' ABC trasmette anche il notiziario mattutino Good Morning America e lo ha fatto dal 1975, anche se questo programma non è considerato parte del blocco ABC Daytime. Oltre alle celeberrime All My Children (1970-2011) e One Life to Live (1968-2012), notevoli sono le soap opera passate viste sulla scaletta diurna di Ryan's Hope, Dark Shadows, Loving, Loving, The City e Port Charles. ABC ha inoltre trasmesso gli ultimi nove anni di produzione del sapone Procter & Gamble The Edge of Night, dopo la sua cancellazione da parte della CBS nel 1975. ABC Daytime ha anche trasmesso un certo numero di spettacoli di gioco, tra cui The Dating Game, The Newlywed Game, Let's Make a Deal, Password, Split Second, The $10.000/$20.000 Pyramid, Family Feud, The Better Sex, Trivia Trap, All-Star Blitz e Hot Streak.\",\n",
       "   'qas': [{'answers': [{'answer_start': 94, 'text': 'General Hospital'},\n",
       "      {'answer_start': 94, 'text': 'General Hospital'},\n",
       "      {'answer_start': 94, 'text': 'General Hospital'}],\n",
       "     'id': '5726e671dd62a815002e9464',\n",
       "     'question': 'Quale serie è il programma più lungo della storia ABC?'},\n",
       "    {'answers': [{'answer_start': 314, 'text': '1975'},\n",
       "      {'answer_start': 314, 'text': '1975'},\n",
       "      {'answer_start': 314, 'text': '1975'}],\n",
       "     'id': '5726e671dd62a815002e9465',\n",
       "     'question': 'Quando ha iniziato la diffusione di Good Morning America?'},\n",
       "    {'answers': [{'answer_start': 700, 'text': 'The Edge of Night'},\n",
       "      {'answer_start': 700, 'text': 'The Edge of Night'},\n",
       "      {'answer_start': 700, 'text': 'The Edge of Night'}],\n",
       "     'id': '5726e671dd62a815002e9466',\n",
       "     'question': 'Quale Proctor and Gamble ha prodotto la soap opera di ABC air?'},\n",
       "    {'answers': [{'answer_start': 222, 'text': '1963'},\n",
       "      {'answer_start': 222, 'text': '1963'},\n",
       "      {'answer_start': 222, 'text': '1963'}],\n",
       "     'id': '5726e671dd62a815002e9468',\n",
       "     'question': \"Quando ha iniziato l' aerazione del General Hospital?\"}]},\n",
       "  {'context': 'La programmazione sportiva è prevista anche per alcuni pomeriggi del fine settimana in qualsiasi momento dalle 12:00 alle 18:00 del pomeriggio (ora orientale dalle 9:00 alle 15:00 del Pacifico) e, durante la stagione calcistica del college, durante la prima serata del sabato sera nell\\' ambito del pacchetto Calcio Sabato Notte. A causa della programmazione erratica e (fuori della stagione calcistica del college) molto incoerente programmazione della programmazione sportiva nei pomeriggi del fine settimana da quando ESPN ha assunto le responsabilità per la divisione sportiva di ABC nel 2006, ABC trasporta il blocco ESPN Sports Sabato sabato pomeriggio di fine sabato (con vari documentari ESPN-prodotti), e la domenica o codifica di serie realtà primetime, serie annullata essere bruciato fuori che non aveva spazio sul programma di primatime,. Durante l\\' estate, ABC airs ESPN ha prodotto programmi di compilazione di highlight per gli Open Championship golf e i tornei di tennis di Wimbledon per fornire una certa presenza per entrambi gli eventi in trasmissione televisiva americana. ABC trasporta anche vari eventi del fine settimana di X Games non trasmessi da ESPN. L\\' ABC trasmette i giochi NBA la domenica, normalmente a partire da gennaio come \"NBA Sunday Showcase\" durante la stagione regolare, e mostra i giochi del giorno di Natale, regolarmente tra le 14.00 e le 14.00 ET, e i giochi di playoff NBA durante il fine settimana, e i diritti esclusivi per le finali NBA. X Giochi. X Giochi. X Giochi. L\\' ABC organizza eventi del fine settimana per le competizioni sportive estreme? 2006.',\n",
       "   'qas': [{'answers': [{'answer_start': 591, 'text': '2006'},\n",
       "      {'answer_start': 591, 'text': '2006'}],\n",
       "     'id': '5726e773f1498d1400e8ef6e',\n",
       "     'question': \"Quando l' ESPN ha assunto la responsabilità della divisione sportiva di ABC?\"},\n",
       "    {'answers': [{'answer_start': 1204, 'text': 'NBA'},\n",
       "      {'answer_start': 1204, 'text': 'NBA'},\n",
       "      {'answer_start': 1204, 'text': 'NBA'}],\n",
       "     'id': '5726e773f1498d1400e8ef6c',\n",
       "     'question': 'Per che campionato fa giochi ABC trasmettere il giorno di Natale?'}]},\n",
       "  {'context': \"Mentre la sua rete radio era in fase di ricostruzione, ABC ha avuto difficoltà ad evitare di ritrovarsi in ritardo sul nuovo mezzo televisivo. Per garantire uno spazio, nel 1947 ABC ha presentato cinque domande di licenza per le emittenti televisive, una per ogni mercato in cui possedeva e gestiva una stazione radio (New York City, Los Angeles, Chicago, San Francisco e Detroit). Tutte queste applicazioni richiedevano che le stazioni trasmettessero sul canale VHF 7, come ha affermato Frank Marx, allora vicepresidente dell' ingegneria di ABC, ritenendo che le frequenze VHF a banda bassa (corrispondenti ai canali da 2 a 6) sarebbero state richieste dall' esercito statunitense e riassegnate. Frank Marx.\",\n",
       "   'qas': [{'answers': [{'answer_start': 488, 'text': 'Frank Marx'},\n",
       "      {'answer_start': 488, 'text': 'Frank Marx'}],\n",
       "     'id': '5726e860708984140094d579',\n",
       "     'question': \"Chi era il vicepresidente di ABC per l' ingegneria?\"},\n",
       "    {'answers': [{'answer_start': 173, 'text': '1947'},\n",
       "      {'answer_start': 173, 'text': '1947'},\n",
       "      {'answer_start': 173, 'text': '1947'}],\n",
       "     'id': '5726e860708984140094d57a',\n",
       "     'question': 'In quale anno ABC ha presentato licenze per 5 emittenti televisive?'},\n",
       "    {'answers': [{'answer_start': 173, 'text': '1947'},\n",
       "      {'answer_start': 173, 'text': '1947'},\n",
       "      {'answer_start': 173, 'text': '1947'}],\n",
       "     'id': '5726e860708984140094d57b',\n",
       "     'question': 'Quale canale ha richiesto la trasmissione delle 5 applicazioni?'}]},\n",
       "  {'context': \"Nell' autunno del 1949, ABC si trovò nella posizione di un outsider, con una copertura inferiore a due delle sue reti concorrenti, CBS e NBC, anche se era alla pari con loro in alcune grandi città e aveva un colpo di testa oltre il suo terzo rivale all' epoca, la DuMont Television Network. Prima della fine del congelamento nel 1952, negli Stati Uniti esistevano solo 108 stazioni televisive esistenti; poche grandi città (come Boston) avevano solo due stazioni televisive, molte altre città (come Pittsburgh e St. Louis) ne avevano una sola, e ancora molte altre (come Denver e Portland) non avevano ancora alcun servizio televisivo. Il risultato è stato uno strano periodo in cui la televisione fiorì in alcune zone e la radio di rete rimase la principale fonte di intrattenimento televisivo e di notizie in altre.\",\n",
       "   'qas': [{'answers': [{'answer_start': 369, 'text': '108'},\n",
       "      {'answer_start': 369, 'text': '108'},\n",
       "      {'answer_start': 369, 'text': '108'}],\n",
       "     'id': '5726e942f1498d1400e8efa0',\n",
       "     'question': 'Nel 1952, quante sono state le emittenti televisive negli Stati Uniti?'},\n",
       "    {'answers': [{'answer_start': 99, 'text': 'due'},\n",
       "      {'answer_start': 99, 'text': 'due'},\n",
       "      {'answer_start': 99, 'text': 'due'}],\n",
       "     'id': '5726e942f1498d1400e8efa1',\n",
       "     'question': 'Quante stazioni aveva Boston nel 1952?'},\n",
       "    {'answers': [{'answer_start': 264, 'text': 'DuMont Television Network'},\n",
       "      {'answer_start': 264, 'text': 'DuMont Television Network'},\n",
       "      {'answer_start': 264, 'text': 'DuMont Television Network'}],\n",
       "     'id': '5726e942f1498d1400e8efa2',\n",
       "     'question': 'Chi era il terzo grande rivale di ABC nel 1949?'},\n",
       "    {'answers': [{'answer_start': 131, 'text': 'CBS e NBC'},\n",
       "      {'answer_start': 131, 'text': 'CBS e NBC'},\n",
       "      {'answer_start': 131, 'text': 'CBS e NBC'}],\n",
       "     'id': '5726e942f1498d1400e8efa3',\n",
       "     'question': 'Nel 1949, ABC aveva una copertura inferiore a quella delle reti concorrenti?'}]},\n",
       "  {'context': \"Alla fine del 1949, l' operatore cinematografico United Paramount Theatres (UPT) fu costretto dalla Corte Suprema degli Stati Uniti a diventare un' entità indipendente, separandosi da Paramount Pictures. Da parte sua, ABC era sull' orlo del fallimento, con solo cinque stazioni di proprietà e gestite e nove affiliate a tempo pieno. I suoi ricavi, che si riferiscono alla pubblicità e sono stati indicizzati rispetto al numero di ascoltatori/visitatori, non sono stati in grado di compensare i suoi forti investimenti negli acquisti e nelle stazioni di costruzione. Nel 1951, una voce diceva addirittura che la rete sarebbe stata venduta alla CBS. Nel 1951, Noble deteneva una partecipazione del 58% in ABC, dandogli 5 milioni di dollari per evitare il fallimento di ABC; poichè le banche rifiutavano un ulteriore credito, quell' importo era stato ottenuto attraverso un prestito della Prudential Insurance Company of America. Corte Suprema degli Stati Uniti.\",\n",
       "   'qas': [{'answers': [{'answer_start': 100,\n",
       "       'text': 'Corte Suprema degli Stati Uniti'}],\n",
       "     'id': '5726ea06dd62a815002e950a',\n",
       "     'question': \"Nel 1949, UPT è stata costretta a diventare un' entità indipendente da chi?\"},\n",
       "    {'answers': [{'answer_start': 303, 'text': 'nove'},\n",
       "      {'answer_start': 303, 'text': 'nove'},\n",
       "      {'answer_start': 303, 'text': 'nove'}],\n",
       "     'id': '5726ea06dd62a815002e950b',\n",
       "     'question': 'Quanti affiliati ha avuto ABC nel 1949?'},\n",
       "    {'answers': [{'answer_start': 643, 'text': 'CBS'},\n",
       "      {'answer_start': 643, 'text': 'CBS'},\n",
       "      {'answer_start': 643, 'text': 'CBS'}],\n",
       "     'id': '5726ea06dd62a815002e950c',\n",
       "     'question': 'Nel 1951, voci affermano che ABC potrebbe essere venduto a quale rete?'},\n",
       "    {'answers': [{'answer_start': 886,\n",
       "       'text': 'Prudential Insurance Company of America'},\n",
       "      {'answer_start': 886, 'text': 'Prudential Insurance Company of America'},\n",
       "      {'answer_start': 886,\n",
       "       'text': 'Prudential Insurance Company of America'}],\n",
       "     'id': '5726ea06dd62a815002e950d',\n",
       "     'question': 'Noble ha acquistato un prestito da quale entità per mantenere ABC solvente nel 1951?'}]},\n",
       "  {'context': \"Leonard Goldenson, il presidente di UPT (che all' epoca cercò di diversificarsi), si avvicinò a Noble nel 1951 su una proposta per l' acquisto di ABC da parte di UPT. Noble ha ricevuto altre offerte, tra cui una dal fondatore della CBS William S. Paley; tuttavia, una fusione con la CBS avrebbe costretto tale rete a vendere quanto meno le sue stazioni di New York City e Los Angeles. Goldenson e Noble raggiunsero un accordo provvisorio nella tarda primavera del 1951 in cui UPT acquistò ABC e la trasformò in una controllata della società che avrebbe mantenuto l' autonomia nella sua gestione. Il 6 giugno 1951 il consiglio di amministrazione di UPT approvò l' accordo provvisorio. Tuttavia, l' operazione ha dovuto essere approvata dalla FCC a causa della presenza di reti televisive e della recente separazione tra Paramount e UPT. Nella misura in cui Paramount Pictures era già azionista della DuMont Television Network, la FCC ha condotto una serie di audizioni per verificare se la Paramount fosse realmente separata dalle United Paramount Theatres e se stesse violando le leggi antitrust.\",\n",
       "   'qas': [{'answers': [{'answer_start': 0, 'text': 'Leonard Goldenson'},\n",
       "      {'answer_start': 0, 'text': 'Leonard Goldenson'},\n",
       "      {'answer_start': 0, 'text': 'Leonard Goldenson'}],\n",
       "     'id': '5726ec6ff1498d1400e8eff2',\n",
       "     'question': \"Chi era presidente dell' UPT nel 1951?\"},\n",
       "    {'answers': [{'answer_start': 236, 'text': 'William S. Paley'},\n",
       "      {'answer_start': 236, 'text': 'William S. Paley'}],\n",
       "     'id': '5726ec6ff1498d1400e8eff3',\n",
       "     'question': 'Chi è stato il fondatore della CBS?'},\n",
       "    {'answers': [{'answer_start': 599, 'text': '6 giugno 1951'},\n",
       "      {'answer_start': 599, 'text': '6 giugno 1951'},\n",
       "      {'answer_start': 599, 'text': '6 giugno 1951'}],\n",
       "     'id': '5726ec6ff1498d1400e8eff4',\n",
       "     'question': \"Quando è stato approvato l' accordo di acquisizione di ABC da parte del consiglio di amministrazione di UPT?\"}]},\n",
       "  {'context': \"Nel 1952, quando la pubblicazione della sesta relazione e dell' ordinanza della FCC annunciò la fine del congelamento delle nuove domande di licenza per le stazioni radio, tra le questioni affrontate dalla Commissione figurava l' eventuale approvazione della concentrazione UPT-ABC. Un Commissario FCC ha visto la possibilità che ABC, finanziata dall' UPT, diventasse una terza rete televisiva redditizia e competitiva. Il 9 febbraio 1953, la FCC ha approvato l' acquisto di ABC da parte di UPT in cambio di 25 milioni di dollari in azioni. La società risultante dalla fusione, ribattezzata American Broadcasting-Paramount Theatres, Inc. e con sede centrale nell' edificio Paramount al 1501 di Broadway a Manhattan, possedeva sei stazioni radio AM e diverse stazioni radio FM, cinque stazioni televisive e 644 cinema in 300 città statunitensi. Per rispettare le restrizioni in materia di proprietà dell' FCC in vigore all' epoca, che vietavano la proprietà comune di due stazioni televisive nello stesso mercato, UPT ha venduto la sua stazione televisiva di Chicago, WBKB-TV, a CBS (che ha successivamente cambiato le lettere di chiamata dell' emittente in WBBM-TV) per 6 milioni di dollari, mentre ha mantenuto l' attuale stazione di Chicago di ABC, WENR-TV. L' impresa risultante dalla fusione ha acquisito le lettere telefoniche della WBKB per il canale 7, che alla fine diventerà WLS-TV. Goldenson ha iniziato a vendere alcuni dei teatri più vecchi per contribuire a finanziare la nuova rete televisiva. 1952.\",\n",
       "   'qas': [{'answers': [{'answer_start': 4, 'text': '1952'},\n",
       "      {'answer_start': 4, 'text': '1952'}],\n",
       "     'id': '5726ed12708984140094d645',\n",
       "     'question': 'Quando è stato pubblicato il sesto rapporto e ordine della FCC?'},\n",
       "    {'answers': [{'answer_start': 591,\n",
       "       'text': 'American Broadcasting-Paramount Theatres, Inc'},\n",
       "      {'answer_start': 591,\n",
       "       'text': 'American Broadcasting-Paramount Theatres, Inc'},\n",
       "      {'answer_start': 591,\n",
       "       'text': 'American Broadcasting-Paramount Theatres, Inc'}],\n",
       "     'id': '5726ed12708984140094d646',\n",
       "     'question': 'Quando UPT bough ABC, che cosa è stato chiamato la società risultante dalla fusione?'},\n",
       "    {'answers': [{'answer_start': 661, 'text': \"l' edificio Paramount\"},\n",
       "      {'answer_start': 591,\n",
       "       'text': 'American Broadcasting-Paramount Theatres, Inc.'},\n",
       "      {'answer_start': 664, 'text': 'edificio Paramount'}],\n",
       "     'id': '5726ed12708984140094d647',\n",
       "     'question': 'Dove si trovava la sede centrale di American Broadcasting-Paramount Theatres, Inc dopo la fusione?'}]},\n",
       "  {'context': \"La stazione di punta della rete, la WJZ-TV di New York City (poi ribattezzata WABC-TV), è stata firmata in onda il 10 agosto 1948, con la sua prima trasmissione in onda per due ore quella sera. Le altre stazioni di proprietà e gestite da ABC sono state lanciate nel corso dei prossimi 13 mesi: la WENR-TV a Chicago ha firmato in onda il 17 settembre, mentre la WXYZ-TV a Detroit è andata in onda il 9 ottobre 1948. Nell' ottobre 1948, a seguito di un afflusso di richieste di licenze di stazioni televisive da essa rilasciate e di uno studio sull' uso dello spettro VHF a fini di radiodiffusione, la FCC ha congelato le nuove applicazioni di stazioni. Tuttavia, KGO-TV a San Francisco, che aveva ricevuto la licenza prima del congelamento, fece il suo debutto il 5 maggio 1949. Il 7 maggio 1949, Billboard ha rivelato che ABC aveva proposto un investimento di 6,25 milioni di dollari, di cui avrebbe speso 2,5 milioni di dollari per convertire 20 ettari (80.937 m2) di terreno a Hollywood in quello che sarebbe diventato The Prospect Studios, e costruire un trasmettitore sul Monte Wilson, in previsione del lancio di KECA-TV, che era prevista per iniziare le operazioni il 1 agosto 1 (ma non avrebbe effettivamente firmare fino a settembre.\",\n",
       "   'qas': [{'answers': [{'answer_start': 115, 'text': '10 agosto 1948'},\n",
       "      {'answer_start': 115, 'text': '10 agosto 1948'},\n",
       "      {'answer_start': 115, 'text': '10 agosto 1948'}],\n",
       "     'id': '5726edeff1498d1400e8f024',\n",
       "     'question': 'Quando ha iniziato a trasmettere WJZ-TV a NYC?'},\n",
       "    {'answers': [{'answer_start': 401, 'text': 'ottobre 1948'},\n",
       "      {'answer_start': 401, 'text': 'ottobre 1948'},\n",
       "      {'answer_start': 401, 'text': 'ottobre 1948'}],\n",
       "     'id': '5726edeff1498d1400e8f025',\n",
       "     'question': 'Quando ha congelato le applicazioni FCC in arrivo per le nuove stazioni?'},\n",
       "    {'answers': [{'answer_start': 1076, 'text': 'Monte Wilson'},\n",
       "      {'answer_start': 1076, 'text': 'Monte Wilson'}],\n",
       "     'id': '5726edeff1498d1400e8f026',\n",
       "     'question': '1 agosto Dove ABC ha costruito il suo trasmettitore per la sua stazione affiliata a San Francisco?'}]},\n",
       "  {'context': \"Gli anni Sessanta sarebbero stati segnati dall' ascesa della serie familiare nel tentativo di ABC di controprogrammare i suoi concorrenti affermati, ma il decennio è stato segnato anche dal graduale passaggio della rete al colore. Il 30 settembre 1960, ABC ha presentato in anteprima The Flintstones, un altro esempio di controprogrammazione; anche se la serie animata di William Hanna e Joseph Barbera è stata filmata a colori fin dall' inizio, inizialmente è stata trasmessa in bianco e nero, in quanto ABC non aveva fatto i necessari aggiornamenti tecnici per trasmettere la sua programmazione a colori all' epoca. I Flintstones hanno permesso ad ABC di presentare una novità, quella della programmazione animata in prima serata, ma ha anche permesso alla rete di iniziare a colmare il buco aperto dalla conclusione della partnership Disney portando la programmazione orientata alla famiglia di altri produttori.\",\n",
       "   'qas': [{'answers': [{'answer_start': 234, 'text': '30 settembre 1960'},\n",
       "      {'answer_start': 234, 'text': '30 settembre 1960'},\n",
       "      {'answer_start': 234, 'text': '30 settembre 1960'}],\n",
       "     'id': '5726efdbdd62a815002e95c6',\n",
       "     'question': 'Quando ABC ha premiato i Flintstones?'},\n",
       "    {'answers': [{'answer_start': 372,\n",
       "       'text': 'William Hanna e Joseph Barbera'},\n",
       "      {'answer_start': 372, 'text': 'William Hanna e Joseph Barbera'},\n",
       "      {'answer_start': 372, 'text': 'William Hanna e Joseph Barbera'}],\n",
       "     'id': '5726efdbdd62a815002e95c8',\n",
       "     'question': 'Chi erano i creatori dei Flintstones?'}]},\n",
       "  {'context': \"Nel 1959, Walt Disney Productions, dopo aver migliorato la sua situazione finanziaria, aveva acquistato le azioni di ABC nel parco a tema Disneyland per 7,5 milioni di dollari e ha avviato discussioni per rinnovare il contratto televisivo di ABC per Walt Disney Presents, che doveva scadere nel 1961. Walt Disney è stato contattato da NBC per produrre trasmissioni a colori della sua serie antologia (che sarebbe stato ribattezzato Walt Disney's Wonderful World of Color di Walt Disney). Goldenson ha detto che ABC non poteva contrastare l' offerta, perchè la rete non ha avuto le risorse tecniche e finanziarie per portare il programma nel formato. Di conseguenza, la prima collaborazione televisiva di ABC e Disney si concluse nel 1961 (la rete avrebbe ripreso il suo rapporto con Disney nel 1985, quando la serie antologica tornò alla rete per una tre stagioni come il Disney Sunday Movie fino a perdere nuovamente i diritti di NBC nel 1988; la serie antologica Disney sarebbe tornata ad ABC nel 1996, dopo l' acquisto da parte della società delle future Capital Cities/ABC, come The Wonderful World of the Wonderful World.\",\n",
       "   'qas': [{'answers': [{'answer_start': 4, 'text': '1959'},\n",
       "      {'answer_start': 4, 'text': '1959'},\n",
       "      {'answer_start': 4, 'text': '1959'}],\n",
       "     'id': '5726f0865951b619008f82e5',\n",
       "     'question': 'Quando Walt Disney Productions ha acquistato le azioni di ABC nel parco a tema Disneyland?'},\n",
       "    {'answers': [{'answer_start': 335, 'text': 'NBC'},\n",
       "      {'answer_start': 335, 'text': 'NBC'},\n",
       "      {'answer_start': 335, 'text': 'NBC'}],\n",
       "     'id': '5726f0865951b619008f82e6',\n",
       "     'question': 'Quale rete si è avvicinata a Walt Disney per produrre trasmissioni a colori della sua serie antologica?'},\n",
       "    {'answers': [{'answer_start': 295, 'text': '1961'},\n",
       "      {'answer_start': 295, 'text': '1961'},\n",
       "      {'answer_start': 295, 'text': '1961'}],\n",
       "     'id': '5726f0865951b619008f82e7',\n",
       "     'question': 'Quando è decaduto il rapporto televisivo tra ABC e Disney?'},\n",
       "    {'answers': [{'answer_start': 794, 'text': '1985'},\n",
       "      {'answer_start': 794, 'text': '1985'},\n",
       "      {'answer_start': 794, 'text': '1985'}],\n",
       "     'id': '5726f0865951b619008f82e8',\n",
       "     'question': 'In quale anno ABC ha ripreso il suo rapporto televisivo con Disney?'}]},\n",
       "  {'context': 'Nel 2000, ABC ha lanciato una campagna promozionale web-based focalizzata intorno al suo logo cerchio, chiamato anche \"il punto\", in cui il personaggio fumetto carattere Little Dot ha indotto i visitatori a \"scaricare il punto\", un programma che avrebbe fatto volare il logo ABC intorno allo schermo e stabilirsi nell\\' angolo in basso a destra. La rete ha incaricato il gruppo Troika Design Group di progettare e produrre la sua identità 2001-02, che ha continuato ad utilizzare la colorazione nero-giallastro del logo e caratterizzato da punti e strisce in vari spot promozionali e di identificazione.',\n",
       "   'qas': [{'answers': [{'answer_start': 89, 'text': 'logo cerchio'},\n",
       "      {'answer_start': 89, 'text': 'logo cerchio'},\n",
       "      {'answer_start': 89, 'text': 'logo cerchio'}],\n",
       "     'id': '572734af708984140094dae3',\n",
       "     'question': 'Nel 2000, ABC ha avviato una campagna basata su Internet incentrata su cosa?'},\n",
       "    {'answers': [{'answer_start': 370, 'text': 'gruppo Troika Design'},\n",
       "      {'answer_start': 370, 'text': 'gruppo Troika Design'},\n",
       "      {'answer_start': 370, 'text': 'gruppo Troika Design'}],\n",
       "     'id': '572734af708984140094dae4',\n",
       "     'question': \"Chi è stato assunto per produrre l' identità ABC 2001-02?\"},\n",
       "    {'answers': [{'answer_start': 119, 'text': 'il punto'},\n",
       "      {'answer_start': 118, 'text': '\"il punto\"'},\n",
       "      {'answer_start': 119, 'text': 'il punto'}],\n",
       "     'id': '572734af708984140094dae6',\n",
       "     'question': 'Qual è il soprannome del logo ABC della campagna 2000?'}]},\n",
       "  {'context': 'Nel 1998, la rete ha iniziato ad utilizzare un\\' identità grafica minimalista, disegnata da Pittard Sullivan, con un piccolo logo in bianco e nero \"ABC Circle\" su sfondo giallo (le promozioni in questo periodo presentavano anche una sequenza di foto fisse delle stelle dei suoi programmi durante la scheda timelot e la sequenza di orari che iniziava ogni notte. Accanto al pacchetto è stata introdotta una nuova melodia tematica a tema a quattro note, basata sulla campagna d\\' immagine \"We Love TV\" della rete introdotta quell\\' anno, che ha creato una firma audio alla pari con le suonerie NBC, i vari suoni a tre note della CBS (inclusa l\\' attuale versione utilizzata dal 2000) e la Fanfare Fox. La firma a quattro note è stata aggiornata ad ogni stagione televisiva successiva (anche se le varianti utilizzate dalla stagione 1998-99 rimangono in uso durante le carte di vanità della casa di produzione mostrate in base ai crediti di chiusura della maggior parte dei programmi). Nell\\' autunno del 2015, ABC si ferma con le sue jingle a quattro note 1998-2002 per le carte di vanità delle società di promozione e produzione seguendo i crediti di chiusura della maggior parte dei suoi programmi in diciassette anni, ora ha una musica diversa e di tipo avventuroso (con la batteria della firma a quattro note del network nel finale). La vecchia melodia a tema a quattro note è ancora utilizzata da ABC on Demand fino all\\' inizio della mostra ABC.',\n",
       "   'qas': [{'answers': [{'answer_start': 91, 'text': 'Pittard Sullivan'},\n",
       "      {'answer_start': 91, 'text': 'Pittard Sullivan'},\n",
       "      {'answer_start': 91, 'text': 'Pittard Sullivan'}],\n",
       "     'id': '572735a15951b619008f86bf',\n",
       "     'question': 'Chi ha progettato il nuovo design grafico ABC 1998?'},\n",
       "    {'answers': [{'answer_start': 997, 'text': '2015'},\n",
       "      {'answer_start': 997, 'text': '2015'},\n",
       "      {'answer_start': 997, 'text': '2015'}],\n",
       "     'id': '572735a15951b619008f86c0',\n",
       "     'question': 'In quale anno ABC ha smesso di usare il jingle a quattro note per la promozione?'},\n",
       "    {'answers': [{'answer_start': 486, 'text': 'We Love TV'},\n",
       "      {'answer_start': 486, 'text': 'We Love TV'}],\n",
       "     'id': '572735a15951b619008f86c1',\n",
       "     'question': \"Quale campagna d' immagine si basava sul nuovo jingle a quattro note per ABC?\"},\n",
       "    {'answers': [{'answer_start': 1395, 'text': 'ABC on Demand'},\n",
       "      {'answer_start': 1395, 'text': 'ABC on Demand'}],\n",
       "     'id': '572735a15951b619008f86c2',\n",
       "     'question': 'Dove è ancora in uso il jingle ABC a quattro note?'}]},\n",
       "  {'context': 'Nel 1983, in occasione del 40° anniversario della fondazione della rete, le sequenze di ID hanno fatto apparire il logo in un disegno CGI oro su sfondo blu, accompagnato dallo slogan \"That Special Feeling\" in un font script. Dieci anni dopo, nel 1993, il logo \"ABC Circle\" ritorna al suo classico schema cromatico bianco su nero, ma con effetti lucidi sia sul cerchio che sulle lettere, e un bordo in bronzo che circonda il cerchio. Il logo ABC è apparso per la prima volta come un bug sullo schermo nella stagione 1993-94, apparso inizialmente solo per 60 secondi all\\' inizio di un atto o segmento, prima di apparire in tutti i programmi (tranne durante le pause commerciali) a partire dalla stagione 1995-96; le rispettive iterazioni del bug del logo traslucido sono state incorporate anche all\\' interno delle promozioni del programma fino alla stagione 2011-2012.',\n",
       "   'qas': [{'answers': [{'answer_start': 506, 'text': 'stagione 1993-94'}],\n",
       "     'id': '572736625951b619008f86d1',\n",
       "     'question': 'In quale stagione il logo ABC è apparso per la prima volta come bug sullo schermo?'},\n",
       "    {'answers': [{'answer_start': 693, 'text': 'stagione 1995-96'}],\n",
       "     'id': '572736625951b619008f86d2',\n",
       "     'question': \"Quando è iniziato a comparire il bug del logo durante l' intero programma per gli spettacoli ABC?\"},\n",
       "    {'answers': [{'answer_start': 4, 'text': '1983'},\n",
       "      {'answer_start': 4, 'text': '1983'},\n",
       "      {'answer_start': 4, 'text': '1983'}],\n",
       "     'id': '572736625951b619008f86d3',\n",
       "     'question': \"In quale anno è stato il 40° anniversario della fondazione dell' ABC?\"},\n",
       "    {'answers': [{'answer_start': 184, 'text': 'That Special Feeling'},\n",
       "      {'answer_start': 184, 'text': 'That Special Feeling'},\n",
       "      {'answer_start': 184, 'text': 'That Special Feeling'}],\n",
       "     'id': '572736625951b619008f86d4',\n",
       "     'question': 'Quale slogan accompagnava il logo del 40° anniversario di ABC?'}]},\n",
       "  {'context': 'Gli anni\\' 70 e\\' 80 hanno visto l\\' emergere di molti pacchetti di immagini grafiche per la rete, in cui l\\' impostazione del logo si basa principalmente su effetti di luce speciali, poi in fase di sviluppo, tra cui bianco, blu, rosa, neon arcobaleno e linee tratteggiate scintillanti. Tra le numerose varianti del logo \"ABC Circle\", una sequenza di ID del 1977 presentava una bolla su sfondo nero che rappresentava il cerchio con lettere in oro lucido e, come tale, era la prima carta di identificazione ABC ad avere un aspetto tridimensionale.',\n",
       "   'qas': [{'answers': [{'answer_start': 354, 'text': '1977'},\n",
       "      {'answer_start': 354, 'text': '1977'},\n",
       "      {'answer_start': 354, 'text': '1977'}],\n",
       "     'id': '572736fc5951b619008f86d9',\n",
       "     'question': \"In quale anno la prima carta d' identità ABC ha avuto un aspetto 3D?\"},\n",
       "    {'answers': [{'answer_start': 383, 'text': 'sfondo nero'},\n",
       "      {'answer_start': 390, 'text': 'nero'},\n",
       "      {'answer_start': 390, 'text': 'nero'}],\n",
       "     'id': '572736fc5951b619008f86da',\n",
       "     'question': 'Quale colore era lo sfondo della sequenza ID 1977 di ABC?'},\n",
       "    {'answers': [{'answer_start': 439, 'text': 'oro lucido'},\n",
       "      {'answer_start': 439, 'text': 'oro'}],\n",
       "     'id': '572736fc5951b619008f86db',\n",
       "     'question': 'In quale colore era il punto rappresentato nella sequenza ID 1977 di ABC?'}]},\n",
       "  {'context': 'Nel 1962, il grafico Paul Rand ridisegna il logo ABC nella sua forma più conosciuta (e attuale) con le lettere minuscole \"abc\" racchiuse in un unico cerchio nero. Il nuovo logo esordisce in onda per le promozioni di ABC all\\' inizio della stagione 1963-64. Le lettere ricordano fortemente il carattere tipografico Bauhaus disegnato da Herbert Bayer negli anni Venti, ma condividono anche similitudini con diversi altri caratteri, come ITC Avant Garde e Horatio, e lo Chalet più simile. La semplicità del logo ha reso più facile la riprogettazione e la duplicazione, il che ha conferito un beneficio per ABC (soprattutto prima dell\\' avvento della computer grafica).',\n",
       "   'qas': [{'answers': [{'answer_start': 21, 'text': 'Paul Rand'},\n",
       "      {'answer_start': 21, 'text': 'Paul Rand'},\n",
       "      {'answer_start': 21, 'text': 'Paul Rand'}],\n",
       "     'id': '57273799f1498d1400e8f4be',\n",
       "     'question': 'Quale grafico ha disegnato il logo ABC nella sua forma più conosciuta?'},\n",
       "    {'answers': [{'answer_start': 313, 'text': 'Bauhaus'}],\n",
       "     'id': '57273799f1498d1400e8f4bf',\n",
       "     'question': \"Di quale carattere tipografico ricordano le lettere dell' iconico logo ABC?\"},\n",
       "    {'answers': [{'answer_start': 334, 'text': 'Herbert Bayer'},\n",
       "      {'answer_start': 334, 'text': 'Herbert Bayer'},\n",
       "      {'answer_start': 334, 'text': 'Herbert Bayer'}],\n",
       "     'id': '57273799f1498d1400e8f4c0',\n",
       "     'question': 'Chi era il carattere tipografico Bauhaus progettato originariamente negli anni Venti?'},\n",
       "    {'answers': [{'answer_start': 238, 'text': 'stagione 1963-64'},\n",
       "      {'answer_start': 238, 'text': 'stagione 1963-64'},\n",
       "      {'answer_start': 4, 'text': '1962'}],\n",
       "     'id': '57273799f1498d1400e8f4c1',\n",
       "     'question': \"Quando ha debuttato per la prima volta l' iconico logo ABC di Paul Rand?\"}]},\n",
       "  {'context': \"Tra maggio e settembre 2005, voci diffuse che Disney-ABC stava considerando la vendita di ABC Radio, con Clear Channel Communications e Westwood One (che in precedenza aveva acquistato la divisione radio di NBC, nonchè i diritti di distribuzione a CBS e al Mutual Broadcasting System durante gli anni' 90) come potenziali acquirenti. Il 19 ottobre 2005, ABC ha annunciato la ristrutturazione del gruppo in sei divisioni: Entertainment Communications, Communications Resources, Kids Communications, News Communications, Corporate Communications e International Communications.\",\n",
       "   'qas': [{'answers': [{'answer_start': 90, 'text': 'ABC Radio'},\n",
       "      {'answer_start': 90, 'text': 'ABC Radio'},\n",
       "      {'answer_start': 90, 'text': 'ABC Radio'}],\n",
       "     'id': '5727387b5951b619008f86e9',\n",
       "     'question': 'Quale entità si è detto che sarebbe stata venduta da ABC tra maggio e settembre 2005?'},\n",
       "    {'answers': [{'answer_start': 337, 'text': '19 ottobre 2005'},\n",
       "      {'answer_start': 337, 'text': '19 ottobre 2005'},\n",
       "      {'answer_start': 337, 'text': '19 ottobre 2005'}],\n",
       "     'id': '5727387b5951b619008f86ea',\n",
       "     'question': 'Quando ha annunciato la ristrutturazione della radio ABC?'},\n",
       "    {'answers': [{'answer_start': 406, 'text': 'sei divisioni'},\n",
       "      {'answer_start': 406, 'text': 'sei'},\n",
       "      {'answer_start': 406, 'text': 'sei'}],\n",
       "     'id': '5727387b5951b619008f86eb',\n",
       "     'question': 'Quante divisioni è stata ristrutturata la radio ABC nel 2005?'}]},\n",
       "  {'context': \"Nel 2004, l' audience media di ABC è diminuita di dieci punti di valutazione, arrivando al quarto posto, dietro NBC, CBS e Fox (per l' anno successivo, la quota di audience media finale della stagione combinata di ABC, NBC e CBS rappresentava solo il 32% delle famiglie statunitensi). Tuttavia, durante la stagione 2004-05, il network ha avuto un successo inaspettato con nuove serie come Desperate Housewives, Lost and Grey's Anatomy e reality series Dancing with the Stars, che ha aiutato ABC salire al secondo posto, saltando davanti a CBS, ma dietro una volpe in aumento. Il 21 aprile 2004, Disney ha annunciato una ristrutturazione della sua divisione Disney Media Networks con Anne Sweeney essere nominato presidente di ABC genitori Disney-ABC Television Group, e ESPN presidente George Bodenheimer diventando co-CEO della divisione con Sweeney, così come presidente di ABC Sport. Il 7 dicembre 2005, ABC Sports e ESPN hanno firmato con NASCAR un accordo di diritti di trasmissione di otto anni, che ha permesso ad ABC e ESPN di trasmettere 17 gare di Coppa Nextel ogni stagione (che comprende poco più della metà delle 36 gare che si tengono ogni anno) in vigore con la stagione 2006. 2004.\",\n",
       "   'qas': [{'answers': [{'answer_start': 4, 'text': '2004'},\n",
       "      {'answer_start': 4, 'text': '2004'}],\n",
       "     'id': '57273954708984140094db05',\n",
       "     'question': \"In quale anno l' audience di ABC si è posizionata al quarto posto dietro le altre grandi reti?\"},\n",
       "    {'answers': [{'answer_start': 683, 'text': 'Anne Sweeney'},\n",
       "      {'answer_start': 683, 'text': 'Anne Sweeney'},\n",
       "      {'answer_start': 683, 'text': 'Anne Sweeney'}],\n",
       "     'id': '57273954708984140094db06',\n",
       "     'question': 'Chi è stato nominato presidente del gruppo televisivo Disney-ABC nel 2004?'},\n",
       "    {'answers': [{'answer_start': 943, 'text': 'NASCAR'},\n",
       "      {'answer_start': 943, 'text': 'NASCAR'},\n",
       "      {'answer_start': 770, 'text': 'ESPN'}],\n",
       "     'id': '57273954708984140094db07',\n",
       "     'question': 'Chi ha firmato con ESPN e ABC un accordo di otto anni nel 2005?'}]},\n",
       "  {'context': \"Nel 2002 le consociate Networks hanno approvato un contratto di affiliazione biennale. Nel mese di settembre, il Presidente della Disney/CEO Michael Eisner ha delineato una proposta di riallineamento delle parti del giorno della rete di trasmissione ABC con l' unità simile nei suoi canali via cavo: Sabato mattina ABC con i canali Disney (Toon & Playhouse), ABC diurno con Soapnet e ABC prime time con la famiglia ABC. 2002 ha visto il debutto della prima serie di successo del network realtà, The Bachelor (il successo eliminazione stile datazione show ha portato ad una spin-off, The Bachelorette, che ha debuttato l' anno successivo, così come due ulteriori spin-off che più tardi ha debuttato nei primi mesi del 2010).\",\n",
       "   'qas': [{'answers': [{'answer_start': 4, 'text': '2002'},\n",
       "      {'answer_start': 4, 'text': '2002'},\n",
       "      {'answer_start': 4, 'text': '2002'}],\n",
       "     'id': '57273a0d708984140094db0d',\n",
       "     'question': 'Quando è stato approvato il nuovo contratto di affiliazione biennale?'},\n",
       "    {'answers': [{'answer_start': 141, 'text': 'Michael Eisner'},\n",
       "      {'answer_start': 141, 'text': 'Michael Eisner'},\n",
       "      {'answer_start': 141, 'text': 'Michael Eisner'}],\n",
       "     'id': '57273a0d708984140094db0e',\n",
       "     'question': 'Chi era presidente e amministratore delegato di Disney nel 2002?'},\n",
       "    {'answers': [{'answer_start': 495, 'text': 'The Bachelor'},\n",
       "      {'answer_start': 495, 'text': 'The Bachelor'},\n",
       "      {'answer_start': 495, 'text': 'The Bachelor'}],\n",
       "     'id': '57273a0d708984140094db0f',\n",
       "     'question': 'Che cosa ha colpito reality series ha debuttato per ABC nel 2002?'},\n",
       "    {'answers': [{'answer_start': 583, 'text': 'The Bachelorette'},\n",
       "      {'answer_start': 583, 'text': 'The Bachelorette'},\n",
       "      {'answer_start': 583, 'text': 'The Bachelorette'}],\n",
       "     'id': '57273a0d708984140094db10',\n",
       "     'question': 'Quale reality series è stato il primo spinoff del Bachelor per ABC?'}]},\n",
       "  {'context': \"Il 30 aprile 2000, a seguito di una controversia con ABC in materia di trasporto aereo, Time Warner Cable ha rimosso le stazioni di proprietà e gestite da ABC dai sistemi del fornitore di cavi in quattro mercati (WABC-TV a New York City, KABC-TV a Los Angeles, KTRK a Houston e WTVD a Raleigh-Durham). In data 31 dicembre 1999 la rete aveva già raggiunto un' ultima ora per rinnovare il contratto di trasporto con il fornitore. ABC ha presentato una petizione d' emergenza alla Federal Communications Commission il 1 ° maggio per costringere TWC a ripristinare le stazioni colpite, la FCC ha deciso a favore di ABC, ordinando Time Warner Cable per ripristinare le stazioni, facendo così il pomeriggio del 2 maggio. ABC si è conclusa la stagione 2000-01 come il network più visto, davanti a NBC.\",\n",
       "   'qas': [{'answers': [{'answer_start': 88, 'text': 'Time Warner Cable'},\n",
       "      {'answer_start': 88, 'text': 'Time Warner Cable'},\n",
       "      {'answer_start': 88, 'text': 'Time Warner Cable'}],\n",
       "     'id': '57273abef1498d1400e8f4da',\n",
       "     'question': \"Quale società di distribuzione via cavo ha rimosso le stazioni ABC dai suoi sistemi in alcuni mercati nell' aprile 2000?\"},\n",
       "    {'answers': [{'answer_start': 53, 'text': 'ABC'},\n",
       "      {'answer_start': 53, 'text': 'ABC'},\n",
       "      {'answer_start': 53, 'text': 'ABC'}],\n",
       "     'id': '57273abef1498d1400e8f4db',\n",
       "     'question': \"Chi è stato il governo della FCC a favore della disputa dell' aprile 2000 tra Time Warner Cable e ABC?\"},\n",
       "    {'answers': [{'answer_start': 53, 'text': 'ABC'},\n",
       "      {'answer_start': 53, 'text': 'ABC'},\n",
       "      {'answer_start': 53, 'text': 'ABC'}],\n",
       "     'id': '57273abef1498d1400e8f4dc',\n",
       "     'question': 'Quale rete principale è stata quella più osservata per la stagione 2000-2001?'},\n",
       "    {'answers': [{'answer_start': 690, 'text': 'pomeriggio del 2'},\n",
       "      {'answer_start': 690, 'text': 'pomeriggio del 2 maggio'},\n",
       "      {'answer_start': 310, 'text': '31 dicembre 1999'}],\n",
       "     'id': '57273abef1498d1400e8f4dd',\n",
       "     'question': 'Quando è stato costretto Time Warner cavo costretto a ripristinare le stazioni ABC ai mercati interessati?'}]},\n",
       "  {'context': 'Oltre a Who Wires to Be a Millionaire, il network è entrato negli anni 2000 con i colpi di scena del decennio precedente come The Practice, NYPD Blue e The Wonderful World of Disney e nuove serie come My Wife and Kids e secondo Jim, tutte riuscite ad aiutare ABC rimanere davanti alla concorrenza nella classifica nonostante la partenza successiva di Millionaire. Il 2000 ha visto la fine di \"TGIF\", che stava lottando per trovare nuovi successi (con Boy Meets World e Sabrina, la strega adolescente, quest\\' ultima trasferitasi a The WB nel settembre 2000, cominciando a calare anche da questo punto) dopo la perdita di Family Matters e Step by Step a CBS come parte del suo tentativo fallito di un blocco di commedia orientato alla famiglia venerdì nella stagione 1997-98. Al di fuori di Venerdì stalwart 20/20, venerdì sera è rimasto un punto debole per ABC per i prossimi 11 anni.',\n",
       "   'qas': [{'answers': [{'answer_start': 71, 'text': '2000'},\n",
       "      {'answer_start': 71, 'text': '2000'},\n",
       "      {'answer_start': 71, 'text': '2000'}],\n",
       "     'id': '57273b69dd62a815002e99d6',\n",
       "     'question': 'Che anno ha terminato il \"TGIF\" di ABC?'},\n",
       "    {'answers': [{'answer_start': 530, 'text': 'The WB'},\n",
       "      {'answer_start': 530, 'text': 'The WB'},\n",
       "      {'answer_start': 534, 'text': 'WB'}],\n",
       "     'id': '57273b69dd62a815002e99d7',\n",
       "     'question': 'A quale rete Sabrina la Strega adolescente si è trasferita nel 2000?'},\n",
       "    {'answers': [{'answer_start': 652, 'text': 'CBS'},\n",
       "      {'answer_start': 652, 'text': 'CBS'},\n",
       "      {'answer_start': 652, 'text': 'CBS'}],\n",
       "     'id': '57273b69dd62a815002e99d8',\n",
       "     'question': 'Quale rete ha iniziato a trasmettere la commedia TGIF Family Matters per la stagione 1997-98?'}]},\n",
       "  {'context': \"Nell' agosto 1999, ABC ha debuttato in anteprima un evento speciale della serie, Who Wants to Be a Millionaire, un game show basato sul programma britannico dello stesso titolo. Ospitato in tutto il suo mandato ABC da Regis Philbin, il programma è diventato un grande successo di rating per tutta la sua corsa estiva iniziale, che ha portato ABC a rinnovare Millionaire come una serie regolare, tornando il 18 gennaio 2000. Al suo apice, il programma ha trasmesso fino a sei notti alla settimana. Sostenuto da Millionaire, durante la stagione 1999-2000, ABC è diventato il primo network a passare dal terzo al primo posto nelle classifiche durante un' unica stagione televisiva. Millionaire ha terminato la sua corsa sulla primatime lineup della rete dopo tre anni nel 2002, con Buena Vista Television rilanciare lo spettacolo come un programma sindacato (in base a quell' incarnazione originale ospite Meredith Vieira) nel settembre di quell' anno.\",\n",
       "   'qas': [{'answers': [{'answer_start': 6, 'text': 'agosto 1999'},\n",
       "      {'answer_start': 6, 'text': 'agosto 1999'},\n",
       "      {'answer_start': 6, 'text': 'agosto 1999'}],\n",
       "     'id': '57273c195951b619008f8721',\n",
       "     'question': 'Quando ha fatto la prima anteprima di ABC Chi vuole diventare milionario?'},\n",
       "    {'answers': [{'answer_start': 218, 'text': 'Regis Philbin'},\n",
       "      {'answer_start': 218, 'text': 'Regis Philbin'},\n",
       "      {'answer_start': 218, 'text': 'Regis Philbin'}],\n",
       "     'id': '57273c195951b619008f8722',\n",
       "     'question': 'Chi originariamente ha ospitato Chi vuole diventare milionario per ABC?'},\n",
       "    {'answers': [{'answer_start': 779, 'text': 'Buena Vista Television'},\n",
       "      {'answer_start': 779, 'text': 'Buena Vista Television'},\n",
       "      {'answer_start': 779, 'text': 'Buena Vista Television'}],\n",
       "     'id': '57273c195951b619008f8723',\n",
       "     'question': 'Quale azienda ha rilanciato Chi vuole diventare milionario come programma sindacato?'},\n",
       "    {'answers': [{'answer_start': 903, 'text': 'Meredith Vieira'},\n",
       "      {'answer_start': 903, 'text': 'Meredith Vieira'},\n",
       "      {'answer_start': 903, 'text': 'Meredith Vieira'}],\n",
       "     'id': '57273c195951b619008f8724',\n",
       "     'question': 'Chi originariamente ha ospitato la versione sindacata di Chi vuole essere milionario?'}]},\n",
       "  {'context': \"Il 31 luglio 1995, The Walt Disney Company ha annunciato un accordo di fusione con Capital Cities/ABC per 19 miliardi di dollari. Gli azionisti di Disney hanno approvato la fusione in occasione di una conferenza speciale tenutasi a New York City il 4 gennaio 1996, con l' acquisizione di Capital Cities/ABC avvenuta il 9 febbraio; a seguito della vendita, Disney ha ridenominato la sua nuova controllata ABC Inc. Oltre alla rete ABC, l' acquisizione di Disney ha integrato nella società i dieci programmi televisivi e radiofonici di proprietà di ABC e 21 stazioni radio; la sua partecipazione dell' 80% in ESPN, le partecipazioni azionarie in The History Channel, A&E Television Networks e Lifetime Entertainment; Capital Cities/ABC e le proprietà di giornali e riviste. Poichè le regole di proprietà della FCC vietavano all' azienda di mantenere sia essa che KABC-TV, Disney ha venduto la stazione indipendente di Los Angeles KCAL-TV a Young Broadcasting per 387 milioni di dollari. Il 4 aprile, Disney ha venduto i quattro giornali che ABC aveva controllato sotto Capital Cities a Knight Ridder per 1,65 miliardi di dollari. Dopo la fusione, Thomas S. Murphy lasciò ABC e Robert Iger prese il suo posto come presidente e CEO. Intorno al momento della fusione, le unità di produzione televisiva Disney aveva già prodotto serie per la rete come Home Improvement e Boy Meets World, mentre l' operazione ha permesso anche l' accesso ABC alla biblioteca di programmazione per bambini Disney per il suo blocco sabato mattina. Nel 1998, ABC ha presentato per la prima volta la sitcom Sports Night di Aaron Sorkin, incentrata sulle fatiche del personale di un notiziario sportivo in stile SportsCenter; nonostante gli elogi critici e i numerosi Emmy Awards, la serie è stata annullata nel 2000 dopo due stagioni.\",\n",
       "   'qas': [{'answers': [{'answer_start': 3, 'text': '31 luglio 1995'},\n",
       "      {'answer_start': 3, 'text': '31 luglio 1995'},\n",
       "      {'answer_start': 0, 'text': 'Il 31 luglio 1995'}],\n",
       "     'id': '57273d19708984140094db3d',\n",
       "     'question': 'Quando è stata annunciata la prima fusione tra Disney e ABC?'},\n",
       "    {'answers': [{'answer_start': 404, 'text': 'ABC Inc.'},\n",
       "      {'answer_start': 404, 'text': 'ABC Inc.'},\n",
       "      {'answer_start': 404, 'text': 'ABC Inc'}],\n",
       "     'id': '57273d19708984140094db3e',\n",
       "     'question': \"Che cosa ha cambiato il nome di Capital City/ABC dopo aver acquisito per la prima volta l' azienda?\"},\n",
       "    {'answers': [{'answer_start': 1083, 'text': 'Knight Ridder'},\n",
       "      {'answer_start': 1083, 'text': 'Knight Ridder'},\n",
       "      {'answer_start': 1083, 'text': 'Knight Ridder'}],\n",
       "     'id': '57273d19708984140094db3f',\n",
       "     'question': 'Chi ha venduto la Disney ai quattro giornali controllati dalla ABC?'},\n",
       "    {'answers': [{'answer_start': 1174, 'text': 'Robert Iger'},\n",
       "      {'answer_start': 1174, 'text': 'Robert Iger'},\n",
       "      {'answer_start': 1174, 'text': 'Robert Iger'}],\n",
       "     'id': '57273d19708984140094db40',\n",
       "     'question': \"Chi ha preso il posto di Thomas Murphy dopo l' acquisizione Disney di ABC?\"},\n",
       "    {'answers': [{'answer_start': 1579, 'text': 'Sports Night'},\n",
       "      {'answer_start': 1579, 'text': 'Sports Night'},\n",
       "      {'answer_start': 1579, 'text': 'Sports Night'}],\n",
       "     'id': '57273d19708984140094db41',\n",
       "     'question': 'Che cosa Aaron Sorkin Aaron Sorkin creato spettacolo ha fatto debutto ABC nel 1998?'}]},\n",
       "  {'context': 'Fu solo nella stagione 1965-66 che il colore divenne il formato dominante per le tre reti televisive. ABC, nel frattempo, è rimasta al terzo posto e aveva ancora bisogno di denaro per diventare un concorrente importante. Tuttavia, i problemi di ABC con la sua transizione al colore divennero secondari rispetto ai problemi finanziari della rete; nel 1964, la rete si trovò, come scriveva Goldenson più tardi nel libro del 1991 \"Beating the Odds: The Untold Story Behind the Rise of ABC\",\"nel bel mezzo di una guerra[dove] il campo di battaglia era Wall Street\". Molte società hanno cercato di rilevare ABC, tra cui Norton Simon, General Electric, International Telephone e Telegraph e Litton Industries.',\n",
       "   'qas': [{'answers': [{'answer_start': 14, 'text': 'stagione 1965-66'}],\n",
       "     'id': '57273dbbf1498d1400e8f508',\n",
       "     'question': 'Per quale stagione televisiva il colore è diventato prima un formato dominante?'},\n",
       "    {'answers': [{'answer_start': 135, 'text': 'terzo posto'},\n",
       "      {'answer_start': 135, 'text': 'terzo'},\n",
       "      {'answer_start': 135, 'text': 'terzo'}],\n",
       "     'id': '57273dbbf1498d1400e8f509',\n",
       "     'question': 'Nella stagione 1965-66, quale posto ha trovato ABC se stesso tra le altre reti di rating?'},\n",
       "    {'answers': [{'answer_start': 428,\n",
       "       'text': 'Beating the Odds: The Untold Story Behind the Rise of ABC'},\n",
       "      {'answer_start': 427,\n",
       "       'text': '\"Beating the Odds: The Untold Story Behind the Rise of ABC\"'},\n",
       "      {'answer_start': 428,\n",
       "       'text': 'Beating the Odds: The Untold Story Behind the Rise of ABC'}],\n",
       "     'id': '57273dbbf1498d1400e8f50a',\n",
       "     'question': 'Qual era il libro di Goldenson del 1991 su ABC intitolato?'}]},\n",
       "  {'context': \"Il 1° maggio 1953, le stazioni di punta di New York City di ABC - WJZ, WJZ-FM e WJZ-TV - cambiarono i rispettivi nominativi in WABC, WABC-FM e WABC-TV e trasferirono le loro attività nelle strutture della 7a West 66th Street, ad un isolato di distanza da Central Park. Fino al 1946 le lettere di chiamata WABC erano state precedentemente utilizzate dalla stazione faro di CBS Radio (ora WCBS (AM)). Le chiamate WJZ sarebbero state successivamente riassegnate all' allora affiliata ABC di Baltimora nel 1959, in un cenno storico al fatto che WJZ era stata originariamente fondata dal proprietario della stazione di Baltimora all' epoca, Westinghouse.\",\n",
       "   'qas': [{'answers': [{'answer_start': 488, 'text': 'Baltimora'},\n",
       "      {'answer_start': 488, 'text': 'Baltimora'},\n",
       "      {'answer_start': 488, 'text': 'Baltimora'}],\n",
       "     'id': '57273ef15951b619008f8753',\n",
       "     'question': 'Il nominativo WJZ verrebbe quindi assegnato ad un affiliato ABC in quale città nel 1959?'}]},\n",
       "  {'context': 'Tuttavia, è emerso un problema per quanto riguarda le indicazioni fornite da ABC e UPT. Nel 1950, Noble nominò Robert Kintner presidente dell\\' ABC mentre egli stesso ricopriva il ruolo di CEO, una carica che ricopriva fino alla sua morte nel 1958. Nonostante la promessa di non ingerenza tra ABC e UPT, Goldenson ha dovuto intervenire nelle decisioni di ABC a causa dei problemi finanziari e del lungo periodo di indecisione della FCC. Goldenson si aggiunse alla confusione quando, nell\\' ottobre del 1954, propose una fusione tra UPT e DuMont Television Network, anch\\' esso impantanato in difficoltà finanziarie. Come parte di questa fusione, la rete sarebbe stata ribattezzata \"ABC-DuMont\" per cinque anni, e DuMont avrebbe ricevuto 5 milioni di dollari in contanti, spazio disponibile per la programmazione DuMont esistente e tempo pubblicitario garantito per i ricevitori di DuMont Laboratories. Inoltre, per rispettare le restrizioni in materia di proprietà imposte dalla FCC, sarebbe stato necessario vendere la stazione WABC-TV o la stazione WABD di proprietà e gestita da DuMont sul mercato di New York City, nonchè altre due stazioni. La concentrazione ABC-DuMont avrebbe avuto le risorse necessarie per competere con CBS e NBC.',\n",
       "   'qas': [{'answers': [{'answer_start': 111, 'text': 'Robert Kintner'},\n",
       "      {'answer_start': 111, 'text': 'Robert Kintner'},\n",
       "      {'answer_start': 111, 'text': 'Robert Kintner'}],\n",
       "     'id': '57273f9d708984140094db51',\n",
       "     'question': \"Chi è stato nominato presidente dell' ABC da Noble nel 1950?\"},\n",
       "    {'answers': [{'answer_start': 536, 'text': 'DuMont Television Network'},\n",
       "      {'answer_start': 536, 'text': 'DuMont Television Network'},\n",
       "      {'answer_start': 536, 'text': 'DuMont Television Network'}],\n",
       "     'id': '57273f9d708984140094db52',\n",
       "     'question': \"Goldenson propose una fusione tra l' UPT e quale rete nell' ottobre del 1954?\"},\n",
       "    {'answers': [{'answer_start': 679, 'text': 'ABC-DuMont'},\n",
       "      {'answer_start': 679, 'text': 'ABC-DuMont'},\n",
       "      {'answer_start': 679, 'text': 'ABC-DuMont'}],\n",
       "     'id': '57273f9d708984140094db53',\n",
       "     'question': \"Nell' ambito del progetto di fusione di Goldenson, quale sarebbe il nome della nuova entità?\"},\n",
       "    {'answers': [{'answer_start': 734,\n",
       "       'text': '5 milioni di dollari in contanti'},\n",
       "      {'answer_start': 734, 'text': '5 milioni di dollari in contanti'},\n",
       "      {'answer_start': 734, 'text': '5 milioni di dollari'}],\n",
       "     'id': '57273f9d708984140094db54',\n",
       "     'question': \"Quanto denaro doveva essere destinato a DuMont Television Network nell' ambito del progetto di fusione di Goldenson?\"}]},\n",
       "  {'context': \"Allo stesso tempo ha fatto tentativi di aiutare a far crescere ABC, Goldenson aveva cercato dalla metà del 1953 per fornire contenuti per la rete contattando i suoi vecchi conoscenti a Hollywood, con cui aveva lavorato quando UPT era una controllata di Paramount Pictures. La fusione di ABC con UPT portò alla creazione di rapporti con gli studi di produzione cinematografica di Hollywood, rompendo una quarantena che esisteva all' epoca tra film e televisione, quest' ultima in precedenza più legata alla radio. Le produzioni di punta di ABC all' epoca erano The Lone Ranger, basate sul programma radiofonico con lo stesso titolo, e The Adventures of Ozzie e Harriet, quest' ultimo (13 stagioni, dal 1952 al 1965) ha tenuto il record per la commedia prime time più lunga della storia televisiva statunitense, fino a quando non è stato superato da The Simpsons nel 2002. Paramount Immagini.\",\n",
       "   'qas': [{'answers': [{'answer_start': 871, 'text': 'Paramount Immagini'},\n",
       "      {'answer_start': 871, 'text': 'Paramount Immagini'}],\n",
       "     'id': '5727403af1498d1400e8f526',\n",
       "     'question': 'Di quale società di film hollywoodiani era controllata da UPT quando si è fusa con ABC?'},\n",
       "    {'answers': [{'answer_start': 560, 'text': 'The Lone Ranger'},\n",
       "      {'answer_start': 560, 'text': 'The Lone Ranger'},\n",
       "      {'answer_start': 560, 'text': 'The Lone Ranger'}],\n",
       "     'id': '5727403af1498d1400e8f527',\n",
       "     'question': 'Quale commedia ABC ha avuto il record per la più lunga commedia in esecuzione fino ad essere passata dai Simpson nel 2002?'}]},\n",
       "  {'context': 'Warner ha cercato con successo misto di adattare alcuni dei suoi film di maggior successo come serie televisiva ABC, e mostrare questi adattamenti come parte della serie ruota Warner Bros. Presents. In onda durante la stagione 1955-56, ha presentato gli adattamenti televisivi dei film del 1942 Kings Row e Casablanca, Cheyenne (adattato dal film del 1947 Wyoming Kid), Sugarfoot (un remake del film The Boy from Oklahoma del 1954) e Maverick. Tuttavia, la più emblematica delle relazioni di ABC con i produttori hollywoodiani era l\\' accordo con Walt Disney; dopo l\\' inizio del legame della rete con lo studio Disney, James Lewis Baughman, che all\\' epoca lavorava come editorialista, osservava che \"presso la sede di ABC a New York, i segretari[erano ora] vestiti di cappelli con orecchie di Topolino\".',\n",
       "   'qas': [{'answers': [{'answer_start': 319, 'text': 'Cheyenne'},\n",
       "      {'answer_start': 319, 'text': 'Cheyenne'},\n",
       "      {'answer_start': 319, 'text': 'Cheyenne'}],\n",
       "     'id': '57274118dd62a815002e9a1c',\n",
       "     'question': \"Quale programma televisivo per l' ABC è stato un adattamento del film Wyoming Kid del 1947?\"},\n",
       "    {'answers': [{'answer_start': 370, 'text': 'Sugarfoot'},\n",
       "      {'answer_start': 370, 'text': 'Sugarfoot'},\n",
       "      {'answer_start': 370, 'text': 'Sugarfoot'}],\n",
       "     'id': '57274118dd62a815002e9a1d',\n",
       "     'question': 'Quale programma per ABC era un remake del film The Boy di Oklahoma?'},\n",
       "    {'answers': [{'answer_start': 546, 'text': 'Walt Disney'},\n",
       "      {'answer_start': 546, 'text': 'Walt Disney'},\n",
       "      {'answer_start': 546, 'text': 'Walt Disney'}],\n",
       "     'id': '57274118dd62a815002e9a1e',\n",
       "     'question': \"Di tutti i loro accordi con i produttori hollywoodiani degli anni' 50, che era il più iconico per ABC?\"},\n",
       "    {'answers': [{'answer_start': 176, 'text': 'Warner Bros'},\n",
       "      {'answer_start': 239, 'text': 'presenta'},\n",
       "      {'answer_start': 164, 'text': 'serie ruota Warner Bros'}],\n",
       "     'id': '57274118dd62a815002e9a1f',\n",
       "     'question': 'In quale serie ABC ha presentato gli adattamenti cinematografici degli anni Cinquanta?'}]},\n",
       "  {'context': 'Walt Disney e suo fratello Roy contattavano Goldenson alla fine del 1953 perchè ABC accettasse di finanziare parte del progetto Disneyland in cambio della produzione di un programma televisivo per la rete. Walt voleva ABC di investire 500.000 dollari e ha accumulato una garanzia di 4,5 milioni di dollari in prestiti aggiuntivi, un terzo del bilancio destinato al parco. Intorno al 1954, ABC accettò di finanziare Disneyland in cambio del diritto di trasmettere un nuovo programma domenicale notturno, Disneyland, che ha debuttato sulla rete il 27 ottobre 1954 come il primo di molti programmi televisivi antologia che Disney avrebbe trasmesso nel corso dei prossimi 50 anni.',\n",
       "   'qas': [{'answers': [{'answer_start': 27, 'text': 'Roy'},\n",
       "      {'answer_start': 27, 'text': 'Roy'},\n",
       "      {'answer_start': 27, 'text': 'Roy'}],\n",
       "     'id': '572741aaf1498d1400e8f53e',\n",
       "     'question': 'Qual era il nome del fratello di Walt Disney?'},\n",
       "    {'answers': [{'answer_start': 383, 'text': '1954'},\n",
       "      {'answer_start': 383, 'text': '1954'},\n",
       "      {'answer_start': 383, 'text': '1954'}],\n",
       "     'id': '572741aaf1498d1400e8f540',\n",
       "     'question': 'In quale anno ABC ha accettato di finanziare Disneyland?'},\n",
       "    {'answers': [{'answer_start': 128, 'text': 'Disneyland'},\n",
       "      {'answer_start': 128, 'text': 'Disneyland'},\n",
       "      {'answer_start': 128, 'text': 'Disneyland'}],\n",
       "     'id': '572741aaf1498d1400e8f541',\n",
       "     'question': \"Qual è stato il primo programma di Disney trasmesso in televisione su ABC a seguito dell' accordo Disney-ABC?\"}]},\n",
       "  {'context': 'Nel luglio 1968, ABC Radio ha lanciato un progetto di programmazione speciale per le sue stazioni FM, che è stato guidato da Allen Shaw, un ex program manager di WCFL a Chicago, che è stato contattato dal presidente di ABC Radio Harold L. Neal per sviluppare un formato per competere con le nuove stazioni progressive rock e DJ-helmed. Il nuovo concetto chiamato \"LOVE Radio\", che comprendeva una selezione limitata di generi musicali, è stato lanciato sulle sette stazioni FM di proprietà e gestite da ABC alla fine di novembre 1968; il concetto ha sostituito quasi tutti i programmi forniti da queste stazioni; tuttavia, diverse affiliate (come KXYZ) hanno mantenuto la maggior parte dei loro contenuti. Nell\\' agosto 1970, Shaw ha annunciato che la politica di scelta musicale di ABC FM dovrebbe essere rivista per consentire agli ascoltatori di accedere a molti stili di musica.',\n",
       "   'qas': [{'answers': [{'answer_start': 125, 'text': 'Allen Shaw'},\n",
       "      {'answer_start': 125, 'text': 'Allen Shaw'},\n",
       "      {'answer_start': 125, 'text': 'Allen Shaw'}],\n",
       "     'id': '572742daf1498d1400e8f550',\n",
       "     'question': 'Chi guida la programmazione speciale del 1968 per le stazioni FM di ABC Radio?'},\n",
       "    {'answers': [{'answer_start': 229, 'text': 'Harold L. Neal'},\n",
       "      {'answer_start': 229, 'text': 'Harold L. Neal'},\n",
       "      {'answer_start': 229, 'text': 'Harold L. Neal'}],\n",
       "     'id': '572742daf1498d1400e8f551',\n",
       "     'question': 'Chi era il presidente di ABC Radio nel 1968?'},\n",
       "    {'answers': [{'answer_start': 364, 'text': 'LOVE Radio'},\n",
       "      {'answer_start': 364, 'text': 'LOVE Radio'},\n",
       "      {'answer_start': 364, 'text': 'LOVE Radio'}],\n",
       "     'id': '572742daf1498d1400e8f552',\n",
       "     'question': 'Qual è stato il nome del nuovo concetto radiofonico progettato da Allen Shaw?'},\n",
       "    {'answers': [{'answer_start': 459, 'text': 'sette'},\n",
       "      {'answer_start': 459, 'text': 'sette'},\n",
       "      {'answer_start': 459, 'text': 'sette'}],\n",
       "     'id': '572742daf1498d1400e8f553',\n",
       "     'question': 'Quante stazioni musicali ABC sono state possedute e gestite nel 1968?'}]},\n",
       "  {'context': \"Sul versante televisivo, nel settembre 1969, ABC lancia il film della Settimana, una vetrina settimanale volta a capitalizzare il crescente successo dei film realizzati per la TV dai primi anni' 60 in poi. The Movie of the Movie of the Week ha trasmesso lungometraggi drammatici diretti da talentuosi cineasti come Aaron Spelling, David Wolper e Steven Spielberg (quest' ultimo ha ottenuto un primo successo grazie alla vetrina per il suo film Duel del 1971) che sono stati prodotti con un budget medio di 400.000-$450.000 dollari. I successi per il network televisivo durante la fine degli anni sessanta e l' inizio degli anni settanta includevano il corteggiamento del padre di Eddie, il gruppo Brady e la famiglia di starne.\",\n",
       "   'qas': [{'answers': [{'answer_start': 39, 'text': '1969'},\n",
       "      {'answer_start': 39, 'text': '1969'},\n",
       "      {'answer_start': 29, 'text': 'settembre 1969'}],\n",
       "     'id': '5727436af1498d1400e8f558',\n",
       "     'question': 'In quale anno ABC ha lanciato il film della settimana?'},\n",
       "    {'answers': [{'answer_start': 444, 'text': 'Duel'},\n",
       "      {'answer_start': 444, 'text': 'Duel'},\n",
       "      {'answer_start': 444, 'text': 'Duel'}],\n",
       "     'id': '5727436af1498d1400e8f559',\n",
       "     'question': 'Per quale film di ABC Movie of the Week Steven Spielberg ha ottenuto il primo successo?'},\n",
       "    {'answers': [{'answer_start': 453, 'text': '1971'},\n",
       "      {'answer_start': 453, 'text': '1971'},\n",
       "      {'answer_start': 453, 'text': '1971'}],\n",
       "     'id': '5727436af1498d1400e8f55a',\n",
       "     'question': 'Quando ha fatto il primo debutto Duel al cinema di Steven Spielberg?'}]},\n",
       "  {'context': 'Nei primi anni Settanta, ABC ha completato la sua transizione al colore; il decennio nel suo complesso segnerebbe una svolta per ABC, come ha cominciato a passare CBS e NBC nel rating per diventare la rete di primo posto. Ha anche iniziato a utilizzare dati comportamentali e demografici per determinare meglio quali tipi di sponsor vendere spazi pubblicitari e fornire programmi che si rivolgessero a certi tipi di pubblico. I guadagni di ABC in termini di audience share sono stati notevolmente agevolati dal fatto che diversi mercati più piccoli erano cresciuti abbastanza per consentire affiliazioni a tempo pieno da parte di tutte e tre le reti.',\n",
       "   'qas': [{'answers': [{'answer_start': 25, 'text': 'ABC'},\n",
       "      {'answer_start': 25, 'text': 'ABC'},\n",
       "      {'answer_start': 25, 'text': 'ABC'}],\n",
       "     'id': '572743fb708984140094db94',\n",
       "     'question': \"Gli anni' 70 hanno permesso a quale rete di spostarsi al primo posto nelle classificazioni?\"},\n",
       "    {'answers': [{'answer_start': 253,\n",
       "       'text': 'dati comportamentali e demografici'}],\n",
       "     'id': '572743fb708984140094db95',\n",
       "     'question': \"Che tipo di dati ABC ha iniziato a utilizzare negli anni' 70 per meglio indirizzare gli annunci e la programmazione di un certo pubblico?\"}]},\n",
       "  {'context': \"Nel 1970, ABC ha debuttato Lunedi Night Football come parte del suo calendario Lunedi prime time; il programma è diventato un successo per la rete e servito come la National Football League (NFL) premier game della settimana fino al 2006, quando Domenica Night Football, che si trasferì alla NBC quell' anno come parte di un accordo di trasmissione che a sua volta ha visto MNF passare all' ESPN, ha assunto come gioco del tendone di lega. Secondo Goldenson, Lunedi Night Football ha contribuito a guadagnare ABC regolarmente segnare una quota di pubblico del 15%-16%; ABC Sport gestito il bilancio per la fascia oraria di lunedì notte per riassegnare il bilancio settimanale per il calendario prime time di ABC a soli sei giorni, rispetto a sette sulle reti concorrenti. 1970 ha visto anche le prime di diverse soap opera del sapone tra cui la lunga durata All My Children, che correva in rete per 41 anni.\",\n",
       "   'qas': [{'answers': [{'answer_start': 233, 'text': '2006'},\n",
       "      {'answer_start': 233, 'text': '2006'},\n",
       "      {'answer_start': 233, 'text': '2006'}],\n",
       "     'id': '5727448b5951b619008f87a2',\n",
       "     'question': 'Quando ha fatto domenica sera prima di calcio calcio su NBC?'},\n",
       "    {'answers': [{'answer_start': 391, 'text': 'ESPN'},\n",
       "      {'answer_start': 391, 'text': 'ESPN'},\n",
       "      {'answer_start': 292, 'text': 'NBC'}],\n",
       "     'id': '5727448b5951b619008f87a3',\n",
       "     'question': 'Quando Domenica Notte Calcio Domenica in anteprima, a quale rete ha fatto Lunedi Notte Calcio si muovono?'}]},\n",
       "  {'context': \"Nel 1970, la FCC ha votato per approvare il Financial Interest and Syndication Rules, un insieme di regolamenti volti a impedire alle principali reti di monopolizzare il panorama broadcast impedendo loro di possedere una qualsiasi delle prime time di programmazione che trasmettono. Nel 1972, le nuove regole sfociarono nella decisione dell' azienda di suddividere ABC Films in due società distinte: Worldvision Enterprises, che produceva e distribuiva programmi per la sindacazione statunitense, e ABC Circle Films come unità di produzione. Worldvision è stata venduta ad un consorzio di dirigenti ABC per quasi 10 milioni di dollari. 1970.\",\n",
       "   'qas': [{'answers': [{'answer_start': 4, 'text': '1970'},\n",
       "      {'answer_start': 4, 'text': '1970'}],\n",
       "     'id': '57274633dd62a815002e9a4e',\n",
       "     'question': 'In quale anno la FCC ha votato a favore delle regole di interesse finanziario e sindacato?'},\n",
       "    {'answers': [{'answer_start': 287, 'text': '1972'},\n",
       "      {'answer_start': 287, 'text': '1972'},\n",
       "      {'answer_start': 287, 'text': '1972'}],\n",
       "     'id': '57274633dd62a815002e9a4f',\n",
       "     'question': 'Quale nuova suddivisione dei film ABC ha prodotto la programmazione per la sindacazione negli Stati Uniti?'}]},\n",
       "  {'context': \"Nell' aprile 1970, il Congresso approvò la legge sul fumo delle sigarette per la sanità pubblica che vietava la pubblicità delle sigarette da tutte le reti televisive e radiofoniche, compresa la ABC, quando entrò in vigore il 2 gennaio 1971. Citando la limitata redditività dei suoi cinema, ABC Great States, la divisione centro-occidentale dei teatri ABC, fu venduta a Henry Plitt nel 1974. Il 17 gennaio 1972, Elton Rule è stato nominato Presidente e Chief Operating Officer di ABC pochi mesi dopo che Goldenson ha ridotto il suo ruolo nella società dopo aver subito un attacco di cuore.\",\n",
       "   'qas': [{'answers': [{'answer_start': 112,\n",
       "       'text': 'pubblicità delle sigarette da tutte le reti televisive e radiofoniche'},\n",
       "      {'answer_start': 13, 'text': '1970'}],\n",
       "     'id': '572746d3dd62a815002e9a66',\n",
       "     'question': 'Che cosa ha vietato la legge sul fumo nel settore della sanità pubblica?'},\n",
       "    {'answers': [{'answer_start': 226, 'text': '2 gennaio 1971'},\n",
       "      {'answer_start': 226, 'text': '2 gennaio 1971'},\n",
       "      {'answer_start': 226, 'text': '2 gennaio 1971'}],\n",
       "     'id': '572746d3dd62a815002e9a67',\n",
       "     'question': 'Quando è entrato in vigore il divieto di pubblicità sulle sigarette per le reti televisive?'},\n",
       "    {'answers': [{'answer_start': 370, 'text': 'Henry Plitt'},\n",
       "      {'answer_start': 370, 'text': 'Henry Plitt'},\n",
       "      {'answer_start': 370, 'text': 'Henry Plitt'}],\n",
       "     'id': '572746d3dd62a815002e9a68',\n",
       "     'question': 'Chi erano i Grandi Stati ABC venduti nel 1974?'},\n",
       "    {'answers': [{'answer_start': 412, 'text': 'Elton Rule'},\n",
       "      {'answer_start': 412, 'text': 'Elton Rule'},\n",
       "      {'answer_start': 412, 'text': 'Elton Rule'}],\n",
       "     'id': '572746d3dd62a815002e9a69',\n",
       "     'question': 'Chi è stato nominato il presidente e CEO di ABC dopo Goldenson ha subito un attacco di cuore?'}]},\n",
       "  {'context': \"Nei primi anni' 70, Michael Eisner, entrato in ABC nel 1966, divenne il responsabile dello sviluppo del programma della rete. Ha contribuito a creare idee per molte serie, tra cui Happy Days (che ha avuto origine come segmento della serie antologica Love, American Style) e diverse soap opera; tuttavia, il merito principale di Eisner alla ABC è stato quello di aver sviluppato una programmazione orientata ai giovani. È stato responsabile della riacquisizione dei diritti per la Looney Tunes-Merrie Melodies biblioteca, portando i cortometraggi di nuovo a ABC dopo aver trascorso diversi anni in CBS, così come lo sviluppo di The Jackson 5ive serie animata 5ive serie e una serie di Osmonds, e Super Greenlighting Super Friends, basato su DC Comics' Justice League of America serie. Eisner lasciò ABC nel 1976 per diventare presidente di Paramount Pictures (e in seguito sarebbe diventato il presidente dell' eventuale società madre di ABC, Disney).\",\n",
       "   'qas': [{'answers': [{'answer_start': 55, 'text': '1966'},\n",
       "      {'answer_start': 55, 'text': '1966'},\n",
       "      {'answer_start': 55, 'text': '1966'}],\n",
       "     'id': '572747dd5951b619008f87a9',\n",
       "     'question': 'Quando Michael Eisner si è unito per la prima volta ad ABC?'},\n",
       "    {'answers': [{'answer_start': 180, 'text': 'Happy Days'},\n",
       "      {'answer_start': 180, 'text': 'Happy Days'},\n",
       "      {'answer_start': 180, 'text': 'Happy Days'}],\n",
       "     'id': '572747dd5951b619008f87aa',\n",
       "     'question': 'Quale serie ABC nasce come segmento breve su Love, American Style?'},\n",
       "    {'answers': [{'answer_start': 839, 'text': 'Paramount Pictures'},\n",
       "      {'answer_start': 839, 'text': 'Paramount Pictures'},\n",
       "      {'answer_start': 839, 'text': 'Paramount Pictures'}],\n",
       "     'id': '572747dd5951b619008f87ac',\n",
       "     'question': \"Di quale azienda Eisner è diventato presidente quando lasciò l' ABC nel 1976?\"}]},\n",
       "  {'context': \"Nella primavera del 1975, Fred Pierce, neo presidente di ABC Television, convinse Fred Silverman a diventare il primo presidente e direttore della programmazione della filiale indipendente di produzione televisiva ABC Entertainment, creata dall' omonima divisione di programmazione della rete. Nel 1974, ABC ha presentato in anteprima la serie di detective S. W. A. T. Nello stesso anno, la rete prese la decisione di competere con il notiziario telefonico mattutino della NBC Today. Il suo primo tentativo in questa competizione fu AM America, ma questo dimostra il successo non era semplice. Uno dei suoi affiliati, WCVB-TV ha debuttato al mattino show Good Day! Primo in anteprima nel 1973 come Good Morning, è stato innovativo per essere prodotto interamente su strada e trasmesso da località al di fuori dell' area di Boston.\",\n",
       "   'qas': [{'answers': [{'answer_start': 82, 'text': 'Fred Silverman'},\n",
       "      {'answer_start': 82, 'text': 'Fred Silverman'},\n",
       "      {'answer_start': 82, 'text': 'Fred Silverman'}],\n",
       "     'id': '572748745951b619008f87b2',\n",
       "     'question': 'Chi è stato il primo presidente e direttore della programmazione di ABC Entertainment?'},\n",
       "    {'answers': [{'answer_start': 357, 'text': 'S. W. A. T.'},\n",
       "      {'answer_start': 357, 'text': 'S. W. A. T.'}],\n",
       "     'id': '572748745951b619008f87b3',\n",
       "     'question': 'Nel 1974, quale serie di detective ha debuttato su ABC?'},\n",
       "    {'answers': [{'answer_start': 688, 'text': '1973'}],\n",
       "     'id': '572748745951b619008f87b4',\n",
       "     'question': 'Quando ha debuttato Good Morning America?'}]},\n",
       "  {'context': 'Gli anni Settanta sono stati evidenziati da diverse serie di successo commedia, fantasia, azione e supereroe a tema serie per la rete tra cui Kung Fu, The Six Million Dollar Man, Wonder Woman, Starsky & Hutch, Charlie\\'s Angels, The Bionic Woman, Fantasy Island e Battlestar Galactica. Molte di queste serie erano illuminate dal verde di Silverman, che lasciò ABC nel 1978 per diventare presidente della divisione intrattenimento della NBC. L\\' entusiasmante successo di Happy Days ha anche portato ad una serie spin-off di successo, Laverne & Shirley, che ha debuttato nel 1976. Gli Angeli di Charlie e la Three\\'s Company (che ha debuttato nel 1977) sono stati due esempi principali di una tendenza tra i principali network durante gli anni Settanta conosciuta come \"jiggle TV\", caratterizzata da attrattive donne, spesso di lusso, nei ruoli principali e ospiti.',\n",
       "   'qas': [{'answers': [{'answer_start': 532, 'text': 'Laverne & Shirley'},\n",
       "      {'answer_start': 532, 'text': 'Laverne & Shirley'},\n",
       "      {'answer_start': 532, 'text': 'Laverne & Shirley'}],\n",
       "     'id': '5727492f708984140094dbb6',\n",
       "     'question': 'Quale spinoff di Happy Days ha debuttato nel 1976?'},\n",
       "    {'answers': [{'answer_start': 766, 'text': 'jiggle TV'},\n",
       "      {'answer_start': 765, 'text': '\"jiggle TV\"'},\n",
       "      {'answer_start': 765, 'text': '\"jiggle TV\"'}],\n",
       "     'id': '5727492f708984140094dbb7',\n",
       "     'question': \"Quali tendenze televisive sono un esempio di Charlies Angels e Three's Company?\"}]},\n",
       "  {'context': \"Da parte sua, il network televisivo ha prodotto alcuni nuovi successi nel corso del 1977: a gennaio è stata presentata la prima di Roots, una miniserie basata su un romanzo di Alex Haley pubblicato l' anno precedente; a settembre, The Love Boat, una serie antologica commedia-drammatica prodotta da Aaron Spelling, che si basava sull' equipaggio di una nave da crociera e presentava tre storie centrate in parte sui vari passeggeri della nave; sebbene criticamente lambasted, la prima è stata la Roots ha continuato a diventare uno dei programmi più votati nella storia della televisione americana, con rating senza precedenti per il suo finale. Il successo di Roots, Happy Days e The Love Boat ha permesso alla rete di conquistare il primo posto nella classifica per la prima volta nella stagione 1976-77. Il 13 settembre 1977, la rete ha debuttato Soap, una controversa parodia di soap opera del sapone, nota per essere la prima serie televisiva ad avere un protagonista apertamente gay (interpretato da un allora sconosciuto Billy Crystal); l' ultima volta, il 20 aprile 1981, è stata lanciata in rete. Alex Haley.\",\n",
       "   'qas': [{'answers': [{'answer_start': 176, 'text': 'Alex Haley'},\n",
       "      {'answer_start': 176, 'text': 'Alex Haley'}],\n",
       "     'id': '572749d7dd62a815002e9a90',\n",
       "     'question': 'La miniserie Roots è stata basata su un romanzo di quale autore?'},\n",
       "    {'answers': [{'answer_start': 299, 'text': 'Aaron Spelling'},\n",
       "      {'answer_start': 299, 'text': 'Aaron Spelling'},\n",
       "      {'answer_start': 299, 'text': 'Aaron Spelling'}],\n",
       "     'id': '572749d7dd62a815002e9a91',\n",
       "     'question': 'Quante stagioni ha fatto correre The Love Boat?'},\n",
       "    {'answers': [{'answer_start': 789, 'text': 'stagione 1976-77'}],\n",
       "     'id': '572749d7dd62a815002e9a92',\n",
       "     'question': 'Durante quale stagione ABC ha conquistato il primo posto nella classifica televisiva?'},\n",
       "    {'answers': [{'answer_start': 898, 'text': 'sapone'},\n",
       "      {'answer_start': 898, 'text': 'sapone'},\n",
       "      {'answer_start': 898, 'text': 'sapone'}],\n",
       "     'id': '572749d7dd62a815002e9a93',\n",
       "     'question': 'Qual è stata la prima serie televisiva con un personaggio apertamente gay?'}]},\n",
       "  {'context': 'Nel frattempo, ABC News, che si è formata come nuova divisione separata, ha cercato di diventare un leader globale nelle notizie televisive. Nel 1977, Roone Arledge è stato nominato presidente della nuova ABC News oltre ad essere presidente di ABC Sports. Nello stesso anno, ABC ha lanciato un\\' importante espansione delle sue strutture per uffici a New York City. La società ha costruito per la prima volta un nuovo edificio di 10 piani su un terreno precedentemente occupato da un magazzino abbandonato all\\' angolo tra Columbus Avenue e West 66th Street; l\\' impianto che è stato costruito al suo posto è soprannominato \"7 Lincoln Square\" (anche se in realtà si trova in 149 Columbus Avenue). Nel frattempo, un ex parcheggio, situato a 30 West 67th Street, è stato trasformato in un imponente edificio di 15 piani. Entrambi gli edifici sono stati completati nel giugno 1979. WABC-TV ha spostato le sue attività dagli uffici del 77 West 66th Street a 149 Columbus Avenue, liberando spazio per la rete ABC per ospitare alcune delle sue attività.',\n",
       "   'qas': [{'answers': [{'answer_start': 151, 'text': 'Roone Arledge'},\n",
       "      {'answer_start': 151, 'text': 'Roone Arledge'},\n",
       "      {'answer_start': 151, 'text': 'Roone Arledge'}],\n",
       "     'id': '57274a8cf1498d1400e8f5b4',\n",
       "     'question': 'Chi è stato nominato presidente di ABC News nel 1977?'},\n",
       "    {'answers': [{'answer_start': 244, 'text': 'ABC Sport'},\n",
       "      {'answer_start': 244, 'text': 'ABC Sport'},\n",
       "      {'answer_start': 244, 'text': 'ABC Sport'}],\n",
       "     'id': '57274a8cf1498d1400e8f5b5',\n",
       "     'question': 'Oltre a ABC News, di quale altra divisione era Roone Arledge presidente?'},\n",
       "    {'answers': [{'answer_start': 622, 'text': '7 Lincoln Square'},\n",
       "      {'answer_start': 622, 'text': '7 Lincoln Square'},\n",
       "      {'answer_start': 622, 'text': '7 Lincoln Square'}],\n",
       "     'id': '57274a8cf1498d1400e8f5b6',\n",
       "     'question': 'Qual è il soprannome della struttura ABC costruita a Columbus Avenue e West 66th Street?'},\n",
       "    {'answers': [{'answer_start': 863, 'text': 'giugno 1979'},\n",
       "      {'answer_start': 863, 'text': 'giugno 1979'},\n",
       "      {'answer_start': 870, 'text': '1979'}],\n",
       "     'id': '57274a8cf1498d1400e8f5b7',\n",
       "     'question': 'Quando sono stati completati 7 Lincoln Square?'}]},\n",
       "  {'context': \"Nel giugno 1978 Arledge creò il newsmagazine 20/20; dopo il suo primo episodio ricevuto duramente negativo recensioni, il programma - che debuttò come serie estiva, prima di diventare un programma tutto l' anno nel 1979 - fu subito rinnovato per presentare un mix di storie approfondite e interviste, con Hugh Downs nominato come suo ancoraggio (più tardi accoppiato con il suo ex collega oggi Barbara Walters). Nel febbraio 1979, ABC ha venduto la sua divisione di registrazione a MCA Inc. per 20 milioni di dollari; l' etichetta è stata interrotta entro il 5 marzo dello stesso anno, e tutti i suoi 300 dipendenti sono stati licenziati (i diritti per le opere di ABC Records e tutte le altre etichette di MCA sono stati da allora acquistati da Universal Music Group).\",\n",
       "   'qas': [{'answers': [{'answer_start': 4, 'text': 'giugno 1978'},\n",
       "      {'answer_start': 4, 'text': 'giugno 1978'},\n",
       "      {'answer_start': 4, 'text': 'giugno 1978'}],\n",
       "     'id': '57274baff1498d1400e8f5dc',\n",
       "     'question': 'Quando è stato creato il newsmagazine 20/20?'},\n",
       "    {'answers': [{'answer_start': 305, 'text': 'Hugh Downs'},\n",
       "      {'answer_start': 305, 'text': 'Hugh Downs'},\n",
       "      {'answer_start': 305, 'text': 'Hugh Downs'}],\n",
       "     'id': '57274baff1498d1400e8f5dd',\n",
       "     'question': \"Chi è stato nominato per la prima volta all' ancora del 20/20?\"},\n",
       "    {'answers': [{'answer_start': 394, 'text': 'Barbara Walters'},\n",
       "      {'answer_start': 394, 'text': 'Barbara Walters'},\n",
       "      {'answer_start': 394, 'text': 'Barbara Walters'}],\n",
       "     'id': '57274baff1498d1400e8f5de',\n",
       "     'question': 'Quale ancora femmina di oggi si è unita a Hugh Downs il 20/20?'},\n",
       "    {'answers': [{'answer_start': 482, 'text': 'MCA Inc.'},\n",
       "      {'answer_start': 482, 'text': 'MCA Inc.'},\n",
       "      {'answer_start': 482, 'text': 'MCA Inc.'}],\n",
       "     'id': '57274baff1498d1400e8f5df',\n",
       "     'question': \"Chi l' ha venduta nel 1979?\"}]},\n",
       "  {'context': \"Al fine di competere con CNN, ABC ha proposto un canale di informazione 24 ore su 24 denominato ABC Cable News, con l' intenzione di lanciare la rete nel 1995; tuttavia, il piano sarà in ultima analisi accantonato dalla direzione dell' impresa. Nel luglio 2004 ABC riproverebbe tale concetto con il lancio di ABC News Now, un canale d' informazione 24 ore su 24 distribuito per la visione su Internet e sui telefoni cellulari. Il 29 agosto 1994, ABC ha acquistato da SJL Broadcast Management, a Toledo, Ohio (precedentemente affiliata ad ABC dal 1958 al 1970), Flint, affiliata del Michigan WJRT-TV e WTVG a Toledo, Ohio (già affiliata ad ABC dal 1958 al 1970), e quest' ultima è passata ad ABC una volta scaduto il suo contratto con NBC due mesi dopo la conclusione dell' acquisto all' inizio del 1995. Entrambe le stazioni sono state acquistate come piano di emergenza nel caso in cui CBS raggiungesse un accordo di affiliazione con WXYZ-TV (in sostituzione di WJBK, che è passata a Fox in seguito all' accordo di affiliazione di gruppo con New World Communications) per consentire alla rete di mantenere una presenza in volo sul mercato di Detroit (l' E. W.). Scripps Company e ABC avrebbe raggiunto un accordo di affiliazione di gruppo che ha rinnovato gli accordi di affiliazione con WXYZ e WEWS, e passare ad altre quattro stazioni, tra cui due le cui affiliazioni Fox sono stati spostati dall' accordo Nuovo Mondo, con la rete.\",\n",
       "   'qas': [{'answers': [{'answer_start': 96, 'text': 'ABC Cable News'},\n",
       "      {'answer_start': 96, 'text': 'ABC Cable News'}],\n",
       "     'id': '57274cac708984140094dbdd',\n",
       "     'question': 'Quale canale ABC ha proposto di entrare in concorrenza con CNN, società di informazione via cavo?'},\n",
       "    {'answers': [{'answer_start': 309, 'text': 'ABC News Now'},\n",
       "      {'answer_start': 309, 'text': 'ABC News Now'},\n",
       "      {'answer_start': 309, 'text': 'ABC News Now'}],\n",
       "     'id': '57274cac708984140094dbde',\n",
       "     'question': 'Quale rete di notizie ha lanciato ABC nel luglio 2004?'},\n",
       "    {'answers': [{'answer_start': 591, 'text': 'WJRT-TV'},\n",
       "      {'answer_start': 591, 'text': 'WJRT-TV'},\n",
       "      {'answer_start': 591, 'text': 'WJRT-TV'}],\n",
       "     'id': '57274cac708984140094dbdf',\n",
       "     'question': \"Nell' agosto 1994, quale affiliata di Flint, Michigan, ha acquistato ABC?\"},\n",
       "    {'answers': [{'answer_start': 601, 'text': 'WTVG'},\n",
       "      {'answer_start': 601, 'text': 'WTVG'},\n",
       "      {'answer_start': 601, 'text': 'WTVG'}],\n",
       "     'id': '57274cac708984140094dbe0',\n",
       "     'question': \"Nell' agosto 1994, che cosa ha acquistato l' affiliata di Toledo, Ohio, ABC?\"}]},\n",
       "  {'context': \"Lo sciopero Writers Guild of America che ha fermato la produzione di programmi di rete per gran parte della stagione 2007-08 influenzato la rete nel 2007-08 e 2008-09, come dimostra vari ABC ha mostrato che in anteprima nel 2007, come Dirty Sexy Money, Pushing Daisies, Eli Stone e Samantha Who? Uno dei programmi di sciopero-sostituzione della rete durante quel periodo era il gioco spettacolo Duel, che ha debuttato nel dicembre 2007. Il programma sarebbe diventato un piccolo successo per la rete durante la sua corsa iniziale a sei episodi, che ha portato ABC a rinnovare Duel come serie regolare a partire dall' aprile 2008. Tuttavia, Duel ha sofferto di scarsa visibilità durante la sua esecuzione come una serie regolare, e ABC ha annullato il programma dopo sedici episodi. Il 15 agosto 2008, Disney Disney ha negato le voci iniziato da Caris & Co. che sarebbe vendere i dieci ABC di proprietà e gestiti stazioni. Scrittori Guild of America. La Gilda degli Scrittori d' America.\",\n",
       "   'qas': [{'answers': [{'answer_start': 922,\n",
       "       'text': 'Scrittori Guild of America'}],\n",
       "     'id': '57274d905951b619008f87e1',\n",
       "     'question': 'Uno sciopero da parte di quale entità ha causato un arresto della produzione per i programmi di rete nella stagione 2007-2008?'},\n",
       "    {'answers': [{'answer_start': 845, 'text': 'Caris & Co.'},\n",
       "      {'answer_start': 845, 'text': 'Caris & Co'},\n",
       "      {'answer_start': 845, 'text': 'Caris & Co.'}],\n",
       "     'id': '57274d905951b619008f87e2',\n",
       "     'question': 'Chi ha iniziato le voci nel 2008 che ABC venderebbe le sue dieci stazioni di proprietà e gestite?'}]},\n",
       "  {'context': \"All' inizio del 2009, Disney-ABC Television Group ha fuso ABC Entertainment e ABC Studios in una nuova divisione, ABC Entertainment Group, che sarà responsabile sia della produzione che della trasmissione. Nel corso di tale riorganizzazione, il gruppo ha annunciato il licenziamento del 5% dell' organico. Il 2 aprile 2009 Citadel Communications ha annunciato che avrebbe rinnovato il marchio di ABC Radio come Citadel Media; tuttavia, ABC News ha continuato a fornire contenuti informativi per Citadel. Il 22 dicembre, Disney-ABC Television Group ha annunciato una partnership con Apple Inc. per rendere i singoli episodi dei programmi di ABC e Disney Channel disponibili per l' acquisto su iTunes.\",\n",
       "   'qas': [{'answers': [{'answer_start': 58, 'text': 'ABC Entertainment'},\n",
       "      {'answer_start': 58, 'text': 'ABC Entertainment'},\n",
       "      {'answer_start': 78, 'text': 'ABC Studios'}],\n",
       "     'id': '57274e6a5951b619008f87f1',\n",
       "     'question': 'Il gruppo televisivo Disney-ABC si è fuso con ABC Studios e quale altra entità nel 2009?'},\n",
       "    {'answers': [{'answer_start': 114, 'text': 'ABC Entertainment Group'},\n",
       "      {'answer_start': 114, 'text': 'ABC Entertainment Group'},\n",
       "      {'answer_start': 118, 'text': 'Entertainment Group'}],\n",
       "     'id': '57274e6a5951b619008f87f2',\n",
       "     'question': \"Dopo la fusione del gruppo televisivo Disney-ABC Television, ABC Studios e ABC Entertainment, qual è stata l' entità che ne è derivata?\"},\n",
       "    {'answers': [{'answer_start': 411, 'text': 'Citadel Media'},\n",
       "      {'answer_start': 411, 'text': 'Citadel Media'},\n",
       "      {'answer_start': 411, 'text': 'Citadel Media'}],\n",
       "     'id': '57274e6a5951b619008f87f3',\n",
       "     'question': 'Nel 2009, cosa ha cambiato il marchio di Citadel Communications nel 2009?'},\n",
       "    {'answers': [{'answer_start': 692, 'text': 'iTunes'},\n",
       "      {'answer_start': 692, 'text': 'iTunes'},\n",
       "      {'answer_start': 692, 'text': 'iTunes'}],\n",
       "     'id': '57274e6a5951b619008f87f4',\n",
       "     'question': 'Il 22 dicembre 2009, ABC ha raggiunto un accordo con Apple per rendere disponibili gli spettacoli ABC su quale servizio?'}]},\n",
       "  {'context': \"La rete ha iniziato a scontrarsi con qualche problema nei rating entro il 2010. Quell' anno, la sesta e ultima stagione di Lost è diventata la stagione più bassa del dramma dal suo debutto nel 2004. Le valutazioni per il colpo un tempo incessante di Ugly Betty sono crollate drammaticamente dopo essere stato spostato al venerdì all' inizio della sua quarta stagione nell' autunno del 2009; un tentativo di aumentare le valutazioni spostando il dramedy a mercoledì fallito, con la sua cancellazione definitiva da parte della rete suscitando reazioni negative da parte del pubblico, e in particolare fanbase dello spettacolo. Con i due precedenti successi della rete di due ex show ora fuori dalla foto, il resto della rete di veterano superiore mostra Casalinghe disperate e Anatomia di Grey, e un altro dramma di successo Fratelli & Sorelle, tutti si è conclusa la stagione 2009-10 registrato i loro voti più bassi mai.\",\n",
       "   'qas': [{'answers': [{'answer_start': 74, 'text': '2010'},\n",
       "      {'answer_start': 74, 'text': '2010'},\n",
       "      {'answer_start': 74, 'text': '2010'}],\n",
       "     'id': '57274f07708984140094dbed',\n",
       "     'question': 'Quale anno ha segnato le valutazioni più basse per il dramma Lost?'},\n",
       "    {'answers': [{'answer_start': 193, 'text': '2004'},\n",
       "      {'answer_start': 193, 'text': '2004'},\n",
       "      {'answer_start': 193, 'text': '2004'}],\n",
       "     'id': '57274f07708984140094dbee',\n",
       "     'question': 'In quale anno ha perso il primo debutto Lost?'},\n",
       "    {'answers': [{'answer_start': 321, 'text': 'venerdì'},\n",
       "      {'answer_start': 321, 'text': 'venerdì'},\n",
       "      {'answer_start': 321, 'text': 'venerdì'}],\n",
       "     'id': '57274f07708984140094dbef',\n",
       "     'question': 'Cattive valutazioni di Ugly Betty cadde drammaticamente dopo il film di serie a che notte?'},\n",
       "    {'answers': [{'answer_start': 455, 'text': 'mercoledì'},\n",
       "      {'answer_start': 455, 'text': 'mercoledì'},\n",
       "      {'answer_start': 455, 'text': 'mercoledì'}],\n",
       "     'id': '57274f07708984140094dbf0',\n",
       "     'question': 'A quale notte ABC si è spostata Ugly Betty nel tentativo di aumentare le classifiche della serie?'}]},\n",
       "  {'context': \"Quando la FCC ha imposto le sue regole di sinterizzazione a pinne nel 1970, ABC ha creato proattivamente due società: Worldvision Enterprises come distributore di sindacazione e ABC Circle Films come società di produzione. Tuttavia, tra la pubblicazione e l' attuazione di questi regolamenti, la separazione del catalogo della rete è stata fatta nel 1973. I diritti di trasmissione delle produzioni antecedenti al 1973 sono stati trasferiti a Worldvision, che è diventata indipendente nello stesso anno. La società è stata venduta diverse volte da quando Paramount Television l' ha acquisita nel 1999, ed è stata recentemente incorporata in CBS Television Distribution, un' unità di CBS Corporation. Tuttavia, Worldvision ha venduto parti del suo catalogo, tra cui le librerie Ruby-Spears e Hanna-Barbera, a Turner Broadcasting System nel 1990. Con l' acquisto di ABC da parte di Disney nel 1996, ABC Circle Films è stata incorporata in Touchstone Television, una controllata di Disney che a sua volta è stata ribattezzata ABC Studios nel 2007.\",\n",
       "   'qas': [{'answers': [{'answer_start': 70, 'text': '1970'},\n",
       "      {'answer_start': 70, 'text': '1970'},\n",
       "      {'answer_start': 70, 'text': '1970'}],\n",
       "     'id': '5727504b5951b619008f881d',\n",
       "     'question': \"Quando ha iniziato l' FCC a imporre regole di sincronia delle pinne?\"},\n",
       "    {'answers': [{'answer_start': 118, 'text': 'Worldvision Enterprises'},\n",
       "      {'answer_start': 118, 'text': 'Worldvision Enterprises'},\n",
       "      {'answer_start': 118, 'text': 'Worldvision Enterprises'}],\n",
       "     'id': '5727504b5951b619008f881e',\n",
       "     'question': 'ABC ha creato quale azienda come distributore di sindacato in risposta alle regole di sinterizzazione delle pinne della FCC?'},\n",
       "    {'answers': [{'answer_start': 178, 'text': 'ABC Circle Films'},\n",
       "      {'answer_start': 178, 'text': 'ABC Circle Films'},\n",
       "      {'answer_start': 178, 'text': 'ABC Circle Films'}],\n",
       "     'id': '5727504b5951b619008f881f',\n",
       "     'question': 'ABC ha creato quale impresa come azienda di produzione in risposta alle regole di sinterizzazione delle pinne?'},\n",
       "    {'answers': [{'answer_start': 808, 'text': 'Turner Broadcasting System'},\n",
       "      {'answer_start': 808, 'text': 'Turner Broadcasting System'},\n",
       "      {'answer_start': 808, 'text': 'Turner Broadcasting System'}],\n",
       "     'id': '5727504b5951b619008f8820',\n",
       "     'question': 'A quale società Worldvision ha venduto una parte del suo catalogo nel 1990?'}]},\n",
       "  {'context': \"Fa parte della biblioteca anche la summenzionata biblioteca Selznick, la Cinerama Productions/Palomar biblioteca teatrale e il catalogo Selmur Productions che la rete ha acquisito alcuni anni fa, e le produzioni interne che continua a produrre (come ad esempio America's Funniest Home Videos, General Hospital, e ABC produzioni News), anche se Disney-ABC Domestic Television (precedentemente noto come Buena Vista Televisione) gestisce la distribuzione televisiva nazionale Disney-ABC televisione nazionale.\",\n",
       "   'qas': [{'answers': [{'answer_start': 474,\n",
       "       'text': 'Disney-ABC televisione nazionale'},\n",
       "      {'answer_start': 474, 'text': 'Disney-ABC televisione nazionale'}],\n",
       "     'id': '57275273dd62a815002e9b16',\n",
       "     'question': 'Quale divisione ABC si occupa della distribuzione televisiva nazionale?'},\n",
       "    {'answers': [{'answer_start': 402, 'text': 'Buena Vista Television'},\n",
       "      {'answer_start': 402, 'text': 'Buena Vista Television'},\n",
       "      {'answer_start': 402, 'text': 'Buena Vista Television'}],\n",
       "     'id': '57275273dd62a815002e9b17',\n",
       "     'question': 'Che cosa era la televisione internazionale Disney-ABC precedentemente conosciuta come televisione internazionale?'},\n",
       "    {'answers': [{'answer_start': 60, 'text': 'Selznick'}],\n",
       "     'id': '57275273dd62a815002e9b18',\n",
       "     'question': 'Quale biblioteca contiene il catalogo di Selmur Productions?'}]},\n",
       "  {'context': 'Fin dalla sua nascita, ABC ha avuto molte stazioni affiliate, che includono WABC-TV e WPVI-TV, le prime due stazioni a gestire la programmazione della rete. A partire dal marzo 2015[aggiornamento], ABC ha otto stazioni di proprietà e gestite, e accordi di affiliazione in corso e pendenti con 235 stazioni televisive aggiuntive che comprendono 49 stati, il Distretto di Columbia, quattro possedimenti statunitensi, Bermuda e Saba; questo rende ABC la più grande rete televisiva televisiva televisiva statunitense per numero totale di affiliati. La rete ha una portata nazionale stimata del 96,26% di tutte le famiglie negli Stati Uniti (o 300.794.157 americani con almeno un televisore). WABC-TV e WPVI-TV. WABC-TV e WPVI-TV.',\n",
       "   'qas': [{'answers': [{'answer_start': 76, 'text': 'WABC-TV e WPVI-TV'}],\n",
       "     'id': '57275339dd62a815002e9b28',\n",
       "     'question': 'Quali sono state le prime due stazioni che hanno portato la programmazione di ABC?'},\n",
       "    {'answers': [{'answer_start': 293,\n",
       "       'text': '235 stazioni televisive aggiuntive'},\n",
       "      {'answer_start': 293, 'text': '235'},\n",
       "      {'answer_start': 293, 'text': '235'}],\n",
       "     'id': '57275339dd62a815002e9b29',\n",
       "     'question': 'Con quante stazioni ABC hanno stipulato accordi di affiliazione nel 2015?'},\n",
       "    {'answers': [{'answer_start': 293, 'text': '235'},\n",
       "      {'answer_start': 293, 'text': '235'}],\n",
       "     'id': '57275339dd62a815002e9b2a',\n",
       "     'question': 'Quale percentuale di famiglie americane ha raggiunto ABC nel marzo 2015?'}]},\n",
       "  {'context': 'Il logo ABC si è evoluto molte volte dalla creazione della rete nel 1943. Il primo logo della rete, introdotto nel 1946, consisteva in uno schermo televisivo contenente le lettere \"T\" e \"V\", con al centro un microfono ABC verticale che rimanda alle radici della rete radio. Quando la fusione ABC-UPT fu finalizzata nel 1953, la rete introdusse un nuovo logo basato sul sigillo della Federal Communications Commission, con le lettere \"ABC\" racchiuse in uno scudo circolare sormontato dall\\' aquila calvo. Nel 1957, poco prima che la rete televisiva iniziasse le sue prime trasmissioni a colori, il logo ABC consisteva in una minuscola minuscola \"abc\" al centro di una grande lettera minuscola a, un disegno noto come \"ABC Circle A\".',\n",
       "   'qas': [{'answers': [{'answer_start': 115, 'text': '1946'},\n",
       "      {'answer_start': 115, 'text': '1946'},\n",
       "      {'answer_start': 68, 'text': '1943'}],\n",
       "     'id': '572754b5dd62a815002e9b44',\n",
       "     'question': 'Quando è stato introdotto il primo logo ABC?'},\n",
       "    {'answers': [{'answer_start': 319, 'text': '1953'}],\n",
       "     'id': '572754b5dd62a815002e9b45',\n",
       "     'question': 'Su cosa si basava il logo di ABC dopo la finalizzazione della fusione ABC-UPT?'},\n",
       "    {'answers': [{'answer_start': 507, 'text': '1957'},\n",
       "      {'answer_start': 507, 'text': '1957'},\n",
       "      {'answer_start': 507, 'text': '1957'}],\n",
       "     'id': '572754b5dd62a815002e9b46',\n",
       "     'question': 'Quando ABC ha adottato il suo logo iconico cerchio?'}]},\n",
       "  {'context': \"Con la cancellazione del 2011 di Supernanny, Extreme Makeover: Home Edition è diventato l' unico programma rimanente sulla programmazione della rete che è stato trasmesso in 4:3 definizione standard. Tutta la programmazione della rete è stata presentata in HD dal gennaio 2012 (ad eccezione di alcune specialità vacanza prodotte prima del 2005 - come le Peanuts specials e Rudolph's Shiny New Year - che continuano ad essere presentate in 4:3 SD), quando Extreme Makeover: Home Edition ha terminato la sua esecuzione come una serie regolare e One Life to Live (che era stato presentato in 16:9 definizione standard dal 2010) anche terminato la sua esecuzione ABC. L' affiliato-sindicato Sabato mattina E / I blocco E / I Weekend Aventure di Litton è anche trasmesso in HD, ed è stato il primo blocco di programmi per bambini su qualsiasi rete di trasmissione degli Stati Uniti per i programmi disponibili nel formato al suo debutto settembre 2011. 2011.\",\n",
       "   'qas': [{'answers': [{'answer_start': 25, 'text': '2011'},\n",
       "      {'answer_start': 25, 'text': '2011'}],\n",
       "     'id': '57275573708984140094dc45',\n",
       "     'question': 'In quale anno è stato annullato Supernanny?'},\n",
       "    {'answers': [{'answer_start': 257, 'text': 'HD'},\n",
       "      {'answer_start': 257, 'text': 'HD'},\n",
       "      {'answer_start': 257, 'text': 'HD'}],\n",
       "     'id': '57275573708984140094dc46',\n",
       "     'question': 'Da gennaio 2012, tutta la programmazione ABC è stata presentata in quale formato?'},\n",
       "    {'answers': [{'answer_start': 721, 'text': 'Weekend Aventure di Litton'},\n",
       "      {'answer_start': 721, 'text': 'Weekend Aventure di Litton'},\n",
       "      {'answer_start': 721, 'text': 'Weekend Aventure di Litton'}],\n",
       "     'id': '57275573708984140094dc47',\n",
       "     'question': 'Qual era il programma del primo blocco di programmi per bambini ad essere trasmesso in HD?'}]},\n",
       "  {'context': \"Il feed master di ABC viene trasmesso in alta definizione 720p, il formato di risoluzione nativo per le proprietà televisive americane di The Walt Disney Company. Tuttavia, la maggior parte delle 16 stazioni affiliate ABC di Hearst Television trasmettono la programmazione della rete in 1080i HD, mentre altre 11 affiliate di proprietà di varie società portano il feed di rete in una definizione standard 480i o per motivi tecnici per le affiliate di altre grandi reti che portano la programmazione ABC su un sottocanale digitale o perchè un' affiliata ABC feed primario non ha ancora aggiornato le loro apparecchiature di trasmissione per consentire la presentazione dei contenuti in HD. 720p ad alta definizione.\",\n",
       "   'qas': [{'answers': [{'answer_start': 689,\n",
       "       'text': '720p ad alta definizione'},\n",
       "      {'answer_start': 58, 'text': '720p'}],\n",
       "     'id': '57275650708984140094dc5f',\n",
       "     'question': 'In quale formato viene trasmesso il feed master di ABC?'},\n",
       "    {'answers': [{'answer_start': 310, 'text': '11'},\n",
       "      {'answer_start': 310, 'text': '11'},\n",
       "      {'answer_start': 310, 'text': '11'}],\n",
       "     'id': '57275650708984140094dc60',\n",
       "     'question': 'Quanti affiliati portano il feed della rete ABC in definizione standard 480i?'},\n",
       "    {'answers': [{'answer_start': 689, 'text': '720p ad alta definizione'},\n",
       "      {'answer_start': 689, 'text': '720p ad alta definizione'},\n",
       "      {'answer_start': 405, 'text': '480i'}],\n",
       "     'id': '57275650708984140094dc61',\n",
       "     'question': 'Qual è il formato nativo per le proprietà TV USA di Walt Disney Company?'}]},\n",
       "  {'context': \"Le difficoltà della rete nel sostenere le serie esistenti e nell' acquisire nuovi successi si sono riversate nel suo calendario 2010-11: i drammi di ABC durante quella stagione hanno continuato a fallire, con il dramma di indagine forense di metà stagione Body of Proof l' unico che si è rinnovato per una seconda stagione. Il network ha anche faticato a stabilire nuove commedie per sostenere i debutti dell' anno precedente, con solo la premiere di fine stagione Happy Endings guadagnando una seconda stagione. Nel frattempo, i nuovi bassi colpiti da Brothers & Sisters hanno portato alla sua cancellazione, e l' unico rinnovamento drammatico dell' anno precedente, V, anche non è riuscito a guadagnare un' altra stagione dopo una corsa media bassa stagione. Nonostante questo e un altro sensibile calo dei rating, ABC riuscirebbe a superare di un margine maggiore rispetto all' anno precedente il terzo posto di NBC.\",\n",
       "   'qas': [{'answers': [{'answer_start': 256, 'text': 'Body of Proof'},\n",
       "      {'answer_start': 256, 'text': 'Body of Proof'},\n",
       "      {'answer_start': 256, 'text': 'Body of Proof'}],\n",
       "     'id': '57275743f1498d1400e8f680',\n",
       "     'question': \"Qual è stato l' unico dramma esteso per una seconda stagione per il calendario 2010-11?\"},\n",
       "    {'answers': [{'answer_start': 465, 'text': 'Happy Endings'},\n",
       "      {'answer_start': 465, 'text': 'Happy Endings'},\n",
       "      {'answer_start': 465, 'text': 'Happy Endings'}],\n",
       "     'id': '57275743f1498d1400e8f681',\n",
       "     'question': \"Qual è stata l' unica commedia a guadagnare una seconda stagione per il calendario 2010-11?\"},\n",
       "    {'answers': [{'answer_start': 915, 'text': 'NBC'},\n",
       "      {'answer_start': 915, 'text': 'NBC'},\n",
       "      {'answer_start': 915, 'text': 'NBC'}],\n",
       "     'id': '57275743f1498d1400e8f682',\n",
       "     'question': 'Quale rete ABC ha battuto per il terzo posto nella classifica televisiva nel 2010-11?'},\n",
       "    {'answers': [{'answer_start': 79, 'text': 'V'},\n",
       "      {'answer_start': 79, 'text': 'V'},\n",
       "      {'answer_start': 553, 'text': 'Brothers & Sisters'}],\n",
       "     'id': '57275743f1498d1400e8f683',\n",
       "     'question': 'Quale programma drammatico è stato annullato da ABC dopo una brutta metà stagione eseguito nel 2010-11?'}]},\n",
       "  {'context': \"Il 14 aprile 2011, ABC ha annullato le lunghe opere di sapone All My Children e One Life to Live dopo 41 e 43 anni in onda, rispettivamente (in seguito al contraccolpo dei tifosi, ABC ha venduto i diritti ad entrambi gli spettacoli a Prospect Park, che alla fine ha rilanciato i saponi su Hulu per un' ulteriore stagione nel 2013 e con entrambe le società che si citano in giudizio per accuse di interferenza con il processo di rilancio degli spettacoli, mancato pagamento delle tasse di licenza. Il talk/lifestyle show che ha sostituito One Life to Live, The Revolution, non è riuscito a generare giudizi soddisfacenti ed è stato a sua volta annullato dopo soli sette mesi. La stagione 2011-12 ha visto l' ABC cadere al quarto posto nel 18-49 demografico nonostante rinnovando una manciata di nuovi spettacoli (compresi i drammi matricole Scandal, Revenge e Once Upon a Time) per la seconda stagione.\",\n",
       "   'qas': [{'answers': [{'answer_start': 234, 'text': 'Prospect Park'},\n",
       "      {'answer_start': 234, 'text': 'Prospect Park'},\n",
       "      {'answer_start': 234, 'text': 'Prospect Park'}],\n",
       "     'id': '5727590df1498d1400e8f6b7',\n",
       "     'question': 'ABC ha venduto i diritti a All My Children e One Life to Live?'},\n",
       "    {'answers': [{'answer_start': 289, 'text': 'Hulu'},\n",
       "      {'answer_start': 289, 'text': 'Hulu'},\n",
       "      {'answer_start': 289, 'text': 'Hulu'}],\n",
       "     'id': '5727590df1498d1400e8f6b8',\n",
       "     'question': 'Su quale servizio sono stati risvegliati All My Children and One Life to Live per una stagione?'},\n",
       "    {'answers': [{'answer_start': 556, 'text': 'The Revolution'},\n",
       "      {'answer_start': 556, 'text': 'The Revolution'},\n",
       "      {'answer_start': 556, 'text': 'The Revolution'}],\n",
       "     'id': '5727590df1498d1400e8f6b9',\n",
       "     'question': 'Quale talk show ha sostituito One Life to Live?'},\n",
       "    {'answers': [{'answer_start': 738, 'text': '18-49 demografico'}],\n",
       "     'id': '5727590df1498d1400e8f6ba',\n",
       "     'question': \"Il 2011-12 ha visto l' ABC scendere al 4° posto tra i rating tra quelli demografici più importanti?\"}]},\n",
       "  {'context': 'La stagione 2013-14 è stato un leggero miglioramento per ABC con tre nuovi successi in The Goldberg, agenti di S. H. I. I. E. L. D., e la risurrezione, che sono stati rinnovati, ma che la stagione ha visto le cancellazioni di holdover i vicini (che languiva nel suo nuovo slot di venerdì tempo nonostante essere prenotato da Last Man Standing e Shark Tank) e Suburgatory. NBC, che aveva ritardato dietro ABC per otto anni, ha terminato la stagione al primo posto nel 18-49 demografico per la prima volta dal 2004, e al secondo posto in total viewership dietro a lungo dominante CBS. ABC se stessa finirebbe la stagione al terzo posto come Fox si schiantò al quarto posto in entrambi i demografia.',\n",
       "   'qas': [{'answers': [{'answer_start': 578, 'text': 'CBS'},\n",
       "      {'answer_start': 578, 'text': 'CBS'},\n",
       "      {'answer_start': 578, 'text': 'CBS'}],\n",
       "     'id': '572759c1f1498d1400e8f6cb',\n",
       "     'question': 'Nel 2013-14, NBC ha finito dietro quale rete nelle valutazioni?'},\n",
       "    {'answers': [{'answer_start': 101,\n",
       "       'text': 'agenti di S. H. I. I. E. L. D.'},\n",
       "      {'answer_start': 101, 'text': 'agenti di S. H. I. I. E. L. D.'}],\n",
       "     'id': '572759c1f1498d1400e8f6cc',\n",
       "     'question': 'Quale serie di Comic Book ha debuttato ABC nel 2013-14?'},\n",
       "    {'answers': [{'answer_start': 235, 'text': 'i vicini'},\n",
       "      {'answer_start': 235, 'text': 'i vicini'},\n",
       "      {'answer_start': 101, 'text': 'agenti di S. H. I. I. E. L. D.'}],\n",
       "     'id': '572759c1f1498d1400e8f6cd',\n",
       "     'question': 'Quale programma che ha trasmesso tra Last Man Standing e Shark Tank è stato annullato da ABC nel 2013?'}]},\n",
       "  {'context': \"Tra i pochi punti luminosi durante questa stagione sono stati il crimine di mezza stagione dramedy Castle così come il successo di due sitcoms famiglia che ha ancorato la rete rivisitata linea commedia di mercoledì, The Middle and Modern Family, il secondo dei quali è stato sia un successo critico e commerciale. Shark Tank (basato sul formato di realtà del Dragon's Den) è diventato anche un dormiente di mezza stagione colpito domenica nella primavera del 2010; la stagione successiva, è diventato il tendostruttura del programma notturno di venerdì della rete, gradualmente contribuendo a rendere ABC un forte concorrente (dopo essere accoppiato con 20/20 e a partire dalla stagione 2012-13, il Tim Allen sitcom ultimo uomo in piedi) contro CBS' lungamente dominante drammatica / lineup di realta'.\",\n",
       "   'qas': [{'answers': [{'answer_start': 216,\n",
       "       'text': 'The Middle and Modern Family'},\n",
       "      {'answer_start': 216, 'text': 'The Middle and Modern Family'},\n",
       "      {'answer_start': 216, 'text': 'The Middle and Modern Family'}],\n",
       "     'id': '57275a505951b619008f889f',\n",
       "     'question': 'Quali due commedie sono state presentate nella nuova formazione commedia di mercoledì di ABC?'},\n",
       "    {'answers': [{'answer_start': 359, 'text': \"Dragon's Den\"},\n",
       "      {'answer_start': 359, 'text': \"Dragon's Den\"},\n",
       "      {'answer_start': 359, 'text': \"Dragon's Den\"}],\n",
       "     'id': '57275a505951b619008f88a0',\n",
       "     'question': 'Shark Tank Shark Tank si basava su quale altro reality show?'},\n",
       "    {'answers': [{'answer_start': 430, 'text': 'domenica'},\n",
       "      {'answer_start': 430, 'text': 'domenica'},\n",
       "      {'answer_start': 430, 'text': 'domenica'}],\n",
       "     'id': '57275a505951b619008f88a1',\n",
       "     'question': 'Su quale giorno della settimana ha debuttato Shark Tank?'},\n",
       "    {'answers': [{'answer_start': 699, 'text': 'Tim Allen'},\n",
       "      {'answer_start': 699, 'text': 'Tim Allen'},\n",
       "      {'answer_start': 699, 'text': 'Tim Allen'}],\n",
       "     'id': '57275a505951b619008f88a2',\n",
       "     'question': 'Nel 2012-13, ABC ha debuttato una commedia Last Man Standing con la commedia Last Man Standing, interpretando chi?'}]},\n",
       "  {'context': \"Daniel Burke è partito da Capital Cities/ABC nel febbraio 1994, e Thomas Murphy ha assunto la presidenza prima di cedere il controllo a Robert Iger. Settembre 1994 ha visto il debutto di NYPD Blue, una grintosa procedura di polizia di Steven Bochco (che ha creato Doogie Howser, MD). e il criticamente pillolava Cop Rock per ABC all' inizio del decennio); durato dieci stagioni, il dramma divenne noto per il suo confine spingendo gli standard televisivi di rete (in particolare il suo uso occasionale di linguaggio grafico e nudità posteriore), che ha portato alcuni affiliati a inizialmente rifiutarsi di trasmettere lo spettacolo nella sua prima stagione.\",\n",
       "   'qas': [{'answers': [{'answer_start': 0, 'text': 'Daniel Burke'},\n",
       "      {'answer_start': 0, 'text': 'Daniel Burke'},\n",
       "      {'answer_start': 0, 'text': 'Daniel Burke'}],\n",
       "     'id': '57275bfb708984140094dc97',\n",
       "     'question': 'Chi è partito come presidente di Capital Cities/ABC nel 1994?'},\n",
       "    {'answers': [{'answer_start': 66, 'text': 'Thomas Murphy'},\n",
       "      {'answer_start': 66, 'text': 'Thomas Murphy'},\n",
       "      {'answer_start': 66, 'text': 'Thomas Murphy'}],\n",
       "     'id': '57275bfb708984140094dc98',\n",
       "     'question': 'Chi ha inizialmente assunto la presidenza di Capital Cities/ABC dopo la partenza di Daniel Burke?'},\n",
       "    {'answers': [{'answer_start': 187, 'text': 'NYPD Blu'},\n",
       "      {'answer_start': 187, 'text': 'NYPD Blu'},\n",
       "      {'answer_start': 187, 'text': 'NYPD Blu'}],\n",
       "     'id': '57275bfb708984140094dc99',\n",
       "     'question': 'Quale dramma del crimine ha debuttato su ABC nel settembre 1994?'},\n",
       "    {'answers': [{'answer_start': 235, 'text': 'Steven Bochco'},\n",
       "      {'answer_start': 235, 'text': 'Steven Bochco'},\n",
       "      {'answer_start': 235, 'text': 'Steven Bochco'}],\n",
       "     'id': '57275bfb708984140094dc9a',\n",
       "     'question': 'Chi era il creatore di NYPD Blue?'},\n",
       "    {'answers': [{'answer_start': 363, 'text': 'dieci stagioni'},\n",
       "      {'answer_start': 363, 'text': 'dieci'},\n",
       "      {'answer_start': 363, 'text': 'dieci'}],\n",
       "     'id': '57275bfb708984140094dc9b',\n",
       "     'question': 'Quante stagioni ha trascorso NYPD Blue?'}]},\n",
       "  {'context': \"Nel 1993, la FCC ha abrogato le norme in materia di interessi finanziari e sindacati, consentendo ancora una volta alle reti di detenere partecipazioni in studi di produzione televisiva. Nello stesso anno, Capital Cities/ABC ha acquistato lo studio di animazione francese DIC Entertainment; ha inoltre firmato un accordo con Time Warner Cable per portare le sue stazioni televisive di proprietà e gestite sui sistemi del provider nei mercati ABC O&O. In quell' anno, ABC aveva una quota di audience totale del 23,63% delle famiglie americane, appena al di sotto del limite del 25% imposto dalla FCC.\",\n",
       "   'qas': [{'answers': [{'answer_start': 4, 'text': '1993'},\n",
       "      {'answer_start': 4, 'text': '1993'},\n",
       "      {'answer_start': 4, 'text': '1993'}],\n",
       "     'id': '57275cb3f1498d1400e8f6da',\n",
       "     'question': \"Quando sono state abrogate le norme relative all' interesse finanziario e alla sindacazione?\"},\n",
       "    {'answers': [{'answer_start': 272, 'text': 'DIC Entertainment'},\n",
       "      {'answer_start': 272, 'text': 'DIC Entertainment'},\n",
       "      {'answer_start': 272, 'text': 'DIC Entertainment'}],\n",
       "     'id': '57275cb3f1498d1400e8f6db',\n",
       "     'question': 'Quale studio di animazione francese ha acquistato ABC nel 1993?'},\n",
       "    {'answers': [{'answer_start': 325, 'text': 'Time Warner Cable'},\n",
       "      {'answer_start': 325, 'text': 'Time Warner Cable'},\n",
       "      {'answer_start': 325, 'text': 'Time Warner Cable'}],\n",
       "     'id': '57275cb3f1498d1400e8f6dc',\n",
       "     'question': 'Con quale fornitore di servizi via cavo ABC ha concluso nel 1993 un accordo per il trasporto delle sue stazioni di proprietà e gestite nei mercati ABC O&O?'},\n",
       "    {'answers': [{'answer_start': 510,\n",
       "       'text': '23,63% delle famiglie americane'},\n",
       "      {'answer_start': 510, 'text': '23,63% delle famiglie americane'}],\n",
       "     'id': '57275cb3f1498d1400e8f6dd',\n",
       "     'question': 'Qual era la quota di audience di ABC nel 1993?'}]},\n",
       "  {'context': \"Durante gli anni' 60, ABC continuò sulla stessa strada che iniziò a seguire a metà degli anni' 50, consolidando la rete come parte del suo sforzo di fidelizzare il pubblico. I finanziamenti della rete sono migliorati e hanno permesso di investire in altre proprietà e nella programmazione. Nel maggio 1960, ABC acquistò la stazione radiofonica di Chicago WLS, che dal 1920 condivideva il tempo di trasmissione con WENR. Questa acquisizione ha consentito ad ABC di consolidare la sua presenza sul mercato. Il 9 maggio 1960 WLS lancia una nuova linea di programmi ABC Radio. Nel 1960, l' imprenditore canadese John Bassett, che cercava di fondare una stazione televisiva a Toronto, cercò l' aiuto di ABC per lanciare la stazione. Leonard Goldenson ha accettato di acquisire una partecipazione del 25% in CFTO-TV; tuttavia, la legislazione della Commissione canadese per la radiotelevisione ha vietato la partecipazione di ABC, con il risultato che l' impresa si è ritirata dal progetto prima del lancio della stazione.\",\n",
       "   'qas': [{'answers': [{'answer_start': 355, 'text': 'WLS'},\n",
       "      {'answer_start': 355, 'text': 'WLS'},\n",
       "      {'answer_start': 355, 'text': 'WLS'}],\n",
       "     'id': '57275e125951b619008f88d7',\n",
       "     'question': 'Quale stazione radio ha acquistato ABC nel maggio 1960?'},\n",
       "    {'answers': [{'answer_start': 508, 'text': '9 maggio 1960'},\n",
       "      {'answer_start': 508, 'text': '9 maggio 1960'},\n",
       "      {'answer_start': 508, 'text': '9 maggio 1960'}],\n",
       "     'id': '57275e125951b619008f88d8',\n",
       "     'question': 'Quando ha lanciato WLS una serie di programmi radio ABC?'},\n",
       "    {'answers': [{'answer_start': 608, 'text': 'John Bassett'},\n",
       "      {'answer_start': 608, 'text': 'John Bassett'},\n",
       "      {'answer_start': 608, 'text': 'John Bassett'}],\n",
       "     'id': '57275e125951b619008f88d9',\n",
       "     'question': \"Quale investitore canadese ha chiesto l' aiuto di ABC per lanciare una stazione nel 1960?\"},\n",
       "    {'answers': [{'answer_start': 802, 'text': 'CFTO-TV'},\n",
       "      {'answer_start': 802, 'text': 'CFTO-TV'},\n",
       "      {'answer_start': 802, 'text': 'CFTO-TV'}],\n",
       "     'id': '57275e125951b619008f88da',\n",
       "     'question': 'In quale progetto Leonard Goldenson ha proposto di investire prima che la Commissione radiotelevisiva canadese si fosse pronunciata contro ABC?'}]},\n",
       "  {'context': \"Sempre alla ricerca di nuovi programmi che la aiutino a competere con NBC e CBS, il management di ABC credeva che lo sport potesse essere un importante catalizzatore per migliorare la quota di mercato della rete. Il 29 aprile 1961, ABC ha debuttato Wide World of Sports, una serie antologica creata da Edgar Scherick attraverso la sua azienda Sports Programs, Inc. e prodotta da un giovane Roone Arledge che ha presentato ogni trasmissione di un evento sportivo diverso. ABC ha acquistato Sports Programs, Inc. in cambio di azioni della società, portandola a diventare il futuro nucleo di ABC Sports, con Arledge come produttore esecutivo degli show di quella divisione. L' ampio mondo dello sport, in particolare, non era dedicato solo ad un singolo sport, ma piuttosto a tutti gli eventi sportivi in generale.\",\n",
       "   'qas': [{'answers': [{'answer_start': 674,\n",
       "       'text': 'ampio mondo dello sport'},\n",
       "      {'answer_start': 674, 'text': 'ampio mondo dello sport'},\n",
       "      {'answer_start': 674, 'text': 'ampio mondo dello sport'}],\n",
       "     'id': '57275e95f1498d1400e8f6f4',\n",
       "     'question': 'ABC ha debuttato quale programma sportivo il 29 aprile 1961?'},\n",
       "    {'answers': [{'answer_start': 302, 'text': 'Edgar Scherick'},\n",
       "      {'answer_start': 302, 'text': 'Edgar Scherick'},\n",
       "      {'answer_start': 302, 'text': 'Edgar Scherick'}],\n",
       "     'id': '57275e95f1498d1400e8f6f5',\n",
       "     'question': 'Chi ha creato il Wide World of Sports di ABC?'},\n",
       "    {'answers': [{'answer_start': 390, 'text': 'Roone Arledge'},\n",
       "      {'answer_start': 390, 'text': 'Roone Arledge'},\n",
       "      {'answer_start': 390, 'text': 'Roone Arledge'}],\n",
       "     'id': '57275e95f1498d1400e8f6f6',\n",
       "     'question': 'Chi ha originariamente prodotto Wide World of Sports per ABC?'}]},\n",
       "  {'context': 'Nel 1965, l\\' entità societaria, gli American Broadcasting-Paramount Theatres, fu ribattezzata \"American Broadcasting Companies\",  mentre la sua divisione cinema divenne ABC Theatres; nel 1966 la sua divisione di registrazione venne ribattezzata ABC Records. Nel dicembre di quell\\' anno, la rete televisiva ABC ha presentato in anteprima The Dating Game, una serie pionieristica nel suo genere, che è stata una rielaborazione del concetto di cieco data in cui un corteggiatore ha selezionato uno dei tre concorrenti vista invisibile sulla base delle risposte alle domande selezionate. A ciò seguì nel luglio 1966 The Newlywed Game, con tre coppie sposate di recente che indovinarono le risposte alle domande del partner (alcune delle quali erano piuttosto rischiose). Come ABC ha iniziato a superare le sue strutture al 7 West 66th Street, Goldenson ha trovato un nuovo quartier generale per ABC in un edificio di 44 piani situato al 1330 Avenue of the Americas di Manhattan, all\\' angolo della 54th Street (ora occupato dalla sede di New York del Financial Times). L\\' operazione ha consentito la conversione dei locali di 66th Street in impianti di produzione per programmi televisivi e radiofonici.',\n",
       "   'qas': [{'answers': [{'answer_start': 95,\n",
       "       'text': 'American Broadcasting Companies'},\n",
       "      {'answer_start': 95, 'text': 'American Broadcasting Companies'},\n",
       "      {'answer_start': 95, 'text': 'American Broadcasting Companies'}],\n",
       "     'id': '57275f6ef1498d1400e8f706',\n",
       "     'question': \"Qual è stata l' entità societaria American Broadcasting-Paramount Theatres ribattezzata nel 1965?\"},\n",
       "    {'answers': [{'answer_start': 337, 'text': 'The Dating Game'},\n",
       "      {'answer_start': 337, 'text': 'The Dating Game'},\n",
       "      {'answer_start': 337, 'text': 'The Dating Game'}],\n",
       "     'id': '57275f6ef1498d1400e8f707',\n",
       "     'question': 'Qual è stato il nome del programma \"blind date concept\" presentato da ABC nel 1966?'},\n",
       "    {'answers': [{'answer_start': 612, 'text': 'The Newlywed Game'},\n",
       "      {'answer_start': 612, 'text': 'The Newlywed Game'},\n",
       "      {'answer_start': 612, 'text': 'The Newlywed Game'}],\n",
       "     'id': '57275f6ef1498d1400e8f708',\n",
       "     'question': 'Quale videogioco con persone appena sposate è stato presentato da ABC nel luglio 1966?'}]},\n",
       "  {'context': \"ABC ha dominato il panorama televisivo americano nel corso degli anni' 70 e nei primi anni' 80 (da 1980, le tre principali reti rappresentano il 90% di tutti i telespettatori televisivi in prima serata negli Stati Uniti). Diverse serie ammiraglia ha debuttato sulla rete durante questo periodo tra cui Dynasty, un dramma opulento da Aaron Spelling che è diventato un successo quando ha debuttato come serie di mezza stagione nel 1981, cinque mesi prima di Spelling altri ABC colpito Charlie Angels ha concluso la sua corsa. Il network è stato anche spinto durante i primi anni' 80 dai continui successi di Happy Days, Three's Company, Laverne & Shirley e Fantasy Island, e ha guadagnato nuovi successi in Too Close for Comfort, Soap spinoff Benson e Happy Days spinoff Mork & Mindy. Nel 1981, ABC (attraverso la sua divisione ABC Video Services) lancia Alpha Repertory Television Service (ARTS), un canale via cavo gestito in joint venture con la Hearst Corporation che offre programmi culturali e artistici, trasmessi come servizio notturno sullo spazio del canale di Nickelodeon.\",\n",
       "   'qas': [{'answers': [{'answer_start': 145, 'text': '90%'},\n",
       "      {'answer_start': 145, 'text': '90%'},\n",
       "      {'answer_start': 145, 'text': '90%'}],\n",
       "     'id': '57276166dd62a815002e9bd8',\n",
       "     'question': 'Quale percentuale di telespettatori televisivi in prima serata ha rappresentato le tre grandi reti nel 1980?'},\n",
       "    {'answers': [{'answer_start': 302, 'text': 'Dynasty'},\n",
       "      {'answer_start': 302, 'text': 'Dynasty'},\n",
       "      {'answer_start': 302, 'text': 'Dynasty'}],\n",
       "     'id': '57276166dd62a815002e9bd9',\n",
       "     'question': \"Quale dramma di Aaron Spelling ha debuttato su ABC negli anni' 80?\"},\n",
       "    {'answers': [{'answer_start': 769, 'text': 'Mork & Mindy'},\n",
       "      {'answer_start': 769, 'text': 'Mork & Mindy'},\n",
       "      {'answer_start': 769, 'text': 'Mork & Mindy'}],\n",
       "     'id': '57276166dd62a815002e9bda',\n",
       "     'question': \"Che cosa è stato uno spinoff Happy Days che ha debuttato negli anni' 80 su ABC?\"},\n",
       "    {'answers': [{'answer_start': 853,\n",
       "       'text': 'Alpha Repertory Television Service (ARTS)'},\n",
       "      {'answer_start': 853,\n",
       "       'text': 'Alpha Repertory Television Service (ARTS)'},\n",
       "      {'answer_start': 853,\n",
       "       'text': 'Alpha Repertory Television Service (ARTS)'}],\n",
       "     'id': '57276166dd62a815002e9bdb',\n",
       "     'question': 'Quale canale ha lanciato ABC nel 1981 per la programmazione culturale e artistica?'}]},\n",
       "  {'context': \"Nel 1983, ABC vendette KXYZ alla Infinity Broadcasting Corporation. Il 4 gennaio 1984, il New York Times ha riferito che ABC, attraverso la sua controllata ABC Video Enterprises, aveva esercitato l' opzione di acquistare fino al 15% (o tra 25 e 30 milioni di dollari) delle azioni di Getty Oil in ESPN, il che le avrebbe permesso di espandere le sue azioni in un secondo momento. Nel giugno 1984, il comitato esecutivo di ABC approvò l' acquisizione della partecipazione azionaria della società in ESPN, e ABC si accordò con Getty Oil per ottenere una partecipazione dell' 80% nel canale, vendendo il restante 20% a Nabisco. In quell' anno, ABC e Hearst hanno raggiunto un accordo con RCA per fondere il servizio ARTS e il servizio arti concorrenti, The Entertainment Channel, in un unico canale via cavo chiamato Arts & Entertainment Television (A&E); il nuovo canale ha successivamente affittato un transponder satellitare separato, ponendo fine all' accordo di condivisione con Nickelodeon per diventare un servizio 24 ore su 24. Nel frattempo, ABC si ritirò dal business parco a tema per il bene quando ha venduto il Parco tematico delle sorgenti d' argento a tema naturale.\",\n",
       "   'qas': [{'answers': [{'answer_start': 33,\n",
       "       'text': 'Infinity Broadcasting Corporation'},\n",
       "      {'answer_start': 33, 'text': 'Infinity Broadcasting Corporation'},\n",
       "      {'answer_start': 33, 'text': 'Infinity Broadcasting Corporation'}],\n",
       "     'id': '5727623a5951b619008f8921',\n",
       "     'question': 'A quale entità ABC ha venduto KXYZ nel 1983?'},\n",
       "    {'answers': [{'answer_start': 284, 'text': 'Getty Oil'},\n",
       "      {'answer_start': 284, 'text': 'Getty Oil'}],\n",
       "     'id': '5727623a5951b619008f8922',\n",
       "     'question': 'Nel 1984, ABC ha acquistato il 15% delle azioni di ESPN?'},\n",
       "    {'answers': [{'answer_start': 750, 'text': 'The Entertainment Channel'},\n",
       "      {'answer_start': 750, 'text': 'The Entertainment Channel'},\n",
       "      {'answer_start': 750, 'text': 'The Entertainment Channel'}],\n",
       "     'id': '5727623a5951b619008f8923',\n",
       "     'question': 'Nel 1984, il canale artistico ABC ARTS è stato fuso con quale altro canale?'},\n",
       "    {'answers': [{'answer_start': 814,\n",
       "       'text': 'Arts & Entertainment Television (A&E)'},\n",
       "      {'answer_start': 814, 'text': 'Arts & Entertainment Television (A&E)'},\n",
       "      {'answer_start': 814, 'text': 'Arts & Entertainment Television (A&E)'}],\n",
       "     'id': '5727623a5951b619008f8924',\n",
       "     'question': 'Qual è stato il canale risultante della fusione ARTS chiamato?'}]},\n",
       "  {'context': \"Nel 1990, Thomas S. Murphy ha delegato la sua carica di presidente a Daniel B. Burke, pur rimanendo presidente e CEO di ABC. Capital Cities/ABC ha registrato ricavi per 465 milioni di dollari. Ora ad un forte secondo posto, la rete è entrata negli anni' 90 con ulteriori visite a misura di famiglia tra cui America's Funniest Home Videos (che ha continuato a diventare il più lungo programma di intrattenimento prime time nella storia della rete), Step by Step, Hangin' con Mr. Cooper, Boy Meets World e Perfect Strangers spinoff Family Matters, così come serie come Doogie Howser, MD. Nel settembre 1991, la rete ha debuttato in anteprima Home Improvement, una sitcom con il fumetto stand-up Tim Allen incentrato sulla famiglia e la vita lavorativa di un ospite incline agli incidenti di un cavo di accesso casa miglioramento mostra. Il successo durato nove stagioni, il suo successo ha portato ABC alla luce verde ulteriori progetti sitcom guidato da comici durante gli anni' 90 tra cui The Drew Carey Show, Brett veicolo Butler Grace Under Fire e Ellen, che è diventato notevole per un episodio 1997 che è servito come l' uscita della stella della serie Ellen DeGeneres (come pure il suo personaggio nella serie) come una lesbica.\",\n",
       "   'qas': [{'answers': [{'answer_start': 69, 'text': 'Daniel B. Burke'},\n",
       "      {'answer_start': 69, 'text': 'Daniel B. Burke'},\n",
       "      {'answer_start': 69, 'text': 'Daniel B. Burke'}],\n",
       "     'id': '572763a8708984140094dcd9',\n",
       "     'question': 'Chi è succeduto a Thomas Murphy come presidente nel 1990?'},\n",
       "    {'answers': [{'answer_start': 100, 'text': 'presidente e CEO'},\n",
       "      {'answer_start': 100, 'text': 'presidente e CEO'},\n",
       "      {'answer_start': 56, 'text': 'presidente'}],\n",
       "     'id': '572763a8708984140094dcda',\n",
       "     'question': 'Quale posizione in ABC ha ricoperto Thomas Murphy dopo il dimissionario presidente?'},\n",
       "    {'answers': [{'answer_start': 169, 'text': '465 milioni di dollari'},\n",
       "      {'answer_start': 169, 'text': '465 milioni di dollari'},\n",
       "      {'answer_start': 169, 'text': '465 milioni di dollari'}],\n",
       "     'id': '572763a8708984140094dcdb',\n",
       "     'question': 'Quanto hanno riportato Capital Cities/ABC sui ricavi nel 1990?'},\n",
       "    {'answers': [{'answer_start': 307,\n",
       "       'text': \"America's Funniest Home Video\"},\n",
       "      {'answer_start': 307, 'text': \"America's Funniest Home Video\"},\n",
       "      {'answer_start': 307, 'text': \"America's Funniest Home Video\"}],\n",
       "     'id': '572763a8708984140094dcdc',\n",
       "     'question': \"Nel 1990, qual è stato il programma di intrattenimento in prima serata più lungo della storia dell' ABC?\"},\n",
       "    {'answers': [{'answer_start': 640, 'text': 'Home Improvement'},\n",
       "      {'answer_start': 640, 'text': 'Home Improvement'},\n",
       "      {'answer_start': 640, 'text': 'Home Improvement'}],\n",
       "     'id': '572763a8708984140094dcdd',\n",
       "     'question': 'Quale commedia con in piedi il comico Tim Allen ha debuttato nel 1991?'}]},\n",
       "  {'context': \"La programmazione diurna è prevista anche nei giorni feriali dalle ore 11:00 alle ore 15:00 (con una pausa di un' ora alle ore 12:00.00 est / Pacifico per le stazioni di telegiornali, altri programmi prodotti localmente come talk show, o programmi sindacati) con i talk / Lifestyle show The View e The Chew e la soap opera General Hospital. La programmazione di ABC News include Good Morning America dalle 7:00 alle 9:00 dei giorni feriali (insieme alle edizioni di un week-end di un' ora); edizioni notturne di ABC World News Tonight (le cui edizioni del fine settimana sono occasionalmente soggette ad abbreviazione o prelazione a causa di sconfinamento delle telecast sportive negli orari del programma), il talk show politico domenicale Questa Settimana, i programmi di notizie del primo mattino World News Now e America This Morning e la tarda serata. Notti di fine settimana con il talk show Jimmy Kimmel Live! Ospedale generale. Ospedale generale. Ospedale generale. Quale soap opera è attualmente presente nella programmazione diurna di ABC? La vista e la masticazione. La vista e la masticazione.\",\n",
       "   'qas': [{'answers': [{'answer_start': 1050,\n",
       "       'text': 'La vista e la masticazione'}],\n",
       "     'id': '572764855951b619008f8951',\n",
       "     'question': 'Quali due talk show sono attualmente presenti durante la programmazione diurna su ABC?'}]},\n",
       "  {'context': \"Attualmente, il New Jersey, Rhode Island e Delaware sono gli unici stati degli Stati Uniti dove ABC non ha un' affiliata con licenza locale (New Jersey è servita da New York City O&O WABC-TV e Philadelphia O&O WPVI-TV; Rhode Island è servita da New Bedford, Massachusetts-licensed WLNE; e Delaware è servita da ABC mantiene affiliazioni con stazioni a basso consumo energetico (trasmissioni sia analogiche che digitali) in alcuni mercati, come Birmingham, Alabama (WBMA-LD), Lima, Ohio (WLQP-LP) e South Bend, Indiana (WBND-LD). In alcuni mercati, tra cui i primi due citati, queste stazioni mantengono anche i simulcast digitali su un canale secondario di una stazione televisiva a piena potenza co-proprietaria/comandata. New Jersey, Rhode Island e Delaware. New Jersey, Rhode Island e Delaware.\",\n",
       "   'qas': [{'answers': [{'answer_start': 16,\n",
       "       'text': 'New Jersey, Rhode Island e Delaware'}],\n",
       "     'id': '57276576dd62a815002e9c18',\n",
       "     'question': 'Quali sono gli unici stati in cui ABC non ha un affiliato con licenza?'},\n",
       "    {'answers': [{'answer_start': 519, 'text': 'WBND-LD'},\n",
       "      {'answer_start': 519, 'text': 'WBND-LD'},\n",
       "      {'answer_start': 519, 'text': 'WBND-LD'}],\n",
       "     'id': '57276576dd62a815002e9c19',\n",
       "     'question': 'Quale stazione ABC di South Bend, Indiana mantiene i simulcast digitali su un sottocanale?'},\n",
       "    {'answers': [{'answer_start': 487, 'text': 'WLQP-LP'},\n",
       "      {'answer_start': 487, 'text': 'WLQP-LP'},\n",
       "      {'answer_start': 487, 'text': 'WLQP-LP'}],\n",
       "     'id': '57276576dd62a815002e9c1a',\n",
       "     'question': \"Che cosa è l' affiliato ABC che serve Lima, Ohio?\"}]},\n",
       "  {'context': \"Tutte le stazioni e gli affiliati di proprietà e gestiti da ABC dispongono di proprie strutture e studi, ma sono state create entità trasversali per produrre programmi nazionali. Di conseguenza, le serie televisive sono state prodotte da ABC Circle Films a partire dal 1962 e da Touchstone Television a partire dal 1985, prima che Touchstone fosse riorganizzata come ABC Studios nel febbraio 2007. Dagli anni' 50, ABC ha due stabilimenti di produzione principali: l' ABC Television Center (ora The Prospect Studios) su Prospect Avenue a Hollywood, California, condiviso con le attività di KABC-TV fino al 1999; e l' ABC Television Center, East, un set di studios situati in tutta la città di New York.\",\n",
       "   'qas': [{'answers': [{'answer_start': 238, 'text': 'ABC Circle Films'},\n",
       "      {'answer_start': 238, 'text': 'ABC Circle Films'},\n",
       "      {'answer_start': 238, 'text': 'ABC Circle Films'}],\n",
       "     'id': '57276690708984140094dd01',\n",
       "     'question': 'Quale entità ha iniziato a produrre serie televisive per ABC nel 1962?'},\n",
       "    {'answers': [{'answer_start': 367, 'text': 'ABC Studios'},\n",
       "      {'answer_start': 367, 'text': 'ABC Studios'},\n",
       "      {'answer_start': 367, 'text': 'ABC Studios'}],\n",
       "     'id': '57276690708984140094dd02',\n",
       "     'question': 'In quale entità è stata riorganizzata Touchstone Television nel 2007?'},\n",
       "    {'answers': [{'answer_start': 467, 'text': 'ABC Television Center'},\n",
       "      {'answer_start': 467, 'text': 'ABC Television Center'},\n",
       "      {'answer_start': 494, 'text': 'The Prospect Studios'}],\n",
       "     'id': '57276690708984140094dd03',\n",
       "     'question': 'Quale dei principali stabilimenti di produzione di ABC si trova a Hollywood, CA?'},\n",
       "    {'answers': [{'answer_start': 616, 'text': 'ABC Television Center, East'},\n",
       "      {'answer_start': 616, 'text': 'ABC Television Center, East'},\n",
       "      {'answer_start': 467, 'text': 'ABC Television Center'}],\n",
       "     'id': '57276690708984140094dd04',\n",
       "     'question': 'Quale dei principali stabilimenti produttivi di ABC si trova a New York?'}]},\n",
       "  {'context': \"ABC è inoltre proprietaria degli studi di Times Square a 1500 Broadway sulla terraferma di Times Square, di proprietà di un fondo di sviluppo per il 42th Street Project; inaugurato nel 1999, Good Morning America e Nightline sono trasmessi da questa particolare struttura. ABC News ha sede un po' più lontano sulla West 66th Street, in un edificio di sei piani che occupa un terreno di 196 piedi (60 m) × 379 piedi (116 m) a 121-135 West End Avenue. L' isolato di West End Avenue, che ospita l' edificio ABC News, è stato ribattezzato Peter Jennings Way nel 2006 in onore dell' ancora principale di ABC News, da tempo scomparso, e ancora di World News Tonight.\",\n",
       "   'qas': [{'answers': [{'answer_start': 534, 'text': 'Peter Jennings'},\n",
       "      {'answer_start': 534, 'text': 'Peter Jennings'},\n",
       "      {'answer_start': 534, 'text': 'Peter Jennings'}],\n",
       "     'id': '5727678e5951b619008f8975',\n",
       "     'question': 'Un isolato di West End Avenue che ospita un edificio ABC News è stato ribattezzato per quale ancoraggio ABC?'},\n",
       "    {'answers': [{'answer_start': 640, 'text': 'World News Tonight'},\n",
       "      {'answer_start': 640, 'text': 'World News Tonight'},\n",
       "      {'answer_start': 640, 'text': 'World News Tonight'}],\n",
       "     'id': '5727678e5951b619008f8976',\n",
       "     'question': 'Che cosa ha fatto Peter Jennings per ABC?'}]},\n",
       "  {'context': \"ABC mantiene diversi servizi di video on demand per la visione ritardata della programmazione della rete, tra cui un tradizionale servizio VOD chiamato ABC on Demand, che viene eseguito sulla maggior parte dei fornitori di cavi tradizionali e IPTV. La Walt Disney Company è anche una comproprietaria di Hulu (nell' ambito di un consorzio che comprende, tra le altre parti, le rispettive società madri di NBC e Fox, NBCUniversal e 21st Century Fox), e ha offerto episodi a figura intera della maggior parte della programmazione di ABC attraverso il servizio di streaming dal 6 luglio 2009 (che sono disponibili per la visualizzazione sul sito web di Hulu e l' applicazione mobile), come parte di un accordo raggiunto in ABC su richiesta.\",\n",
       "   'qas': [{'answers': [{'answer_start': 719, 'text': 'ABC su richiesta'},\n",
       "      {'answer_start': 719, 'text': 'ABC su richiesta'},\n",
       "      {'answer_start': 152, 'text': 'ABC on Demand'}],\n",
       "     'id': '572768d9708984140094dd13',\n",
       "     'question': 'La Walt Disney Company è un proprietario parziale di quale servizio di streaming VOD?'},\n",
       "    {'answers': [{'answer_start': 574, 'text': '6 luglio 2009'},\n",
       "      {'answer_start': 574, 'text': '6 luglio 2009'},\n",
       "      {'answer_start': 574, 'text': '6 luglio 2009'}],\n",
       "     'id': '572768d9708984140094dd14',\n",
       "     'question': 'Quando Hulu ha iniziato ad offrire i programmi ABC per lo streaming?'}]},\n",
       "  {'context': \"Gli episodi più recenti degli spettacoli della rete sono solitamente resi disponibili su WATCH ABC, Hulu e ABC on Demand il giorno dopo la loro trasmissione originale. Inoltre, l' ABC on Demand (come i servizi televisivi video su richiesta forniti dalle altre reti televisive statunitensi) impedisce l' inoltro rapido dei contenuti accessibili. Restrizioni implementate da Disney-ABC Television Group il 7 gennaio 2014 limitano lo streaming dello streaming dell' episodio più recente di qualsiasi programma ABC su Hulu e WATCH ABC fino a otto giorni dopo la loro prima trasmissione, al fine di incoraggiare la visione in diretta o della stessa settimana (tramite DVR e via cavo su richiesta), con streaming in onda giorno dopo giorno su entrambi i servizi limitati agli abbonati dei fornitori di televisione a pagamento partecipanti (come Comcast, Verizon Fi. il giorno successivo alla trasmissione originale.\",\n",
       "   'qas': [{'answers': [{'answer_start': 860,\n",
       "       'text': 'il giorno successivo alla trasmissione originale'},\n",
       "      {'answer_start': 124, 'text': 'giorno dopo'}],\n",
       "     'id': '572769e85951b619008f8985',\n",
       "     'question': 'Quando sono gli episodi recenti di spettacoli ABC tipicamente resi disponibili sui servizi VOD?'},\n",
       "    {'answers': [{'answer_start': 303,\n",
       "       'text': 'inoltro rapido dei contenuti accessibili'}],\n",
       "     'id': '572769e85951b619008f8986',\n",
       "     'question': 'Che cosa non ammette ABC on Demand per i telespettatori online?'},\n",
       "    {'answers': [{'answer_start': 404, 'text': '7 gennaio 2014'},\n",
       "      {'answer_start': 404, 'text': '7 gennaio 2014'},\n",
       "      {'answer_start': 404, 'text': '7 gennaio 2014'}],\n",
       "     'id': '572769e85951b619008f8987',\n",
       "     'question': 'Quando il gruppo televisivo Disney-ABC ha introdotto restrizioni su Hulu e WATCH ABC per incoraggiare la visione dal vivo?'}]},\n",
       "  {'context': 'Una versione riveduta del logo ABC è stata introdotta per le promozioni per la stagione 2013-2014 durante la presentazione iniziale del network il 14 maggio 2013, e ufficialmente introdotta in onda il 17 giugno (anche se alcuni affiliati hanno implementato il nuovo design prima di allora), come parte di una revisione dell\\' identità di ABC da parte dell\\' agenzia di design LoyalKaspar. Il logo aggiornato ha un design lucido più semplice rispetto alla versione precedente e contiene scritte più simili a quelle originali di Paul Rand. Il logo viene visualizzato in onda, online e in pubblicità stampata in quattro varianti che sfumano il rispettivo colore utilizzato con la colorazione nera nativa del disegno del cerchio: una versione dorata è utilizzata principalmente su punti vendita orientati all\\' intrattenimento (come ABC. com, WATCH ABC, e da ABC Studios) e il bug sullo schermo; le versioni in acciaio blu e grigio scuro sono utilizzate principalmente da ABC News; una versione rossa è utilizzata per ESPN su ABC, mentre tutte le versioni in acciaio blu e grigio scuro sono utilizzate principalmente da ABC News; una versione rossa per ESPN su ABC, mentre tutte le altre versioni. Un nuovo carattere tipografico personalizzato,\"ABC Modern\" (ispirato al logotipo), è stato creato anche per l\\' uso in pubblicità e altri materiali promozionali.',\n",
       "   'qas': [{'answers': [{'answer_start': 374, 'text': 'LoyalKaspar'},\n",
       "      {'answer_start': 374, 'text': 'LoyalKaspar'},\n",
       "      {'answer_start': 374, 'text': 'LoyalKaspar'}],\n",
       "     'id': '57276a8f5951b619008f8995',\n",
       "     'question': \"Nel 2013, l' identità di ABC è stata rinnovata da quale agenzia di design?\"},\n",
       "    {'answers': [{'answer_start': 607, 'text': 'quattro varianti'},\n",
       "      {'answer_start': 607, 'text': 'quattro'},\n",
       "      {'answer_start': 607, 'text': 'quattro'}],\n",
       "     'id': '57276a8f5951b619008f8996',\n",
       "     'question': 'In quante varianti è attualmente visualizzato il nuovo logo ABC?'},\n",
       "    {'answers': [{'answer_start': 1238, 'text': 'ABC Modern'},\n",
       "      {'answer_start': 1238, 'text': 'ABC Modern'},\n",
       "      {'answer_start': 1238, 'text': 'ABC Modern'}],\n",
       "     'id': '57276a8f5951b619008f8997',\n",
       "     'question': \"Quale nuovo carattere tipografico è stato creato per ABC per l' uso pubblicitario?\"},\n",
       "    {'answers': [{'answer_start': 1011, 'text': 'ESPN'},\n",
       "      {'answer_start': 1011, 'text': 'ESPN su ABC'},\n",
       "      {'answer_start': 1011, 'text': 'ESPN'}],\n",
       "     'id': '57276a8f5951b619008f8998',\n",
       "     'question': 'Quale rete utilizza una versione rossa del nuovo logo ABC?'}]},\n",
       "  {'context': \"Nel 1954, tutte le reti americane avevano riacquistato il controllo della loro programmazione, con maggiori entrate pubblicitarie: i ricavi di ABC sono aumentati del 67% (ottenendo 26 milioni di dollari), quelli di NBC del 30% ($100 milioni) e quelli di CBS del 44% ($117 milioni). Tuttavia, in quell' anno, ABC contava solo 14 filiali primarie contro le 74 che detenevano la maggior parte dei programmi CBS e le 71 affiliate principalmente alla NBC. La maggior parte dei mercati al di fuori di quelli più grandi non erano abbastanza grandi da supportare tre affiliati a tempo pieno. In alcuni mercati che erano abbastanza grandi per una terza affiliata a tempo pieno, l' unica allocazione commerciale disponibile era sulla banda UHF meno desiderabile. Fino a quando l' All-Channel Receiver Act (passato dal Congresso nel 1961) ha imposto l' inclusione della sintonizzazione UHF, la maggior parte dei telespettatori necessari per acquistare un convertitore per essere in grado di guardare le stazioni UHF, e la qualità del segnale era marginale nella migliore delle ipotesi anche con un convertitore. Inoltre, durante l' era della televisione analogica, le stazioni UHF non erano adeguatamente ricevibili in terreni aspri. Questi fattori hanno fatto in modo che molti potenziali proprietari di stazioni si scelsero di investire in una stazione UHF, soprattutto quella che avrebbe dovuto prendere un' affiliazione con una rete più debole. 14.\",\n",
       "   'qas': [{'answers': [{'answer_start': 325, 'text': '14'},\n",
       "      {'answer_start': 325, 'text': '14'}],\n",
       "     'id': '57276c64f1498d1400e8f7b2',\n",
       "     'question': 'Quanti sono stati gli affiliati primari di ABC nel 1954?'},\n",
       "    {'answers': [{'answer_start': 770, 'text': 'All-Channel Receiver Act'},\n",
       "      {'answer_start': 770, 'text': 'All-Channel Receiver Act'},\n",
       "      {'answer_start': 770, 'text': 'All-Channel Receiver Act'}],\n",
       "     'id': '57276c64f1498d1400e8f7b3',\n",
       "     'question': \"Quale legislazione importante è stata approvata dal Congresso per l' industria televisiva nel 1961?\"},\n",
       "    {'answers': [{'answer_start': 859, 'text': 'sintonizzazione UHF'},\n",
       "      {'answer_start': 730, 'text': 'UHF'},\n",
       "      {'answer_start': 730, 'text': 'UHF'}],\n",
       "     'id': '57276c64f1498d1400e8f7b4',\n",
       "     'question': 'Il All-Channel Receiver Act ha imposto il supporto di quale tipo di sintonia?'}]},\n",
       "  {'context': \"Di conseguenza, con l' eccezione dei mercati più grandi, ABC è stata retrocessa allo status secondario su una o su entrambe le stazioni esistenti, di solito attraverso spazi liberi fuori orario (una notevole eccezione durante questo periodo è stata WKST-TV a Youngstown, Ohio, ora WYTV, nonostante le piccole dimensioni del mercato circostante e la sua vicinanza a Cleveland e Pittsburgh anche decenni prima del crollo economico della città). Secondo Goldenson, questo significava che un' ora di programmazione ABC riportava una visuale cinque volte inferiore rispetto ai suoi concorrenti. Tuttavia, l' apporto di denaro da parte della rete all' epoca le consentirebbe di accelerare la produzione di contenuti. Tuttavia, la portata limitata di ABC continuerebbe a spingerla per i prossimi due decenni; diversi mercati più piccoli non diventerebbero abbastanza grandi da supportare un' affiliata ABC a tempo pieno fino agli anni' 60, con alcuni mercati molto piccoli che avrebbero dovuto aspettare fino agli anni' 80 o addirittura l' avvento della televisione digitale negli anni' 20, il che ha permesso a stazioni come la WTRF-TV di Wheeling, West Virginia di iniziare a trasmettere programmi ABC su un canale secondario digitale dopo la trasmissione.\",\n",
       "   'qas': [{'answers': [{'answer_start': 259, 'text': 'Youngstown'},\n",
       "      {'answer_start': 249, 'text': 'WKST-TV a Youngstown'},\n",
       "      {'answer_start': 259, 'text': 'Youngstown'}],\n",
       "     'id': '57276d7f708984140094dd3f',\n",
       "     'question': \"ABC aveva uno status secondario sulle stazioni esistenti in quale città dell' Ohio?\"},\n",
       "    {'answers': [{'answer_start': 537, 'text': 'cinque volte'},\n",
       "      {'answer_start': 537, 'text': 'cinque volte'}],\n",
       "     'id': '57276d7f708984140094dd40',\n",
       "     'question': 'Relegazione allo stato secondario per ABC ha portato in visuale quanto più basso rispetto ai loro concorrenti, secondo Goldenson?'},\n",
       "    {'answers': [{'answer_start': 1122, 'text': 'WTRF-TV'},\n",
       "      {'answer_start': 1122, 'text': 'WTRF-TV'},\n",
       "      {'answer_start': 1122, 'text': 'WTRF-TV'}],\n",
       "     'id': '57276d7f708984140094dd41',\n",
       "     'question': \"L' inizio della televisione digitale ha permesso a quale affiliata di Wheeling, West Virginia di iniziare a trasmettere programmi ABC in digitale?\"}]},\n",
       "  {'context': 'Il 3 settembre 1958, la serie antologia Disneyland serie è stato ritirato Walt Disney Presents come è diventato dissociato con il parco a tema dello stesso nome. Il movimento dei western occidentali, che ABC ha il merito di aver iniziato, rappresentava un quinto di tutte le serie primetime sulla televisione americana nel gennaio 1959, a quel punto gli spettacoli di detective stavano cominciando ad aumentare anche in popolarità. ABC ha richiesto ulteriori produzioni alla Disney. Alla fine del 1958, Desilu Productions lanciò la sua serie di detective The Untouchables to CBS; dopo che la rete rifiutò lo spettacolo a causa del suo uso della violenza, Desilu lo presentò ad ABC, che accettò di riprendere la mostra, e debuttò The Untouchables nell\\' aprile 1959. La serie ha continuato a diventare rapidamente \"immensamente popolare\".',\n",
       "   'qas': [{'answers': [{'answer_start': 74, 'text': 'Walt Disney Presents'},\n",
       "      {'answer_start': 74, 'text': 'Walt Disney Presents'},\n",
       "      {'answer_start': 74, 'text': 'Walt Disney Presents'}],\n",
       "     'id': '57276f82dd62a815002e9cd0',\n",
       "     'question': 'Qual è stata la serie antologia Disneyland ritirata nel 1958?'},\n",
       "    {'answers': [{'answer_start': 503, 'text': 'Desilu Productions'},\n",
       "      {'answer_start': 503, 'text': 'Desilu Productions'},\n",
       "      {'answer_start': 503, 'text': 'Desilu Productions'}],\n",
       "     'id': '57276f82dd62a815002e9cd1',\n",
       "     'question': 'Quale azienda di produzione ha lanciato The Untouchables a CBS nel 1958?'},\n",
       "    {'answers': [{'answer_start': 645, 'text': 'violenza'}],\n",
       "     'id': '57276f82dd62a815002e9cd2',\n",
       "     'question': 'Perchè CBS ha abbassato il campo per The Untouchables?'},\n",
       "    {'answers': [{'answer_start': 752, 'text': 'aprile 1959'},\n",
       "      {'answer_start': 752, 'text': 'aprile 1959'},\n",
       "      {'answer_start': 752, 'text': 'aprile 1959'}],\n",
       "     'id': '57276f82dd62a815002e9cd3',\n",
       "     'question': 'Quando ha debuttato Untouchables su ABC?'}]},\n",
       "  {'context': \"A causa delle pressioni esercitate dagli studi cinematografici per aumentare la loro produzione, mentre i principali network cominciarono a trasmettere film usciti in sala, ABC si unì a CBS e NBC per trasmettere film la domenica sera del 1962, con il lancio dell' ABC Sunday Night Movie, che debuttò un anno dopo i suoi concorrenti e fu inizialmente presentato in bianco e nero. Nonostante un significativo aumento degli spettatori (con la sua quota di audience aumentata al 33% rispetto al 15% del 1953), ABC è rimasta al terzo posto, con un fatturato totale di 15,5 milioni di dollari, un terzo del fatturato ottenuto dalla CBS nello stesso periodo. Per recuperare il ritardo, ABC ha seguito The Flintstones con un' altra serie animata di Hanna-Barbera, The Jetsons, che ha debuttato il 23 settembre 1962 come la prima serie televisiva ad essere trasmessa a colori sulla rete. Il 1 aprile 1963, ABC ha debuttato l' ospedale generale della soap opera, che sarebbe continuato a diventare il programma di intrattenimento di lunga durata della rete televisiva. Quell' anno ha visto anche la prima di The Fugitive (il 17 settembre), una serie drammatica incentrata su un uomo in fuga dopo essere stato accusato di aver commesso un omicidio che non ha commesso.\",\n",
       "   'qas': [{'answers': [{'answer_start': 264,\n",
       "       'text': 'ABC Sunday Night Movie'},\n",
       "      {'answer_start': 264, 'text': 'ABC Sunday Night Movie'},\n",
       "      {'answer_start': 264, 'text': 'ABC Sunday Night Movie'}],\n",
       "     'id': '5727705f5951b619008f89f3',\n",
       "     'question': 'Qual è stato il titolo del programma cinematografico trasmesso da ABC che ha debuttato la domenica del 1962?'},\n",
       "    {'answers': [{'answer_start': 563, 'text': '15,5 milioni di dollari'},\n",
       "      {'answer_start': 563, 'text': '15,5 milioni di dollari'}],\n",
       "     'id': '5727705f5951b619008f89f4',\n",
       "     'question': 'Quali sono state le entrate di ABC nel 1962?'},\n",
       "    {'answers': [{'answer_start': 741, 'text': 'Hanna-Barbera'},\n",
       "      {'answer_start': 741, 'text': 'Hanna-Barbera'},\n",
       "      {'answer_start': 741, 'text': 'Hanna-Barbera'}],\n",
       "     'id': '5727705f5951b619008f89f5',\n",
       "     'question': 'Quale azienda di produzione ha prodotto la serie animata The Flintstones per ABC?'},\n",
       "    {'answers': [{'answer_start': 756, 'text': 'The Jetsons'},\n",
       "      {'answer_start': 756, 'text': 'The Jetsons'},\n",
       "      {'answer_start': 756, 'text': 'The Jetsons'}],\n",
       "     'id': '5727705f5951b619008f89f6',\n",
       "     'question': 'Qual è stata la prima serie ad essere trasmessa a colori su ABC?'},\n",
       "    {'answers': [{'answer_start': 882, 'text': '1 aprile 1963'},\n",
       "      {'answer_start': 882, 'text': '1 aprile 1963'},\n",
       "      {'answer_start': 882, 'text': '1 aprile 1963'}],\n",
       "     'id': '5727705f5951b619008f89f7',\n",
       "     'question': \"Quando ha debuttato per la prima volta l' ospedale generale su ABC?\"}]},\n",
       "  {'context': \"Il 7 dicembre 1965 Goldenson annunciò una proposta di fusione con ITT al management di ABC; le due società si accordarono sull' operazione il 27 aprile 1966. La FCC ha approvato la fusione il 21 dicembre 1966; tuttavia, il giorno precedente (20 dicembre), Donald F. Turner, responsabile dell' autorità antitrust del Dipartimento di Giustizia degli Stati Uniti, ha espresso dubbi in merito a questioni quali l' emergente mercato televisivo via cavo, e le preoccupazioni circa l' integrità giornalistica di ABC e come potrebbe essere influenzata dalla proprietà estera di ITT. Il management di ITT ha promesso che l' azienda avrebbe permesso ad ABC di mantenere la propria autonomia nell' attività editoriale. La fusione è stata sospesa, e un reclamo è stato presentato dal Dipartimento di Giustizia nel luglio 1967, con ITT al processo in corso nel mese di ottobre 1967, la fusione è stata ufficialmente annullata dopo la conclusione del processo il 1 ° gennaio 1968.\",\n",
       "   'qas': [{'answers': [{'answer_start': 66, 'text': 'ITT'},\n",
       "      {'answer_start': 66, 'text': 'ITT'},\n",
       "      {'answer_start': 66, 'text': 'ITT'}],\n",
       "     'id': '572771a5f1498d1400e8f840',\n",
       "     'question': 'Leonard Goldenson ha annunciato una proposta di fusione con quale società nel dicembre 1965?'},\n",
       "    {'answers': [{'answer_start': 256, 'text': 'Donald F. Turner'},\n",
       "      {'answer_start': 256, 'text': 'Donald F. Turner'},\n",
       "      {'answer_start': 256, 'text': 'Donald F. Turner'}],\n",
       "     'id': '572771a5f1498d1400e8f841',\n",
       "     'question': 'Quale regolatore di anitrust ha espresso dubbi sulla fusione ITT e ABC?'},\n",
       "    {'answers': [{'answer_start': 316, 'text': 'Dipartimento di Giustizia'},\n",
       "      {'answer_start': 316, 'text': 'Dipartimento di Giustizia'},\n",
       "      {'answer_start': 316, 'text': 'Dipartimento di Giustizia'}],\n",
       "     'id': '572771a5f1498d1400e8f842',\n",
       "     'question': 'La fusione tra ITT e ABC è stata sospesa a seguito di una denuncia presentata da chi nel luglio 1967?'}]},\n",
       "  {'context': \"Nel dicembre 1984, Thomas S. Murphy, amministratore delegato di Capital Cities Communications, ha contattato Leonard Goldenson in merito a una proposta di fusione delle rispettive società. Il 16 marzo 1985, il comitato esecutivo di ABC ha accettato l' offerta di fusione, che è stata formalmente annunciata il 18 marzo 1985, con Capital Cities l' acquisto di ABC e le relative proprietà per $3,5 miliardi e $118 per ciascuna delle azioni di ABC, nonchè una garanzia del 10% (o $3) per un totale di $121 per azione. Per finanziare l' acquisto, Capital Cities ha preso in prestito 2,1 miliardi di dollari da un consorzio di banche, che ha venduto alcune attività che Capital Cites non poteva acquistare o conservare a causa delle regole di proprietà FCC per un totale di 900 milioni di dollari e ha venduto diversi sistemi televisivi via cavo, che sono stati venduti alla Washington Post Company (formando l' attuale Cable One). I restanti 500 milioni di dollari sono stati concessi in prestito da Warren Buffett, che ha promesso che la sua società Berkshire Hathaway avrebbe acquistato 3 milioni di dollari in azioni, a 172,50 dollari al pezzo. A causa del divieto imposto dalla FCC alla FCC di detenere sullo stesso mercato le emittenti televisive e radiofoniche da parte di un' unica società (anche se l' accordo avrebbe altrimenti rispettato le nuove regole di proprietà attuate dalla FCC nel gennaio 1985, che consentivano alle emittenti di detenere un massimo di 12 stazioni televisive), ABC e Capital Cities hanno deciso rispettivamente di vendere WXYZ-TV e Tampa alla società E. W. Scripps (anche se in origine Capital Cities/ABC) la stazione indipendente WFTS-TV.\",\n",
       "   'qas': [{'answers': [{'answer_start': 64,\n",
       "       'text': 'Capital Cities Communications'},\n",
       "      {'answer_start': 64, 'text': 'Capital Cities Communications'},\n",
       "      {'answer_start': 64, 'text': 'Capital Cities Communications'}],\n",
       "     'id': '57277373dd62a815002e9d24',\n",
       "     'question': 'Nel 1984 Thomas Murphy contattò Leonard Goldenson per la fusione di ABC con quale azienda?'},\n",
       "    {'answers': [{'answer_start': 996, 'text': 'Warren Buffett'},\n",
       "      {'answer_start': 996, 'text': 'Warren Buffett'},\n",
       "      {'answer_start': 996, 'text': 'Warren Buffett'}],\n",
       "     'id': '57277373dd62a815002e9d26',\n",
       "     'question': '500 milioni di dollari sono stati forniti per la fusione tra capitale City - ABC da quale investitore?'},\n",
       "    {'answers': [{'answer_start': 1467, 'text': '12 stazioni televisive'},\n",
       "      {'answer_start': 499, 'text': '12'},\n",
       "      {'answer_start': 499, 'text': '12'}],\n",
       "     'id': '57277373dd62a815002e9d28',\n",
       "     'question': 'Le nuove regole di proprietà adottate dalla FCC nel 1985 hanno permesso alle emittenti di detenere un numero massimo di stazioni?'}]},\n",
       "  {'context': \"La fusione tra ABC e Capital Cities è stata approvata dalla Confederazione il 5 settembre 1985. Dopo la fusione tra ABC/Capital Cities, perfezionatasi il 3 gennaio 1986, la società risultante dalla fusione, denominata Capital Cities/ABC, Inc., ha aggiunto al portafoglio di trasmissione di ABC quattro emittenti televisive (WPVI-TV/Philadelphia, KTRK-TV/Houston, KFSN-TV/Fresno e WTVD/Raleigh) e diverse emittenti radiofoniche, includendo anche Fairchild Publications e quattro quotidiani (tra cui Ha anche avviato diversi cambiamenti nella sua gestione: Frederick S. Pierce è stato nominato presidente della divisione di trasmissione radiotelevisiva di ABC; Michael P. Millardi è diventato vice presidente di ABC Broadcasting, e presidente di ABC Stazioni di proprietà e ABC Video Enterprises; John B. Sias è stato nominato presidente della rete televisiva ABC; Brandon Stoddard è diventato presidente di ABC Entertainment (una posizione per la quale era stato nominato nel novembre 1985); e Nel febbraio 1986, Thomas S. Murphy, che dal 1964 ricopriva il ruolo di CEO di Capital Cities, è stato nominato presidente e CEO emerito di ABC. Jim Duffy si è dimesso come presidente della televisione di ABC per una posizione di gestione a ABC Communications, una filiale che si è specializzata nella programmazione di servizi comunitari, compresi gli spettacoli legati all' educazione letteraria.\",\n",
       "   'qas': [{'answers': [{'answer_start': 218,\n",
       "       'text': 'Capital Cities/ABC, Inc.'},\n",
       "      {'answer_start': 218, 'text': 'Capital Cities/ABC, Inc'},\n",
       "      {'answer_start': 218, 'text': 'Capital Cities/ABC, Inc'}],\n",
       "     'id': '572774cf5951b619008f8a51',\n",
       "     'question': 'Dopo il completamento della fusione tra ABC e Capital Cities, qual era la società risultante nota come \"Capital Cities\"?'},\n",
       "    {'answers': [{'answer_start': 218, 'text': 'Capital Cities/ABC, Inc.'},\n",
       "      {'answer_start': 218, 'text': 'Capital Cities/ABC, Inc.'},\n",
       "      {'answer_start': 592, 'text': 'presidente'}],\n",
       "     'id': '572774cf5951b619008f8a52',\n",
       "     'question': 'Dopo la fusione Capital Cities - ABC, Frederick Pierce è stato nominato in quale posizione?'},\n",
       "    {'answers': [{'answer_start': 659, 'text': 'Michael P. Millardi'},\n",
       "      {'answer_start': 659, 'text': 'Michael P. Millardi'},\n",
       "      {'answer_start': 659, 'text': 'Michael P. Millardi'}],\n",
       "     'id': '572774cf5951b619008f8a53',\n",
       "     'question': 'Dopo la fusione tra ABC e Capital Cities, chi è diventato il vice presidente della trasmissione di ABC?'},\n",
       "    {'answers': [{'answer_start': 659, 'text': 'Michael P. Millardi'},\n",
       "      {'answer_start': 659, 'text': 'Michael P. Millardi'},\n",
       "      {'answer_start': 659, 'text': 'Michael P. Millardi'}],\n",
       "     'id': '572774cf5951b619008f8a54',\n",
       "     'question': 'Sulla scia della fusione tra ABC e Capital Cities, chi era il presidente di ABC News e ABC Sports?'}]},\n",
       "  {'context': \"Per quanto riguarda la programmazione, quattro degli spettacoli di tendostrutture di ABC degli anni' 70 terminarono le loro corse a metà degli anni' 80: Laverne & Shirley terminò la sua corsa nel 1983, Happy Days e Three's Company terminò nel 1984 (con quest' ultima che produsse un breve spinoff quell' anno), mentre The Love Boat terminò la sua corsa nel 1986. Dopo quasi un decennio di problemi di rating, NBC aveva riconquistato il primato tra le reti dei Big Three nel 1984 sul successo di serie come The Cosby Show, Cheers e Miami Vice. Per contrastare l' NBC, ABC decise di rifocalizzarsi su commedie e serie familiari a partire dalla metà degli anni Ottanta, tra cui Belvedere, Roseanne, Chi è il capo, Just the Ten of Us, The Wonder Years, Full House e Perfect Strangers. Laverne & Shirley.\",\n",
       "   'qas': [{'answers': [{'answer_start': 153, 'text': 'Laverne & Shirley'},\n",
       "      {'answer_start': 153, 'text': 'Laverne & Shirley'}],\n",
       "     'id': '57277585708984140094de2b',\n",
       "     'question': 'Quale sitcom ha terminato la sua trasmissione per ABC nel 1983?'},\n",
       "    {'answers': [{'answer_start': 409, 'text': 'NBC'},\n",
       "      {'answer_start': 409, 'text': 'NBC'},\n",
       "      {'answer_start': 409, 'text': 'NBC'}],\n",
       "     'id': '57277585708984140094de2c',\n",
       "     'question': 'Quale rete ha riconquistato il primato in America nel 1984?'},\n",
       "    {'answers': [{'answer_start': 409, 'text': 'NBC'},\n",
       "      {'answer_start': 409, 'text': 'NBC'},\n",
       "      {'answer_start': 409, 'text': 'NBC'}],\n",
       "     'id': '57277585708984140094de2d',\n",
       "     'question': 'Quale commedia per ABC ha terminato la sua trasmissione nel 1986?'},\n",
       "    {'answers': [{'answer_start': 318, 'text': 'The Love Boat'},\n",
       "      {'answer_start': 318, 'text': 'The Love Boat'},\n",
       "      {'answer_start': 318, 'text': 'The Love Boat'}],\n",
       "     'id': '57277585708984140094de2e',\n",
       "     'question': 'ABC ha iniziato a concentrarsi su quale tipo di serie dopo il successo della NBC nel 1984?'}]},\n",
       "  {'context': 'Dopo il successo iniziale di questa serie, ABC ha rinnovato il suo programma notturno di venerdì sera intorno alle commedie per famiglie alla fine degli anni\\' 80, culminando nel 1989 il debutto del blocco \"TGIF\" (che le promozioni a cui si faceva riferimento stavano per \"Grazie Goodness It\\'s Funny\"). Molte delle serie presentate durante l\\' esecuzione del blocco sono state prodotte da Miller-Boyett Productions, uno studio di Warner Bros, che ha programmato brevemente l\\' intera formazione del venerdì della stagione 1990-91 (con Going Places in ingresso a Family Matters, Full House e Perfect Strangers sul palinsesto \"TGIF\") e attraverso il suo accordo di sviluppo con Paramount Television prima del 1986 (come Miller-Milkis, e più tardi, Miller-Milkis.',\n",
       "   'qas': [{'answers': [{'answer_start': 206, 'text': 'TGIF'},\n",
       "      {'answer_start': 206, 'text': 'TGIF'}],\n",
       "     'id': '57277632f1498d1400e8f8c4',\n",
       "     'question': 'Quale film comico ha debuttato nel 1989 per ABC?'},\n",
       "    {'answers': [{'answer_start': 387, 'text': 'Miller-Boyett Productions'},\n",
       "      {'answer_start': 387, 'text': 'Miller-Boyett Productions'},\n",
       "      {'answer_start': 387, 'text': 'Miller-Boyett Productions'}],\n",
       "     'id': '57277632f1498d1400e8f8c6',\n",
       "     'question': 'La maggior parte delle produzioni della linea TGIF sono state prodotte da quale società di produzione?'},\n",
       "    {'answers': [{'answer_start': 428, 'text': 'Warner Bros'},\n",
       "      {'answer_start': 428, 'text': 'Warner Bros'},\n",
       "      {'answer_start': 428, 'text': 'Warner Bros'}],\n",
       "     'id': '57277632f1498d1400e8f8c7',\n",
       "     'question': 'Miller-Boyett Productions era uno studio affiliato a quale azienda?'}]},\n",
       "  {'context': \"Nel 1968, ABC ha approfittato dei nuovi regolamenti di proprietà FCC che hanno permesso alle società di trasmissione di possedere un massimo di sette stazioni radio a livello nazionale per acquistare le stazioni radio di Houston KXYZ e KXYZ-FM per 1 milione di dollari in azioni e 1,5 milioni di dollari in obbligazioni. Quell' anno Roone Arledge è stato nominato presidente di ABC Sports; la società fondò anche ABC Pictures, una società di produzione cinematografica che pubblicò la sua prima foto quell' anno, la regia di Ralph Nelson Charly. È stato rinominato ABC Motion Pictures nel 1979; l' unità è stata sciolta nel 1985. Lo studio ha inoltre gestito due filiali, Palomar Pictures International e Selmur Pictures. Nel luglio 1968, ABC continuò le sue acquisizioni nel settore dei parchi di divertimento con l' apertura di ABC Marine World a Redwood City, California; quel parco fu venduto nel 1972 e demolito nel 1986, con la terra che occupò il parco diventando poi sede della sede centrale di Oracle Corporation.\",\n",
       "   'qas': [{'answers': [{'answer_start': 144, 'text': 'sette stazioni radio'},\n",
       "      {'answer_start': 144, 'text': 'sette'},\n",
       "      {'answer_start': 144, 'text': 'sette'}],\n",
       "     'id': '572776e85951b619008f8a7f',\n",
       "     'question': 'Le nuove norme di radiodiffusione emanate dalla FCC nel 1968 hanno permesso alle aziende di possedere un massimo di quante stazioni radio?'},\n",
       "    {'answers': [{'answer_start': 538, 'text': 'Charly'},\n",
       "      {'answer_start': 538, 'text': 'Charly'},\n",
       "      {'answer_start': 538, 'text': 'Charly'}],\n",
       "     'id': '572776e85951b619008f8a80',\n",
       "     'question': 'Qual è stata la prima immagine pubblicata da ABC Pictures?'},\n",
       "    {'answers': [{'answer_start': 525, 'text': 'Ralph Nelson'},\n",
       "      {'answer_start': 525, 'text': 'Ralph Nelson'},\n",
       "      {'answer_start': 413, 'text': 'ABC Pictures'}],\n",
       "     'id': '572776e85951b619008f8a81',\n",
       "     'question': 'Da chi è stato prodotto Charly?'},\n",
       "    {'answers': [{'answer_start': 624, 'text': '1985'},\n",
       "      {'answer_start': 624, 'text': '1985'},\n",
       "      {'answer_start': 624, 'text': '1985'}],\n",
       "     'id': '572776e85951b619008f8a82',\n",
       "     'question': 'Quando è stata sciolta la divisione ABC Pictures?'},\n",
       "    {'answers': [{'answer_start': 849, 'text': 'Redwood City, California'},\n",
       "      {'answer_start': 849, 'text': 'Redwood City, California'},\n",
       "      {'answer_start': 726, 'text': 'luglio 1968'}],\n",
       "     'id': '572776e85951b619008f8a83',\n",
       "     'question': 'Dove è stato aperto ABC Marine World?'}]},\n",
       "  {'context': \"Solo alla fine degli anni Cinquanta la rete ABC divenne un serio concorrente di NBC e CBS, in gran parte a causa della varietà di programmi che soddisfacevano le aspettative del pubblico, come i film occidentali e le serie di detective. Nonostante un incremento di quasi il 500% dei ricavi pubblicitari tra il 1953 e il 1958, la rete aveva una copertura nazionale compresa tra il 10% e il 18% della popolazione totale statunitense, in quanto aveva ancora relativamente meno affiliati di NBC e CBS. Nel 1957, il presidente dell' ABC Entertainment Ollie Treiz scoprì che il Bandstand di varietà prodotto localmente aveva tirato valutazioni molto forti nel mercato di Philadelphia su WFIL-TV; Treiz ha infine negoziato un accordo per prendere lo spettacolo nazionale, sotto il titolo rivisto American Bandstand; lo spettacolo divenne rapidamente un fenomeno sociale presentando nuovi talenti musicali e danze ai giovani americani e aiutato a fare una stella fuori dal suo ospite, Dick Cla. occidentale e detective series.\",\n",
       "   'qas': [{'answers': [{'answer_start': 987,\n",
       "       'text': 'occidentale e detective series'},\n",
       "      {'answer_start': 987, 'text': 'occidentale e detective series'}],\n",
       "     'id': '5727780a5951b619008f8a9d',\n",
       "     'question': 'Che tipo di programmi si possono accreditare per il successo di ABC alla fine degli anni Cinquanta?'},\n",
       "    {'answers': [{'answer_start': 373, 'text': 'tra il 10% e il 18%'},\n",
       "      {'answer_start': 373, 'text': 'tra il 10% e il 18%'},\n",
       "      {'answer_start': 274, 'text': '500%'}],\n",
       "     'id': '5727780a5951b619008f8a9e',\n",
       "     'question': \"Qual è stato il raggio d' azione dei telespettatori nazionali di ABC nel 1958?\"},\n",
       "    {'answers': [{'answer_start': 546, 'text': 'Ollie Treiz'},\n",
       "      {'answer_start': 546, 'text': 'Ollie Treiz'},\n",
       "      {'answer_start': 546, 'text': 'Ollie Treiz'}],\n",
       "     'id': '5727780a5951b619008f8a9f',\n",
       "     'question': \"Chi era il presidente dell' ABC Entertainment nel 1957?\"},\n",
       "    {'answers': [{'answer_start': 546, 'text': 'Ollie Treiz'},\n",
       "      {'answer_start': 546, 'text': 'Ollie Treiz'},\n",
       "      {'answer_start': 546, 'text': 'Ollie Treiz'}],\n",
       "     'id': '5727780a5951b619008f8aa0',\n",
       "     'question': 'Chi ha ospitato lo spettacolo di stand bandstand ha debuttato su ABC nel 1957?'}]},\n",
       "  {'context': 'Questo tipo di programmi ha presentato ad ABC l\\' immagine della \"filosofia del controprogrammare contro i suoi concorrenti\", offrendo una forte scaletta di programmi in contrasto con quelli visti sulle reti rivali, che ha aiutato Goldenson a dare alla rete un continuum tra film e televisione. Serie occidentale di ABC (oltre a serie come l\\' actioner Zorro) è andato contro e sconfitto il varietà mostra trasmessa da NBC e CBS nell\\' autunno del 1957, e i suoi spettacoli detective ha fatto lo stesso nell\\' autunno del 1959. Per catturare il pubblico della rete, sono state programmate serie brevi della durata di 66 minuti mezz\\' ora prima della gara. Nel maggio 1961, Life criticava l\\' entusiasmo pubblico e la sponsorizzazione di questi tipi di spettacoli a spese della programmazione delle notizie e denunciava una legge non ufficiale \"che sostituisceva i buoni programmi con quelli cattivi\".',\n",
       "   'qas': [{'answers': [{'answer_start': 351, 'text': 'Zorro'},\n",
       "      {'answer_start': 351, 'text': 'Zorro'},\n",
       "      {'answer_start': 351, 'text': 'Zorro'}],\n",
       "     'id': '57277944f1498d1400e8f90b',\n",
       "     'question': \"Quale serie d' azione ABC è andata contro gli spettacoli di varietà della NBC nell' autunno del 1957?\"},\n",
       "    {'answers': [{'answer_start': 668, 'text': 'Life'},\n",
       "      {'answer_start': 668, 'text': 'Life'},\n",
       "      {'answer_start': 668, 'text': 'Life'}],\n",
       "     'id': '57277944f1498d1400e8f90c',\n",
       "     'question': 'Quale rivista ha criticato la strategia di programmazione di ABC nel maggio 1961?'},\n",
       "    {'answers': [{'answer_start': 471, 'text': 'detective'},\n",
       "      {'answer_start': 471, 'text': 'detective'}],\n",
       "     'id': '57277944f1498d1400e8f90d',\n",
       "     'question': \"Che tipo di spettacoli sono stati utilizzati come controprogrammazione da ABC nell' autunno del 1959?\"}]},\n",
       "  {'context': 'Nel maggio 2013, ABC ha lanciato \"WATCH ABC\", un rinnovamento dei suoi tradizionali servizi di streaming multipiattaforma che comprendono il portale di streaming esistente della rete su ABC. com e un\\' applicazione mobile per smartphone e tablet computer; oltre a fornire episodi completi di programmi ABC, il servizio consente flussi di programmazione in diretta di affiliati ABC locali in mercati selezionati (la prima offerta da parte di una rete televisiva statunitense). Analogamente al servizio WatchESPN della rete consociata ESPN (che ha dato origine al marchio \"WATCH\" utilizzato dai servizi di streaming delle reti televisive Disney), i flussi in diretta di stazioni ABC sono disponibili solo agli abbonati autenticati dei fornitori di televisione a pagamento partecipanti in determinati mercati. New York City O&O WABC-TV e Philadelphia O&O WPVI-TV sono state le prime stazioni ad offrire flussi di programmazione sul servizio (con anteprima gratuita per i non abbonati fino a giugno 2013), con le restanti sei ABC O&Os che offrono stream entro l\\' inizio della stagione 2013-2014. Hearst Television ha anche raggiunto un accordo per offrire i flussi delle sue affiliate ABC (tra cui stazioni a Boston, Kansas City, Milwaukee e West Palm Beach) sul servizio.',\n",
       "   'qas': [{'answers': [{'answer_start': 34, 'text': 'WATCH ABC'},\n",
       "      {'answer_start': 34, 'text': 'WATCH ABC'},\n",
       "      {'answer_start': 34, 'text': 'WATCH ABC'}],\n",
       "     'id': '57277af2708984140094dec3',\n",
       "     'question': 'Quale servizio ha lanciato ABC nel maggio 2013?'},\n",
       "    {'answers': [{'answer_start': 806,\n",
       "       'text': 'New York City O&O WABC-TV e Philadelphia O&O WPVI-TV'},\n",
       "      {'answer_start': 806,\n",
       "       'text': 'New York City O&O WABC-TV e Philadelphia O&O WPVI-TV'},\n",
       "      {'answer_start': 824, 'text': 'WABC-TV e Philadelphia O&O WPVI-TV'}],\n",
       "     'id': '57277af2708984140094dec4',\n",
       "     'question': 'Quali sono state le prime stazioni locali ad offrire flussi di programmazione su WATCH ABC?'},\n",
       "    {'answers': [{'answer_start': 1091, 'text': 'Hearst Television'},\n",
       "      {'answer_start': 1091, 'text': 'Hearst Television'},\n",
       "      {'answer_start': 1091, 'text': 'Hearst Television'}],\n",
       "     'id': '57277af2708984140094dec5',\n",
       "     'question': 'Quale azienda ha raggiunto un accordo per rifornire anche le sue affiliate ABC?'},\n",
       "    {'answers': [{'answer_start': 500, 'text': 'WatchESPN'},\n",
       "      {'answer_start': 500, 'text': 'WatchESPN'},\n",
       "      {'answer_start': 500, 'text': 'WatchESPN'}],\n",
       "     'id': '57277af2708984140094dec6',\n",
       "     'question': 'Qual è il nome del servizio di streaming di ESPN analogo a WATCH ABC?'}]},\n",
       "  {'context': 'Il gruppo Sinclair Broadcast Group è il maggiore operatore di stazioni ABC per totale numerico, possiede o fornisce servizi a 28 affiliate ABC e ad altre due affiliate solo sotto-canale; Sinclair possiede la più grande affiliata di sottocanale ABC per dimensione del mercato, WABM-DT2/WDBB-DDBB-DT2 nel mercato di Birmingham, che funge da ripetitore di WBMA-LD (la più grande affiliata delle \"Big Four\" a basso consumo energetico). La società E. W. Scripps Company è il maggiore operatore di stazioni ABC in termini di portata complessiva del mercato, possiede 15 stazioni affiliate ABC (comprese le filiali in mercati più grandi come Cleveland, Phoenix, Detroit e Denver) e attraverso la proprietà della società affiliata di Phoenix KNXV, della società affiliata di Las Vegas KTNV-TV e della società affiliata di Tucson KGUN-TV, l\\' unico fornitore di programmi ABC per la programmazione ABC.',\n",
       "   'qas': [{'answers': [{'answer_start': 10,\n",
       "       'text': 'Sinclair Broadcast Group'},\n",
       "      {'answer_start': 10, 'text': 'Sinclair Broadcast Group'},\n",
       "      {'answer_start': 10, 'text': 'Sinclair Broadcast Group'}],\n",
       "     'id': '57277bfc708984140094ded9',\n",
       "     'question': 'Qual è il più grande operatore di stazioni ABC?'},\n",
       "    {'answers': [{'answer_start': 10, 'text': 'Sinclair'}],\n",
       "     'id': '57277bfc708984140094deda',\n",
       "     'question': 'Qual è la più grande affiliata di sottocanale ABC in base alle dimensioni dei mercati?'},\n",
       "    {'answers': [{'answer_start': 432, 'text': 'La società E. W. Scripps'},\n",
       "      {'answer_start': 443, 'text': 'E. W. Scripps Company'}],\n",
       "     'id': '57277bfc708984140094dedb',\n",
       "     'question': 'Qual è il più grande gestore di stazioni ABC in termini di portata sul mercato?'},\n",
       "    {'answers': [{'answer_start': 561, 'text': '15'},\n",
       "      {'answer_start': 561, 'text': '15'}],\n",
       "     'id': '57277bfc708984140094dedd',\n",
       "     'question': 'Quanti sono gli affiliati ABC di E. W. Scripps Company?'}]},\n",
       "  {'context': 'Il 16 giugno 2007, ABC ha iniziato a mettere in fase una nuova campagna di imaging per la prossima stagione 2007-08,\"Start Here\". Sviluppato anch\\' esso dalla Troika, il progetto on-air doveva enfatizzare la disponibilità di contenuti ABC su più piattaforme (in particolare, utilizzando un sistema di icone che rappresentano diversi dispositivi, come televisione, computer e dispositivi mobili) e \"semplificare e portare molta più coerenza e continuità alla rappresentazione visiva di ABC\". Anche il logo ABC è stato ridisegnato in modo significativo come parte della transizione, con un effetto \"palla\" lucido che è stato progettato appositamente per l\\' HD. In aria, il logo era accompagnato da effetti animati di acqua e nastro. I nastri rossi sono stati utilizzati per rappresentare la divisione intrattenimento, mentre i nastri blu sono stati utilizzati per ABC News.',\n",
       "   'qas': [{'answers': [{'answer_start': 117, 'text': 'Start Here'},\n",
       "      {'answer_start': 116, 'text': '\"Start Here\"'},\n",
       "      {'answer_start': 117, 'text': 'Start Here'}],\n",
       "     'id': '57277cf6dd62a815002e9e76',\n",
       "     'question': 'Qual è stato il nome della campagna di imaging ABC iniziata nel 2007?'},\n",
       "    {'answers': [{'answer_start': 158, 'text': 'Troika'},\n",
       "      {'answer_start': 158, 'text': 'Troika'},\n",
       "      {'answer_start': 158, 'text': 'Troika'}],\n",
       "     'id': '57277cf6dd62a815002e9e77',\n",
       "     'question': 'Chi ha sviluppato la campagna di imaging 2007 per ABC?'},\n",
       "    {'answers': [{'answer_start': 785, 'text': 'la divisione intrattenimento'},\n",
       "      {'answer_start': 798, 'text': 'intrattenimento'},\n",
       "      {'answer_start': 798, 'text': 'intrattenimento'}],\n",
       "     'id': '57277cf6dd62a815002e9e78',\n",
       "     'question': 'Nastro rosso nel logo sono stati utilizzati per rappresentare quale divisione di ABC?'},\n",
       "    {'answers': [{'answer_start': 861, 'text': 'ABC News'},\n",
       "      {'answer_start': 861, 'text': 'ABC News'},\n",
       "      {'answer_start': 861, 'text': 'ABC News'}],\n",
       "     'id': '57277cf6dd62a815002e9e79',\n",
       "     'question': 'Nastri blu nel logo sono stati utilizzati per rappresentare quale divisione di ABC?'}]},\n",
       "  {'context': \"La rete ha la singolare distinzione di avere affiliate possedute e gestite separatamente che servono lo stesso mercato in Tampa, Florida (WFTS-TV e WWSB) e Grand Rapids, Michigan (WZZM e WOTV), con una situazione analoga in Kansas City, Missouri (KMBC-TV e KQTV). KQTV è concesso in licenza a St. Joseph, Missouri, che è designato da Nielsen come mercato separato dalla città di Kansas nonostante sia situato entro 55 miglia (89 km) l' uno dall' altro, mentre WWSB e WOTV servono le aree che non ricevono un segnale adeguato dal loro mercato primario affiliato ABC (nel caso di WWSB, questo risale a quando WTSP era la principale affiliata ABC Tampa prima del 1994, con WWSB, questo risale a quando WTSP era la principale affiliata ABC Tampa prima del 1994.\",\n",
       "   'qas': [{'answers': [{'answer_start': 138, 'text': 'WFTS-TV e WWSB'},\n",
       "      {'answer_start': 138, 'text': 'WFTS-TV e WWSB'},\n",
       "      {'answer_start': 138, 'text': 'WFTS-TV e WWSB'}],\n",
       "     'id': '57277de9dd62a815002e9ea4',\n",
       "     'question': 'Quali sono i due affiliati ABC per Tampa, Florida?'},\n",
       "    {'answers': [{'answer_start': 247, 'text': 'KMBC-TV e KQTV'},\n",
       "      {'answer_start': 247, 'text': 'KMBC-TV e KQTV'},\n",
       "      {'answer_start': 247, 'text': 'KMBC-TV e KQTV'}],\n",
       "     'id': '57277de9dd62a815002e9ea5',\n",
       "     'question': 'Quali sono le due affiliate di ABC per Kansas City, Missouri?'},\n",
       "    {'answers': [{'answer_start': 180, 'text': 'WZZM e WOTV'},\n",
       "      {'answer_start': 180, 'text': 'WZZM e WOTV'},\n",
       "      {'answer_start': 180, 'text': 'WZZM e WOTV'}],\n",
       "     'id': '57277de9dd62a815002e9ea6',\n",
       "     'question': 'Quali sono i due affiliati ABC per Grand Rapids Michigan?'},\n",
       "    {'answers': [{'answer_start': 607, 'text': 'WTSP'},\n",
       "      {'answer_start': 607, 'text': 'WTSP'},\n",
       "      {'answer_start': 607, 'text': 'WTSP'}],\n",
       "     'id': '57277de9dd62a815002e9ea7',\n",
       "     'question': \"Che cosa era l' affiliata ABC primaria di Tampa prima del 1994?\"}]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With our DatasetDict object, we can easily do some basic indexing, using a format similar to that employed in the\n",
    "# pandas library.\n",
    "\n",
    "# Let's take a look at the third example in our test split:\n",
    "squad_it_dataset[\"test\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfJ001xT3Vna"
   },
   "source": [
    "Now, let's load this dataset in remotely (instead of downloading it locally beforehand using `wget` before locally downloading the dataset). We'll use the HTTPS protocol in order to load in our `DatasetDict` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324,
     "referenced_widgets": [
      "0d6afe7ae5af40db9beb4bfbf7183185",
      "c5b542cb7d6141068ceb18b5c8fea4d7",
      "365c9bc0d75547ef98c61b2bb66b5d5d",
      "c410889940b24b839df05a2d6542f64d",
      "f85561d800ca457e954675463597a72a",
      "41f2a69a3f40431d95d561c1c7cc177a",
      "3580488f8ad5453eba41cc91de664f63",
      "9ea7f11f232846dc9197c461f65e21d8",
      "086ac4850f7d42deb673cd0ce492c1e7",
      "9b1b7da05e1947b885ebf091cb9f41b7",
      "1e69cc0ec4854b34b203825cf960cc86",
      "8bd3047c4c034b6aad7b7687b5946285",
      "2ef81a6c002943f695c43c30d66b15a3",
      "3c3e52cdf6b84b4ca1de545b771cac47",
      "af03a32584df45a6895ab81d829640f8",
      "2067a514486a43ae8e02d708d527aa33",
      "4d8add88e1c341769ecbb6ee8f11774e",
      "e7b3863b67a44dc192a18207ef777b07",
      "1b53d197f2284c6bb28a0c91ebe09742",
      "2dff8ef313bc42bea54e11f3b9d457d1",
      "f788afe376ce43a482614dc64e6e5c30",
      "399508cd776c482a8edfcf051fae382f",
      "3a28e31d7caf4d8983a332670ce0a18d",
      "27526615450f4234af86c3d9c3816223",
      "b6b1d30de3c14507940bf69496fa866a",
      "3e52cdd7be784b37ab991f539540d432",
      "16f39feb485e43e89bb5df06448ff9c2",
      "e2b4ba01be4b450991c595a73976d6a1",
      "4ac8c90e16354f4584735f46b48219eb",
      "230747e0419d4d348b109ae0039e7d71",
      "f79e361f44134defaa31a7ef5d6d2cdf",
      "dddda941fe8e46f294e80c0cbc219f81",
      "a664139fa35e4859af79b0501b9dfcca",
      "c683c9375669406c9aed79819475e329",
      "4c43fe90ef584734936f528415f894d1",
      "cb12096c172f421eb42f951cb571bb53",
      "7c1b30e74890461faceb0134fb85ab4c",
      "e5e360adf801414c9c80d34e56cd3ab4",
      "f8d1ffd865d94cdd8a2df0fed792896e",
      "7fd97a795dc545738fa959d1d12beaeb",
      "2f2c22a95cf94c9888fa1f97127d95b9",
      "f8fbf01f2efc49ebaf09e26d92527591",
      "a775bd38dbdd4525abe05c832910bb91",
      "4fc806f444f04be5997d80692a2f2c50"
     ]
    },
    "id": "m13BOjQjYK5k",
    "outputId": "4bd7f4c2-c5a1-4831-a183-3721cff9d7d1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6afe7ae5af40db9beb4bfbf7183185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd3047c4c034b6aad7b7687b5946285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a28e31d7caf4d8983a332670ce0a18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c683c9375669406c9aed79819475e329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing our ' data_files ' argument.\n",
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "\n",
    "# Loading in our dataset.\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "\n",
    "# Final result\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6zyelyT08Zl"
   },
   "source": [
    "### **6.1.2: Basic Functionality: Dataset.map(), Dataset.filter(), and more**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdrdzgYFaoli"
   },
   "source": [
    "$\\textbf{Introduction:}$\n",
    "\n",
    "**Source:** [Time to slice and dice](https://huggingface.co/learn/nlp-course/chapter5/3?fw=pt)\n",
    "\n",
    "In this section, we'll be working with the **Drug Review Dataset** that's hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/) to demonstrate some important HF Datasets functions and methods. This dataset contains patient reviews on various drugs, along with the condition being treated as well as a 10-star rating of the patient’s satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV98e2Ji0S0w",
    "outputId": "c79e3bc5-a8a6-4941-a6af-ddd96b00317a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-18 05:19:48--  https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘drugsCom_raw.zip’\n",
      "\n",
      "drugsCom_raw.zip        [       <=>          ]  41.00M  30.2MB/s    in 1.4s    \n",
      "\n",
      "2024-09-18 05:19:49 (30.2 MB/s) - ‘drugsCom_raw.zip’ saved [42989872]\n",
      "\n",
      "Archive:  drugsCom_raw.zip\n",
      "  inflating: drugsComTest_raw.tsv    \n",
      "  inflating: drugsComTrain_raw.tsv   \n"
     ]
    }
   ],
   "source": [
    "# Loading in & unzipping our dataset\n",
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Psocuc3lbQsL"
   },
   "outputs": [],
   "source": [
    "# Instantiating our dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "  # TSV is just a variant of CSV that uses tabs instead of commas as the separator.\n",
    "  # In Python, \\t is the tab character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qX3RrR-TEHDm"
   },
   "source": [
    "Let's know grab a random sample from our dataset in order to better understand the data that we're working with. The `Dataset.shuffle()` method can be used to randomly shuffle the dataset. The `Dataset.select()` method can be used to select elements from a dataset given an iterable (e.g., list) of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yhB6h0abyZk",
    "outputId": "e3e164bd-fb02-4e7f-9940-d8785aecb5d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grabbing a small sample from our dataset:\n",
    "sample = drug_dataset[\"train\"].shuffle(seed = 42).select(range(1000))\n",
    "  # We pass ' range(1000) ' to our shuffle method to take the first 1000 examples\n",
    "  # of our shuffled dataset.\n",
    "\n",
    "# The first three examples in our shuffled Dataset:\n",
    "sample[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrVayw5TcUiV"
   },
   "source": [
    "Right now, one may notice the following:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">1.   The `Unnamed: 0` column is possibly an anonymized ID for each patient; if so, we'd like to rename this column with a more descriptive name.\n",
    "2.   Our `condition` column contains a mixture of uppercase and lowercase labels which we'd like to normalize.\n",
    "3.   The entries of our `review` colum are all of various lengths; additionally, some contain HTML character codes (e.g., `&\\#039;`), which we'd like to strip.\n",
    "\n",
    "---\n",
    "\n",
    "All of the above are examples of dataset pre-processing you might want/have to do in order to properly prepare your training corpus -- they also present a great opportunity to demonstrate many of 🤗 Dataset's useful methods & functions. Let's now address each of these issues below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKu0AiseFTpz"
   },
   "source": [
    "$\\textbf{Issue 1:}$\n",
    "\n",
    "Before going ahead with our edits, let's first confirm the validity of our Patient ID hypothesis. We'll do this by checking whether the number of unique `Unnamed: 0` values is equivalent to the length of our dataset (i.e., the total number of entries in our dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfbUsVPpbe8_",
    "outputId": "e9cf654f-681b-4cc9-e349-d4dc7979c24b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Dataset.unique() function can be used to verify that the number of IDs\n",
    "# matches the number of rows in each split:\n",
    "for split in drug_dataset.keys():\n",
    "  assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))\n",
    "\n",
    "\n",
    "# Having confirmed our suspicions, let's call the Dataset.rename_column()\n",
    "# method to rename \"Unnamed: 0\" to a more descriptive title.\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "\n",
    "# Final Result:\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEiTX39-GC3K"
   },
   "source": [
    "Great! We can now move on to **Issue 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15P9HG7ofRq1"
   },
   "source": [
    "$\\textbf{Issue 2:}$\n",
    "\n",
    "Before we can address **Issue 2**, we need to introduce two new functions: `Dataset.filter()` and `Dataset.map()`.\n",
    "\n",
    "`Dataset.filter()` allows one to apply a filter function to all the elements in a Dataset's Arrow table (individually or in batches), updating the table to only include examples which when applied to the filter function return `True`. You can read more about the function in its documentation [here](https://huggingface.co/docs/datasets/v3.0.0/en/package_reference/main_classes#datasets.Dataset.filter).\n",
    "\n",
    "`Dataset.map()` is a more general function which allows one to apply a function to all examples in a Dataset's Arrow table (individually or in batches), updating the table. If our function returns a column that already exists, this function will override this column's values. You can read more about the function in its documentation [here](https://huggingface.co/docs/datasets/v3.0.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "We'll discuss using `batched = True` soon, but for now we assume that we are only feeding in individual inputs into the above two functions. As such, the functions we pass in as arguments to these functions should only expect a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "bf9943f434bc4eca8c41af071ed6c07a",
      "5d7d8625a69e4dedbc8aaf567f5c765f",
      "02c5738e694448b3b6803748c9616572",
      "4cf48abee0db4c7d915ae59f58ee8323",
      "964b4560f89c4891a22c3ca3ccaf4ac2",
      "95fd0f779b0144eeb0fc942e0b8e7c31",
      "ab67fd6841a24718b0da30ef1937413e",
      "6bbfacc098f94bb2af7eb996b24969a9",
      "dfe4967377404e39968df437f43df46e",
      "470d08173d8d4df6b28c0d2da97d4934",
      "28ab52191de048e8a1ae874ff6c3ac47",
      "6d28b36d1b0f4d5ca376c608c7b81f34",
      "7fff304070da45e6aa697f21f5ce37e5",
      "248f958a4ce948d28a357d458bc138b4",
      "7223ef5c3b8c4d2099f718195dfce53e",
      "5d4cacc86561403491e1e32c158a9a1d",
      "fb27dfe141914bd5a1a61f76ded54910",
      "22740b7bc3944dba94bc60276b848c5c",
      "aaa717a474924d8793fc631c91bf5b3b",
      "43a89feac24f4fc1b9ca2a299d096948",
      "04d8073b2fd74211b1698843de1bbd88",
      "2fca19112c6b4a03a593e0692474053e",
      "1b96539c850e481a8e98ac560d88437a",
      "36b9153ea1b749cdba79ed9d3e640f05",
      "072886f2ee584f3aa0c8fab312ad8d3c",
      "2cb6771daf0044a682ecd87e1ea118e3",
      "cd42d06c1ea64741bacc2a0a88f56a1d",
      "ec5e0372597f46908525208389aa7560",
      "36c285cb860545d9970f1701aeda341e",
      "72656095a5c0432daca522e0f46887d1",
      "05552194d4454736b267c0f01cf03b9d",
      "55e93d09f30948eb9f058283cbf27ea0",
      "3d21dbbd1a0549c19ccbf49409ac286b",
      "b6181652e2c14ed288934f8c315d8059",
      "632345cc70ce42f2807c75d57e8c7742",
      "c2375ec7b23746bab5800af868eedeaa",
      "0c1835d957d5406bb04206a7f7468f0b",
      "2244e4919b1f4f17a9d4720688a6811b",
      "583d7d9a684a4a27a07f356d3385c953",
      "b0b14863de944a60857e827180c4baca",
      "95edb6fa314f432790c32b21b5688f0c",
      "283e8a195a8b4ac58e9fd66a89e6a232",
      "27da97c605a545978f81e0f02f268361",
      "b17bbfabe508495886d87fcc9761d4ca"
     ]
    },
    "id": "JEWdDb51he1L",
    "outputId": "1e2f3d43-5dad-4f9d-ebe7-2776657c20c7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9943f434bc4eca8c41af071ed6c07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/161297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d28b36d1b0f4d5ca376c608c7b81f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b96539c850e481a8e98ac560d88437a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6181652e2c14ed288934f8c315d8059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['left ventricular dysfunction', 'adhd', 'birth control']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we use Dataset.filter() to filter out any examples whose entires to\n",
    "# the \"condition\" column are None:\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "# Now, we use Dataset.map() to normalize our \"condition\" column by lowercasing\n",
    "# every entry:\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"condition\": x[\"condition\"].lower()})\n",
    "\n",
    "# Let's check if our functions worked:\n",
    "drug_dataset[\"train\"][\"condition\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKfD8UVsi3Gm"
   },
   "source": [
    "$\\textbf{Issue 3:}$\n",
    "\n",
    "Let's check the lengths of each of the entries in our \"review\" column to get a better appreciation of the variety of different reviews we are working with.\n",
    "\n",
    "As we can see below, `Dataset.map()` is a smart function in that when our input function returns a dictionary whose key does not correspond to one of the column names in the dataset, `Dataset.map()` will automatically create a new column.\n",
    "\n",
    "---\n",
    "\n",
    "*Note: An alternative way to add a new column to each of our Dataset splits would be to use the `Dataset.add_column()` function. You can read this function's documentation [here](https://huggingface.co/docs/datasets/v3.0.0/en/package_reference/main_classes#datasets.Dataset.add_column).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "2f27c33474374605afe2a28e13924ed8",
      "f1b3d1b2bef2426bae36fab59f2ee875",
      "547dc7e6501a488fa4cd964e59f72d7f",
      "f6425a44a8ae4840be235aa5028b309f",
      "a4553b4a607d43da96d07d5c820232e6",
      "c69b42d38ace418b992c53557bcc1484",
      "2436944370cb48cba8c7596ca7cf4f4e",
      "5bc7fe960e3149f89e67b6aa0149e77f",
      "629a999788db4ab3b168fd47fd492d90",
      "595a47155e6e404c98bc83d580b3dc0a",
      "ce39ca919e5645da9579220ccaac6c5f",
      "11e58c9647484d9ebfe21157c5921ce2",
      "344440a421b840f2a195b0836333683a",
      "a4529452e35d4ea59e7bfa92fcde0328",
      "c7ecd3a9f13d4d1d94b9c06c6eb229d2",
      "6b51ad7657db4f22b1e99531c651c0b7",
      "97c16372bab64716afcd7db92a081b45",
      "710877685a514b2e842f7f9bc43d601d",
      "0b433daea9d44931bddecd5336273f00",
      "16d32f3c85f14a778d6906497b82c2bb",
      "1c3a01f3740a424f86a25fd887b46b23",
      "ae237b6bcc6b4b50826e3073c2276207"
     ]
    },
    "id": "UWB6Xn8OjaZV",
    "outputId": "4658f811-492b-4f0b-f061-514fa112b654"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f27c33474374605afe2a28e13924ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e58c9647484d9ebfe21157c5921ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'patient_id': [111469, 13653, 53602],\n",
       " 'drugName': ['Ledipasvir / sofosbuvir',\n",
       "  'Amphetamine / dextroamphetamine',\n",
       "  'Alesse'],\n",
       " 'condition': ['hepatitis c', 'adhd', 'birth control'],\n",
       " 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'],\n",
       " 'rating': [10.0, 10.0, 10.0],\n",
       " 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'],\n",
       " 'usefulCount': [41, 3, 0],\n",
       " 'review_length': [1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use splitting on whitespace as a rough indicator of the length of each of\n",
    "# our reviews:\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review_length\": len(x[\"review\"].split())})\n",
    "\n",
    "'''\n",
    "Now, let's use Dataset.sort() to sort the entries in our dataset by the values\n",
    "stored in our newly created column. The first couple of entries in our sorted\n",
    "Dataset will contain the shortest reviews.\n",
    "'''\n",
    "\n",
    "# Here, we'll just take a look at the training split:\n",
    "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRlS_MuIG8s7"
   },
   "source": [
    "It turns out that we're working with some pretty short reviews. In addition to stripping HTML character codes, let's also filter our dataset so that each split only contains reviews which are longer than 30 words. Such reviews wouldn't be sufficiently informative if we were trying to build a language model that can predict somebody's condition based upon a provided review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1ce6b1a50c184af1b6494ac514b77db3",
      "a5940fe42fc64cf48f327b6080c48e01",
      "40c774677a964f4980179be646ca8211",
      "f631f52a73e24dea914396d8942192a1",
      "f01ee10fdf0b49998862c50db28461d9",
      "a20cf84aa06e41b9acfa2fc9c91f934d",
      "beab426124584f0ab5c35607671db35d",
      "017f632f86e245cea02d7f0049b86a76",
      "f1105955667b4407b03b14e02a95c77a",
      "2573067ea2604793a21ff864585db5e4",
      "6b7f360cbed14fa7b594ac9e7f452dc0",
      "cbb6ccff2c794f2097cb4ecfba713392",
      "7446f374811542a6acd95179ae65c483",
      "7399624495ab470eb43fdd9a82697c69",
      "fcd24b6959c24c4fb881db996be993a0",
      "9ed93da36ac74a908c2c6a4c49d0d9ed",
      "0bafc668d6614d769e8b4b32ef020356",
      "d692f1b12c5f44d5b97ab5b271a0af4d",
      "62e58310ea9c446d8c76fc56f4256a4c",
      "3e02d42354b34d3b8d7346755d395d8e",
      "a9f05233f6854d449a0f25af9a5fd983",
      "06f1715a0b5a41b592adce90ffc2ebaa",
      "25d3a4802c3c42c488a13868533e90ab",
      "425e638b713548e680e0fda771896485",
      "bd6f9d87e4c943c2ae9ef01fbffa6f40",
      "765ed0ed5463480ca0d548aadfb5f6c9",
      "798569c6db6a48c2bb7e0d772f2e690f",
      "ae4ffa0e35ee4bc79d411f72e8f24595",
      "0560c3d25fbb4063baf2bf7cdabf8f8e",
      "a6a7e34c9cff47b3a2084a1776a6469b",
      "b4af19f570a14f79885bc1d7ed1bf712",
      "e8fade9b4d0d4528a97c3b9639d792d6",
      "00c42286b9db42e19a544e2a8029d596",
      "7e56fb46c87d464086f0a3e011b591c5",
      "7108a91a51854ec8aabd6e55ce09cbe3",
      "34444ae2dcbe4c29a8e22c8809fdc846",
      "ac5b531d2b184974b169f9cb8639708c",
      "c7f39472eb5e4463a9513f6a48782f34",
      "d9c2dcbe066e4ac0924d91f111c1e394",
      "84308a8b1cc644aab938bccf6eace0e6",
      "57caa5833dd84b58b2be7b3244e7df6f",
      "7f0326c5e3a844d98018435b27f2883d",
      "2e0fa0065605432b9cc0087a7226ad9e",
      "dce7bc9c368c4a0ab04a343d09ed63ad"
     ]
    },
    "id": "r4e8Id6lki9G",
    "outputId": "1a2ba64d-23e3-47a9-b1f3-199416543343"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce6b1a50c184af1b6494ac514b77db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/160398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb6ccff2c794f2097cb4ecfba713392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d3a4802c3c42c488a13868533e90ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e56fb46c87d464086f0a3e011b591c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, we filter all of the examples in our dataset by review length:\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "\n",
    "# Now, we use Dataset.map() to unescape (i.e. remove) all the HTML characters\n",
    "# in our corpus:\n",
    "import html\n",
    "\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyqRFVltnC2o"
   },
   "source": [
    "Now, we're about halfway through Section 6.1.2. The last few topics we need to cover are as follows:\n",
    "\n",
    "---\n",
    "\n",
    ">1.   Increasing the peformance of `Dataset.map` using `batched = True`.\n",
    "2.   Converting `Dataset` objects to `pandas.DataFrame` objects and back.\n",
    "3.   Saving a dataset.\n",
    "\n",
    "---\n",
    "\n",
    "Let's now get to these agenda items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEIo0BrcnP-Q"
   },
   "source": [
    "$\\textbf{(1) Batched Functionality with Dataset.map()}$\n",
    "\n",
    "If we pass in `batched = True`, our `Dataset.map()` function will pass in a batch of examples to the map function all at once (the batch size is configurable but defaults to 1,000). In particular, our map function will receive a dictionary whose keys are fields of our dataset and whose values are lists of values (of the same length as our specified batch size) (see the first code cell of **Issue 3**, for example).\n",
    "\n",
    "For example, we could've speeded up the process of unescaping all of our HTML characters back in **Issue 3** by using:\n",
    "\n",
    "<pre><code class=\"python\">drug_dataset = drug_dataset.map(\n",
    "  lambda x: {\"review\":[html.unescape(o) for o in x[\"review\"]]}, batched=True\n",
    "  )\n",
    "</code></pre>\n",
    "\n",
    "We can further speed up the performance of `Dataset.map()` using multiprocessing. In order to enable multiprocessing, pass in an integer value specifying the number of processes to use to the `num_proc` argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqYdBJRNIBo3"
   },
   "source": [
    "To truly appreciate the power of these computational speed-ups, let's time our `Dataset.map()` function applied to a set of earlier copies of our `drug_dataset`, each with a different configuration `Dataset.map()` configuration. In particular, we'll be directly importing each `drug_dataset` copy from the UC Irvine Machine Learning Repository -- meaning, namely, that no HTML unescaping has been applied to any of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vm2LArwLxgRd"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import html\n",
    "\n",
    "# Creating new temporary directories & storing the relevant files\n",
    "for idx in range(1, 4):\n",
    "  os.mkdir(f\"/content/huggingface_folder/temp_folder_{str(idx)}\")\n",
    "  os.chdir(f\"/content/huggingface_folder/temp_folder_{str(idx)}\")\n",
    "  !wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "  !unzip drugsCom_raw.zip\n",
    "\n",
    "# Instantiate drug_dataset_1\n",
    "os.chdir(\"/content/huggingface_folder/temp_folder_1\")\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "drug_dataset_1 = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "# Instantiate drug_dataset_2\n",
    "os.chdir(\"/content/huggingface_folder/temp_folder_2\")\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "drug_dataset_2 = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "# Instantiate drug_dataset_3\n",
    "os.chdir(\"/content/huggingface_folder/temp_folder_3\")\n",
    "\n",
    "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
    "drug_dataset_3 = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "\n",
    "## Timing our Dataset.map() function with various different computational\n",
    "## speed-ups implemented:\n",
    "# No batched functionality, no multiprocessing\n",
    "%time drug_dataset_1 = drug_dataset_1.map(lambda x: {\"review\": html.unescape(x[\"review\"])})\n",
    "\n",
    "# Batched functionality, no multiprocessing\n",
    "%time drug_dataset_2 = drug_dataset_2.map(lambda x: {\"review\":[html.unescape(o) for o in x[\"review\"]]}, batched = True)\n",
    "\n",
    "# Batched functionality, 4 processes\n",
    "%time drug_dataset_3 = drug_dataset_3.map(lambda x: {\"review\":[html.unescape(o) for o in x[\"review\"]]}, batched = True, num_proc = 4)\n",
    "\n",
    "\n",
    "# Changing back to the original directory (for Section 6)\n",
    "os.chdir(\"/content/huggingface_folder\")\n",
    "\n",
    "# Removing the temporary directories we created (we no longer need them or their\n",
    "# contents):\n",
    "for idx in range(1,4):\n",
    "  os.remove(f\"/content/huggingface_folder/temp_folder_{str(idx)}/drugsComTest_raw.tsv\")\n",
    "  os.remove(f\"/content/huggingface_folder/temp_folder_{str(idx)}/drugsComTrain_raw.tsv\")\n",
    "  os.remove(f\"/content/huggingface_folder/temp_folder_{str(idx)}/drugsCom_raw.zip\")\n",
    "  os.rmdir(f\"/content/huggingface_folder/temp_folder_{str(idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Osu310qIyfv7"
   },
   "source": [
    "As we see above, deploying `Dataset.map()` utilizing `batched` and/or `num_proc` has results in significant time-saves. In our case, using a single process ended up being faster in comparison to multiprocessing (likely due to the introduced communication overhead); for more complicated mapping functions, though, `num_proc` can prove to be a significant time-saver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhIMGn6w1jTt"
   },
   "source": [
    "$\\textbf{(2) Converting Datasets to DataFrames & Back}$\n",
    "\n",
    "To enable the conversion between various third-party libraries, HF Datasets provides a `Datasets.set_format()` function. This function changes only the *output format* of the dataset -- the underlying *data format* is still Apache Arrow. Under the hood, `Datasets.set_format()` does this by changing the return format for the dataset's `__getitem__()` dunder method.\n",
    "\n",
    "Let's see how `Datasets.set_format()` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "IU65DjUr2-xw",
    "outputId": "11677b5b-b53f-4c5b-ab85-36af1b4c2ea4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "train_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-358acf70-b27f-4f80-9724-855142ec5769\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35696</td>\n",
       "      <td>Buprenorphine / naloxone</td>\n",
       "      <td>opiate dependence</td>\n",
       "      <td>\"Suboxone has completely turned my life around...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>November 27, 2016</td>\n",
       "      <td>37</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155963</td>\n",
       "      <td>Cialis</td>\n",
       "      <td>benign prostatic hyperplasia</td>\n",
       "      <td>\"2nd day on 5mg started to work with rock hard...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>November 28, 2015</td>\n",
       "      <td>43</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-358acf70-b27f-4f80-9724-855142ec5769')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-358acf70-b27f-4f80-9724-855142ec5769 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-358acf70-b27f-4f80-9724-855142ec5769');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-72a11793-09b4-4817-930b-8b8c37723663\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72a11793-09b4-4817-930b-8b8c37723663')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-72a11793-09b4-4817-930b-8b8c37723663 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   patient_id                  drugName                     condition  \\\n",
       "0       95260                Guanfacine                          adhd   \n",
       "1       92703                    Lybrel                 birth control   \n",
       "2      138000                Ortho Evra                 birth control   \n",
       "3       35696  Buprenorphine / naloxone             opiate dependence   \n",
       "4      155963                    Cialis  benign prostatic hyperplasia   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "3  \"Suboxone has completely turned my life around...     9.0   \n",
       "4  \"2nd day on 5mg started to work with rock hard...     2.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  \n",
       "3  November 27, 2016           37            124  \n",
       "4  November 28, 2015           43             68  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the output format of our dataset to a pandas DataFrame\n",
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "# Creating a DataFrame instance of the training split of drug_dataset\n",
    "train_df = drug_dataset[\"train\"][:]\n",
    "'''\n",
    "    Notice that we take a (complete) slice of drug_dataset[\"train\"]. This is\n",
    "    because the data format of drug_dataset[\"train\"] is still a Dataset object,\n",
    "    whereas a slice of drug_dataset[\"train\"] (which renders a particular ouput\n",
    "    format for drug_dataset[\"train\"]) is a DataFrame object.\n",
    "'''\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaK0t4RF3wVE"
   },
   "source": [
    "In real-life applications, we'd now take some time in order to pre-process our dataset using some of the more advanced functionality available in the `pandas` library. Depending upon the size of the dataset you're working with (as well as your available RAM & disk space), it can be a good idea to switch over to `pandas` in order to do some of your more advanced dataset pre-processing, as 🤗 Datasets can in certain instances be clunky to work with.\n",
    "\n",
    "Here, we're going to skip the pre-processing step & go straight to reverting our `DataFrame` object back to a `Dataset` object:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3oAs3Rc3uoO",
    "outputId": "c384e0bf-fa72-4c32-96a6-1afe548b9650"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "    num_rows: 138514\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xjo9Vgze4Aq8"
   },
   "source": [
    "Finally, we can reset the output format of `drug_dataset` from `\"pandas\"` back to `\"arrow\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jagyhRr39E7"
   },
   "outputs": [],
   "source": [
    "drug_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxHoC6894H7Z"
   },
   "source": [
    "$\\textbf{(3) Saving Our Dataset}$\n",
    "\n",
    "There's a lot more functionality to explore as it pertains to HF Datasets (some of which we'll see in Section 6.4). For now, though, let's be satisfied with what we've covered so far & move on to saving our processed dataset.\n",
    "\n",
    "HF automatically caches every downloaded dataset and the operations performed on it$^1$; to save a dataset to your disk, though, you'll need to manually do so.\n",
    "\n",
    "HF Datasets provides three main functions to save your dataset in different formats.\n",
    "\n",
    "\\begin{array}{cc}\n",
    "  \\textbf{Data format} & \\textbf{Function} \\\\\n",
    "  \\hline\n",
    "  \\small{Arrow} & \\scriptsize{Dataset.save\\_to\\_disk()} \\\\\n",
    "  \\small{CSV} & \\scriptsize{Dataset.to\\_csv()} \\\\\n",
    "  \\small{JSON} & \\scriptsize{Dataset.to\\_json()}\n",
    "\\end{array}\n",
    "\n",
    "Using the `Arrow` format, you can store your entire dataset into a single directory. In particular, this directory will contain a $\\;$ *dataset_dict.json* $\\;$ followed by a subdirectory for each split of your Dataset (in our case, a *train* and *test* split). Each split subdirectory contains its own $\\;$ *dataset.arrow* $\\;$ table, as well as some metadata in $\\;$ *dataset_info.json* $\\;$ and $\\;$ *state.json* $\\;$ (and possibly some additional files).\n",
    "\n",
    "Another advantage of the `Arrow` format is that you can easily load your dataset back in. For example, when saving this dataset by doing something like the following:\n",
    "\n",
    "<pre><code class=\"python\">drug_dataset.save_to_disk(\"drug-reviews\")\n",
    "</code></pre>\n",
    "\n",
    "you can easily load the dataset back in using `load_from_disk()`:\n",
    "\n",
    "<pre><code class=\"python\">drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
    "</code></pre>\n",
    "\n",
    "For the `CSV` and `JSON` formats, you'll need to store each split as a separate file. For example, you could save each `Dataset` in your `DatasetDict` object into separate `JSONL` files utilizing something like the following command:\n",
    "\n",
    "<pre><code class=\"python\">for split, dataset in drug_dataset.items():\n",
    "  dataset.to_json(f\"drug-reviews-{split}.jsonl\")\n",
    "</code></pre>\n",
    "\n",
    "To subsequently load in these files, use the techniques discussed in Section 6.1.1.\n",
    "\n",
    "---\n",
    "\n",
    "$^1$ *Note: By default, datasets are cached at `/.cache/huggingface/datasets`. You can change this location by setting the `HF_DATASETS_CACHE` environment variable to another directory path, such as follows:*\n",
    "\n",
    "<pre><code class=\"python\">os.environ[\"HF_DATASETS_CACHE\"] = \"/path/to/another/directory/datasets\"\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYen8FTB1i1q"
   },
   "source": [
    "### **6.1.3: Apache Arrow and memory-mapped files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfoQ8A93p9gJ"
   },
   "source": [
    "$\\textbf{Introduction:}$\n",
    "\n",
    "**Source:** [The magic of memory mapping](https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt#the-magic-of-memory-mapping)\n",
    "\n",
    "Before getting into a theoretical explanation of memory-mapping and the Arrow file format, let's practically demonstrate the memory saves achieved by this file format through **The Pile**.\n",
    "\n",
    "From the [HF NLP Course](https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt#what-is-the-pile):\n",
    "\n",
    ">\"The Pile is an English text corpus that was created by EleutherAI for training large-scale language models. It includes a diverse range of datasets, spanning scientific articles, GitHub code repositories, and filtered web text. The training corpus is available in 14 GB chunks, and you can also download several of the individual components.\"\n",
    "\n",
    "For our demonstration, we're going to be looking at the PubMed Abstracts dataset, a corpus of abstracts from 15 million biomedical publications on PubMed. This dataset is in a `JSON Lines` format and has been compressed using the `zstandard` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312,
     "referenced_widgets": [
      "7757895fa6e64754a1bb29a861004099",
      "1098a24df60b4d4eb635ac75ffc3b3fa",
      "d995b0c09dd5484c99eae8ffb5f850b5",
      "b827d13ee84641a794e52a166116da3b",
      "2017fcef83174a87ae5a1e515384bb20",
      "3db7eaf5931e413d9c4635bc5f5168b1",
      "526ea82f6f6144be972927eebb7516ba",
      "2d8b9e2e766a4de59da113b0246bb9d1",
      "3f8af543dc3e4e52bf9f5b035f9857ae",
      "a86adb9538dc481eb2fa3aa9aa99ff34",
      "c7c6fa4a40c24bbbbb10bfb7458592f0",
      "de8cf7730a4641bdbee70cf84052f859",
      "4e081c937e084489a873ce02ae52817d",
      "fc8c5e5967464854aa4ffe86bd313416",
      "27b34efb731a4ad6968ea286843fa883",
      "c027c1b8a3cb4d9695c74f1fb1e4d3fd",
      "f82de2e8229048cb830a9ebe688fcb07",
      "18631042540f47078dd64dda1e3bdbe1",
      "012751883c5545a88975c1d9918373c6",
      "dd62466222c448a9b966dafa16ee86e5",
      "9649fce961514fad84b5aeafe2ae3312",
      "3447893d73dd48058ab70b6332457484",
      "72a9f15d88644d4a867419502ec69103",
      "e5da73f5f68e4c7288a56f4835d3aa01",
      "5af68aa8eb6f4270ba6089e8be799389",
      "87a62c6065e14478b43e0408fc214375",
      "ba8789535bc84b63a7f943474af3b981",
      "e0d19b85f6494157a385bd93c632f4c6",
      "9b0dace58aa44bc9a6a7238290171053",
      "a54c414867994d7fa6ad381237f75118",
      "0f501b8946e5459bb5ecd6c2a5ee1626",
      "e74d72d65fc44b74addbf9ca760173ea",
      "e09b76a53ca54466aea3b1534597678f"
     ]
    },
    "id": "HvmvLPvroGdX",
    "outputId": "0481945e-8730-4ff0-ad43-adbdfb7271a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7757895fa6e64754a1bb29a861004099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)_title_abstracts_2019_baseline.jsonl.zst:   0%|          | 0.00/6.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8cf7730a4641bdbee70cf84052f859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a9f15d88644d4a867419502ec69103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['meta', 'text'],\n",
       "    num_rows: 15518009\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load in dataset\n",
    "data_files = \"https://huggingface.co/datasets/casinca/PUBMED_title_abstracts_2019_baseline/resolve/main/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "\n",
    "# See our results\n",
    "pubmed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yapkaI9atQeA"
   },
   "source": [
    "With 15,518,009 rows of data, we can already tell that this is a pretty large dataset. To get a better appreciation of the size of the dataset we're working with, let's measure the size of the dataset on our disk space. We can do so by accessing the `dataset_size` attribute, which measures the size of our dataset in bytes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VX5A2JQntsU2",
    "outputId": "74b7c648-ab93-43c6-dd23-dc78c5064b00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNumber of files in dataset :\u001b[0m 20978892555\n",
      "\u001b[1mDataset size (cache file) :\u001b[0m 19.54 GB\n"
     ]
    }
   ],
   "source": [
    "# Size of our dataset (in bytes)\n",
    "print(f\"\\033[1mNumber of files in dataset :\\033[0m {pubmed_dataset.dataset_size}\")\n",
    "\n",
    "# Size of our dataset (in gigabytes)\n",
    "size_gb = pubmed_dataset.dataset_size / (1024**3)\n",
    "print(f\"\\033[1mDataset size (cache file) :\\033[0m {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-NwBfokuO3B"
   },
   "source": [
    "As we can see, our dataset is nearly takes up nearly **20 GB** on our disk space. Now, let's check up how much RAM we're utilizing in order to load & access this dataset. For a point of reference, if we were loading this dataset in using `pandas`, we should expect (according to Wes Kinney’s famous [rule of thumb](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)) that we'd be using 5 to 10 times as much RAM as the size of our dataset (i.e., **100 - 200 GB** of RAM).\n",
    "\n",
    "One way to measure memory usage in Python is through the `psutil` library, which we'll install here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z1kFY2iAuHaO",
    "outputId": "a4b597e4-651f-4bd7-fbdc-7eb3f555127a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M5BgcdAvOdC"
   },
   "source": [
    "Using `psutil`, we can check the memory usage of our current process through the `Process()` class. In particular, we'll be looking at the memory profile of the `rss` attribute, or *resident set size*. RSS measures the fraction of memory that a process occupies in RAM. This measurement also includes the memory used by the Python interpreter, any libraries we’ve loaded, and a variety of other processes, so the actual amount of memory used to load the dataset is likely a bit smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQPZmEdXvI0E",
    "outputId": "339ec89a-dc59-44ff-fec3-cfc6ac136d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mRAM used:\u001b[0m 720.29 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"\\033[1mRAM used:\\033[0m {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ltb23erv_-B"
   },
   "source": [
    "In contrast to our expectations, we find that we've been able to load and access our dataset with ***significantly less RAM compared to the actual size of our dataset.***\n",
    "\n",
    "Now, let's get into the reasons why HF Datasets is allowing us to make such significant saves on memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udFSzbVoxfbt"
   },
   "source": [
    "$\\textbf{Apache Arrow & The Arrow Data Format}$\n",
    "\n",
    "As detailed [here](https://arrow.apache.org/overview/), the Arrow data format is a standardized, language-agnostic [in-memory](https://en.wikipedia.org/wiki/In-memory_database) columnar format for representing structured, table-like datasets in-memory. This data format also comes with a rich data type system, including nested and user-defined data types. Let's break down some of these terms.\n",
    "\n",
    "**Columnar Formatting:**\n",
    "\n",
    "![columnar_format.png](data:image/png;base64,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)\n",
    "\n",
    "[Columnar format](https://www.microsoft.com/en-us/research/publication/columnar-storage-formats/#:~:text=The%20columnar%20data%20formats%20are,the%20particular%20query%20or%20workload.) is a data storage format that organizes data by column instead of row. As opposed to row-oriented storage, columnar storage can significantly reduce the amount of data fetched from disk by allowing access to only the columns that are relevant for the particular query or workload. With further efficient encoding and compression techniques, the columnar format can drastically reduce storage requirements without sacrificing query performance.\n",
    "\n",
    "The columnar format also enables [vectorization](https://www.intel.com/content/www/us/en/developer/articles/technical/vectorization-a-key-tool-to-improve-performance-on-modern-cpus.html#:~:text=Vectorization%20is%20the%20process%20of,to%20multiple%20data%20(SIMD).) using the latest SIMD ([Single Instruction, Multiple Data](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data)) operations included in modern processors. In other words, the columnar format is one of many reasons we're able to use `batched = True` in our `Dataset.map()` function, for example.\n",
    "\n",
    "**Standardization & Arrow Libraries:**\n",
    "\n",
    "![standardization.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "\n",
    "Instead of having to deploy a unique internal data format for every database and every programming language, employing costly serialization and deserialization to move data from one system to another, Arrow's in-memory columnar data format represents an out-of-the-box solution which allows systems that use or support Arrow to transfer data between them at little-to-no cost.\n",
    "\n",
    "The larger Arrow project contains libraries that enable you to work with data in the Arrow columnar format in a variety of languages. Languages such as C++ and Java contain distinct implementations of the Arrow format, whereas the libraries in languages such as Python are built on top of the C++ library. In particular, Python uses the [`pyarrow`](https://arrow.apache.org/docs/python/index.html) library. These official libraries enable third-party projects to work with Arrow data without having to implement the Arrow columnar format themselves.\n",
    "\n",
    "**How Does This Apply to 🤗 Datasets?:**\n",
    "\n",
    "HF Datasets uses Arrow for its [local caching system](https://huggingface.co/docs/datasets/en/about_arrow#memory-mapping). Datasets are backed by an [on-disk cache](https://nordvpn.com/cybersecurity/glossary/disk-cache/#:~:text=Disk%20cache%20is%20a%20temporary,accessed%20information%20is%20readily%20available.), which is memory-mapped for fast lookup; in other words, HF Datasets treats each dataset as a memory-mapped file.\n",
    "\n",
    "What is memory mapping? In short, we can understand memory mapping as a mapping between RAM and filesystem storage that allows the HF library to access and operate on elements of the dataset without needing to fully load it into memory. This how we are able to utilize MB of RAM for a dataset which is taking up GB of our disk space. If you're interested in learning about the more granular details of memory mapping, [this article](https://www.ibm.com/docs/es/aix/7.2?topic=memory-understanding-mapping) might be of some use.\n",
    "\n",
    "Another major benefit of memory-mapped files is that such files can be shared across multiple processes, allowing us to parallelize the operations of `Dataset.map()` via the `num_proc` argument without needing to move or copy the dataset, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqvcaknc1_0w"
   },
   "source": [
    "### **6.1.4: Saving disk space by streaming datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn6r6hKO8o13"
   },
   "source": [
    "$\\textbf{Introduction:}$\n",
    "\n",
    "**Source:** [Streaming datasets](https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt#streaming-datasets)\n",
    "\n",
    "To understand the point of streaming datasets, let's return to **The Pile**. If one tried to download this dataset in its entirety, one would need 825 GB of free disk space! To deal with situations where we need to access datasets as large as **The Pile** without comprising our RAM *or* our storage space, HF Datasets provides a streaming feature which allows us to download and access elements on the fly, without needing to download the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXHAcmTT9f4g"
   },
   "source": [
    "To better understand how streamed datasets operate, let's stream in our PubMed Abstracts dataset; see Section 6.1.3 if you haven't yet downloaded the dataset and/or prepared `data_files`. To do this, we simply have to pass `streaming = True` to our `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GD9gVDrs9p33"
   },
   "outputs": [],
   "source": [
    "pubmed_dataset_streamed = load_dataset(\n",
    "    \"json\", data_files=data_files, split=\"train\", streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJclQJhA9spa"
   },
   "source": [
    "Instead of returning a familiar `Dataset`, streaming in our dataset has returned an `IterableDataset` object. Fortunately, an [`IterableDataset`](https://huggingface.co/docs/datasets/v3.0.0/en/package_reference/main_classes#datasets.IterableDataset) object isn't too different from a `Dataset` object: it's simply a `Dataset` object backed by an iterable. As this description implies, to access the elements of an `IterableDataset` we need to iterate over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eeDwKwG-UCU",
    "outputId": "4ee6a6dd-5484-4810-90b7-1a0c7ad48a2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
       " 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age. Systematic review of the published literature. Out-patient clinics, emergency departments and hospitalisation wards in 23 health centres from 10 countries. Cohort studies reporting the frequency of hypoxaemia in children under 5 years of age with ALRI, and the association between hypoxaemia and the risk of dying. Prevalence of hypoxaemia measured in children with ARI and relative risks for the association between the severity of illness and the frequency of hypoxaemia, and between hypoxaemia and the risk of dying. Seventeen published studies were found that included 4,021 children under 5 with acute respiratory infections (ARI) and reported the prevalence of hypoxaemia. Out-patient children and those with a clinical diagnosis of upper ARI had a low risk of hypoxaemia (pooled estimate of 6% to 9%). The prevalence increased to 31% and to 43% in patients in emergency departments and in cases with clinical pneumonia, respectively, and it was even higher among hospitalised children (47%) and in those with radiographically confirmed pneumonia (72%). The cumulated data also suggest that hypoxaemia is more frequent in children living at high altitude. Three papers reported an association between hypoxaemia and death, with relative risks varying between 1.4 and 4.6. Papers describing predictors of hypoxaemia have focused on clinical signs for detecting hypoxaemia rather than on identifying risk factors for developing this complication. Hypoxaemia is a common and potentially lethal complication of ALRI in children under 5, particularly among those with severe disease and those living at high altitude. Given the observed high prevalence of hypoxaemia and its likely association with increased mortality, efforts should be made to improve the detection of hypoxaemia and to provide oxygen earlier to more children with severe ALRI.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing the first element of our dataset\n",
    "next(iter(pubmed_dataset_streamed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1FuGpud-Yg4"
   },
   "source": [
    "To process the elements of a streamed dataset on the fly, we can use the `IterableDataset.map()` and `IterableDataset.filter()` functions. The process of using this function is exactly the same as using the `Dataset.map()` and `Dataset.filter()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydhjYSWo-1MV"
   },
   "source": [
    "The real difference between `Dataset` and `IterableDataset` comes down to how we access elements in our streamed dataset.\n",
    "\n",
    "For example, unlike `Dataset.shuffle()`, `IterableDataset.shuffle()` can only shuffle all the elements in a pre-defined `buffer_size`:\n",
    "\n",
    "<pre><code class = \"python\">shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n",
    "next(iter(shuffled_dataset))\n",
    "</code></pre>\n",
    "\n",
    "Once an example is accessed, its spot in the buffer is filled with the next example in the corpus; in the case above, this would be the 10,0001st example.\n",
    "\n",
    "In place of `Dataset.select()`, we can use `IterableDataset.take()` and `IterableDataset.skip()`.\n",
    "\n",
    "For example, to access the first 5 examples in the PubMed Abstracts dataset:\n",
    "\n",
    "<pre><code class = \"python\"><b>Input:</b>\n",
    "dataset_head = pubmed_dataset_streamed.take(5)\n",
    "list(dataset_head)\n",
    "</code></pre>\n",
    "\n",
    "<pre><code class = \"python\"> <b>Output:</b>\n",
    "[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
    "  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},\n",
    " {'meta': {'pmid': 11409575, 'language': 'eng'},\n",
    "  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},\n",
    " {'meta': {'pmid': 11409576, 'language': 'eng'},\n",
    "  'text': \"Hypoxaemia in children with severe pneumonia in Papua New Guinea ...\"},\n",
    " {'meta': {'pmid': 11409577, 'language': 'eng'},\n",
    "  'text': 'Oxygen concentrators and cylinders ...'},\n",
    " {'meta': {'pmid': 11409578, 'language': 'eng'},\n",
    "  'text': 'Oxygen supply in rural africa: a personal experience ...'}]\n",
    "</code></pre>\n",
    "\n",
    "Similarly, you can use the `IterableDataset.skip()` function to create training and validation splits from a shuffled dataset as follows:\n",
    "\n",
    "---\n",
    "\n",
    "*Note: To create training and validation splits when working with `Dataset` objects, you should work with the [`Dataset.train_test_split()`](https://huggingface.co/docs/datasets/v3.0.0/en/package_reference/main_classes#datasets.Dataset.train_test_split) function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6G6WHh8ApAH"
   },
   "outputs": [],
   "source": [
    "# Generate a shuffled dataset\n",
    "shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size = 5000, seed = 50)\n",
    "\n",
    "# Skip the first 1,000 examples, putting the rest into the training set\n",
    "train_dataset = shuffled_dataset.skip(1000)\n",
    "\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_dataset = shuffled_dataset.take(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqHn_-DEyyH4"
   },
   "source": [
    "## **6.2: 🤗 Tokenizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMj8nCRaDYKF"
   },
   "source": [
    "$\\textbf{Section Introduction:}$\n",
    "\n",
    "**Source:** [The 🤗 Tokenizers Library](https://huggingface.co/learn/nlp-course/chapter6/1?fw=pt)\n",
    "\n",
    "This section will be dedicated to everything tokenization. First, we'll start out with an introduction to the WordPiece tokenization algorithm, a popular tokenization algorithm employed in many BERT-based Transformer models, such as DistilBERT. Having developed a basic understanding of WordPiece and tokenization algorithms in general, we'll move on to discussing how the 🤗 Tokenizers library puts these tokenization algorithms into practice via its integration with the 🤗 Datasets and (soon-to-be-introduced) 🤗 Transformers libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj0Nkkuw32yL"
   },
   "source": [
    "### **6.2.1: WordPiece tokenization algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdZ747j2EH_D"
   },
   "source": [
    "$\\textbf{Introduction:}$\n",
    "\n",
    "**Source:** [WordPiece Tokenization](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt#wordpiece-tokenization)\n",
    "\n",
    "Originally developed by Google for pretraining BERT, the WordPiece tokenization algorithm is a popular tokenization algorithm which has been applied to a variety of BERT-based Transformer models.\n",
    "\n",
    "We'll spend the first portion of this section explaining WordPiece from a theoretical perspective. In particular, we'll look at the \"training algorithm\" for generating a tokenized vocabulary using WordPiece before discussing how pre-tokenized words are actually tokenized using the generated vocabulary.\n",
    "\n",
    ">**Note:** We'll talk about pre-tokenization in more depth in Section 6.2.3, but the basic gist of this process is taking a sentence and (a) converting it to a list of individual words (e.g., for `sentence = 'This is a sentence.'`, `sentence` becomes `['This', 'is', 'a', 'sentence', '.']` and (b) generating offset mappings for each pre-token (e.g., in one type of offset mapping, the offset mapping for `This` would be `(0,3)`, indicating that `sentence[0]` is the first character in `This` while `sentence[3]` indexes the last character).\n",
    "\n",
    "Unlike with training a deep neural network, tokenization training is a deterministic process. Note that WordPiece is an example of a **[subword tokenization algorithm](https://huggingface.co/docs/transformers/en/tokenizer_summary#subword-tokenization)**; subword tokenization algorithms have been demonstrated to be much more effective than full-word tokenization algorithms, and are in general the industry standard.\n",
    "\n",
    "Finally, we'll actually code up WordPiece using a small corpus of text & a small final vocabulary size (i.e., not a practical application of WordPiece, but a demonstrative one).\n",
    "\n",
    "---\n",
    "*Note: Google technically never open-sourced its implementation of the training algorithm of WordPiece. What follows is an educated guess of the algorithm developed by 🤗 based on published literature; as such, some of the finer details of the algorithm are likely missing in the demonstration to come.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6frdjEfDIZjn"
   },
   "source": [
    "$\\textbf{WordPiece training algorithm:}$\n",
    "\n",
    "First, WordPiece starts off with a small vocabulary containing all of the model's special tokens and the initial WordPiece alphabet. This alphabet is generated by splitting each word into each of it's characters, appending the appropriate prefix (`##` for BERT) to every character except the first in each word. For example, `word` would be tokenized as such:\n",
    "\n",
    "<pre>w ##o ##r ##d\n",
    "</pre>\n",
    "\n",
    "From here, WordPiece learns certain merge rules in order to build up the WordPiece vocabulary to its desired size (e.g., `##r` and `##d` might be merged into `##rd`, expanding the alphabet above to 5 tokens instead of 4). In particular, WordPiece calculates the following score for each pair of tokens, applying merges at each iteration of the training algorithm to the pair with the highest score:\n",
    "\n",
    "$$score = \\frac{freq\\_of\\_pair}{freq\\_of\\_first\\_element \\; \\times \\; freq\\_of\\_second\\_element}$$\n",
    "\n",
    "Instead of just merging the most frequent pairs, WordPiece specifically applies merges to tokens whose individual parts are less frequent in the vocabulary.\n",
    "\n",
    "Thus the WordPiece training algorithm essentially constitutes taking a large corpus of text, generating the initial vocabulary by adding any special tokens to the initial alphabet generated from all of the words in the large corpus of text, and engaging in the iterative process of generating scores for each pair of vocab words & applying a merge rule to the pair with the highest score until the WordPiece vocabulary has been built up to the desired size. The final generated vocabulary is the output of the WordPiece training algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02Hbq4lhLWQG"
   },
   "source": [
    "$\\textbf{WordPiece tokenization algorithm:}$\n",
    "\n",
    "Having generated our token vocabulary during the training portion of WordPiece, we can now finally tokenize our input text. Starting from the first character of the pre-tokenized word we want to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it.\n",
    "\n",
    "For example, suppose we are trying to tokenize the word `'bugs'` with the following vocabulary:\n",
    "\n",
    "<pre>Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\"]\n",
    "</pre>\n",
    "\n",
    "`'b'` is the longest subword starting at the beginning of this word, so WordPiece first tokenizes `'bugs'` as `['b', '##ugs']`. Then `'##u'` is the longest subword starting at the beginning of `'##ugs'` that is in the vocabulary, so we then tokenize this word as `['b', '##u', '##gs']`. Finally, `'##gs'` is the longest subword starting at the beginning of `'##gs'`, so our final tokenization of the word `'bugs'` ends up being the last list: `['b', '##u', '##gs']`.\n",
    "\n",
    "Before we move on to actually implementing WordPiece, note that if the tokenization \"inference\" process gets to a stage where it’s not possible to find a subword in the vocabulary, then WordPiece tokenizes the whole word as unknown. For example, `'bum'` would get tokenized as `['[UNK]']` given the above vocabulary; even though we can start with `'b'` and `'##u'`, `'##m'` is not in our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07kZ-dH9O1Dy"
   },
   "source": [
    "$\\textbf{Implementing WordPiece:}$\n",
    "\n",
    "In this section, we'll actually implement WordPiece. First, let's start off with our corpus of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A5r69c2PVq9"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eVdfud-PiWq"
   },
   "source": [
    "Now, let's first compute the frequency of each word in this corpus as we apply the pre-tokenization process. Since WordPiece is popular in BERT-based Transformer models, we'll be using HF Transformers' [`bert-based-cased`](https://huggingface.co/google-bert/bert-base-cased) model for our pre-tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862,
     "referenced_widgets": [
      "c340bb8368de4c16bc05091e8124a10e",
      "4833e21c66ef4d77b100e5ba188b02e9",
      "0a61fc5b0d5f4926aa85580e5eb90efd",
      "25bbe896aa904617965f9df756fd8dbc",
      "17041c2d31c34b6885cd3b847ee89329",
      "e9701d9cefd643e7a3d1e05b9fa1a09b",
      "4d33807e6e344eacbae52ab87a1357cd",
      "ddf0d4ff2c244195a8b9167d44f4ee35",
      "eec5d63c812e4b39bd62eb6997dac0d1",
      "0a4a91c3de334d8cbcebd8e002428eb2",
      "e54843a658f84384b0fedabdedb9304b",
      "3f190ea8f4af487e8a812cf9138baa8a",
      "89d2fad9d1f644febf924d09390c9fe7",
      "d811f33ac6b849f08f7cd32cef9fac6c",
      "534841ff61d945fa83fa8638fe652018",
      "66dc835ff00940d0bbc28c562084f92d",
      "01ac18b8d7774c34936beda5f61b9fca",
      "09986645fea8406d88954409e331bf19",
      "5520a824919e43ecaf6e2747cc9adb1c",
      "42a58baca2f84cb5b32cc706a0838e1c",
      "75a413fcc058490d9d86d71dacdb2e0d",
      "9b837f2296b0461b84712066a7d8a176",
      "485eaa9c2b3f4244bcccdd7ed6792af7",
      "90233d1ccc79463d9285290137c32be8",
      "a30a2353dbb0450c974b784a235d753f",
      "8100bfb8bca847fdb78ff74c6d96d163",
      "4797eb5cbe5e44b8864d0ab049c10cbf",
      "06e2150e58264d67b67a2f753471d9d4",
      "0b8f3a44091245c5b9d8affba3c934c1",
      "41f76e8ebe32497887c160dcdac4c5d5",
      "e32ab8b52e4e404b838e66714c9a688c",
      "e7745a07534d4528a97a870d1892c1df",
      "9d85c6ed72be4217ae2c6fa45319425a",
      "240b042b819f46d8a987075bc19feb56",
      "7f80d88d99ac4026bcf8fa427121a85d",
      "5616574cb56f4b119679b2b67d6eab28",
      "50864d1d14194e7b95131fe04b28faad",
      "64f172f7a5c84aaa8b1bd6a9d4ea0f5e",
      "ec34b0c354154af699efbe08113bf455",
      "039492ffa401491fa261ab07d9232426",
      "a5a434f164ef4a58bac4567c39b4a837",
      "06644e131f9843d6b2bc1e20777491d1",
      "ddb7aa53ce8d4943a56ce84b50232f20",
      "c1213b9168c840158d050e58cbe8ffea"
     ]
    },
    "id": "orSLofI3PX0A",
    "outputId": "49f1cc4c-5a8f-4753-caef-7e28da9d24fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c340bb8368de4c16bc05091e8124a10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f190ea8f4af487e8a812cf9138baa8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485eaa9c2b3f4244bcccdd7ed6792af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240b042b819f46d8a987075bc19feb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'Course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# Loading in our tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Computing the frequencies of each word (& character) in our corpus of text\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "  # Pre-tokenize each sentence\n",
    "  words = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "  new_words = [word for word, offset in words]\n",
    "  for word in new_words:\n",
    "    word_freqs[word] += 1\n",
    "\n",
    "# Final result\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwy6e9EIQSM0"
   },
   "source": [
    "Now, let's generate our initial vocbaulary. Our initial vocabulary is composed of our initial alphabet as well as all of the special tokens utilized by the BERT Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y17vN4NXQb0n",
    "outputId": "641007bf-0bb7-4635-ab8a-6b91471fd706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "  # Take the first character of each word in word_freqs.keys()\n",
    "  if word[0] not in alphabet:\n",
    "    alphabet.append(word[0])\n",
    "\n",
    "  # Iterate through the rest of the characters in each word contained in word_freqs.keys() to append\n",
    "  # the necessary WordPiece prefix\n",
    "  for letter in word[1:]:\n",
    "    if f\"##{letter}\" not in alphabet:\n",
    "      alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "\n",
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7roF6MsRjTd"
   },
   "source": [
    "Next, we split each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9ct1fd9Rki8",
    "outputId": "70d70375-27ac-4f64-ff56-c262fa0e9e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', '##h', '##i', '##s'],\n",
       " 'is': ['i', '##s'],\n",
       " 'the': ['t', '##h', '##e'],\n",
       " 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n",
       " 'Face': ['F', '##a', '##c', '##e'],\n",
       " 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n",
       " '.': ['.'],\n",
       " 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n",
       " 'about': ['a', '##b', '##o', '##u', '##t'],\n",
       " 'tokenization': ['t',\n",
       "  '##o',\n",
       "  '##k',\n",
       "  '##e',\n",
       "  '##n',\n",
       "  '##i',\n",
       "  '##z',\n",
       "  '##a',\n",
       "  '##t',\n",
       "  '##i',\n",
       "  '##o',\n",
       "  '##n'],\n",
       " 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n",
       " 'shows': ['s', '##h', '##o', '##w', '##s'],\n",
       " 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n",
       " 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n",
       " 'algorithms': ['a',\n",
       "  '##l',\n",
       "  '##g',\n",
       "  '##o',\n",
       "  '##r',\n",
       "  '##i',\n",
       "  '##t',\n",
       "  '##h',\n",
       "  '##m',\n",
       "  '##s'],\n",
       " 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n",
       " ',': [','],\n",
       " 'you': ['y', '##o', '##u'],\n",
       " 'will': ['w', '##i', '##l', '##l'],\n",
       " 'be': ['b', '##e'],\n",
       " 'able': ['a', '##b', '##l', '##e'],\n",
       " 'to': ['t', '##o'],\n",
       " 'understand': ['u',\n",
       "  '##n',\n",
       "  '##d',\n",
       "  '##e',\n",
       "  '##r',\n",
       "  '##s',\n",
       "  '##t',\n",
       "  '##a',\n",
       "  '##n',\n",
       "  '##d'],\n",
       " 'how': ['h', '##o', '##w'],\n",
       " 'they': ['t', '##h', '##e', '##y'],\n",
       " 'are': ['a', '##r', '##e'],\n",
       " 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n",
       " 'and': ['a', '##n', '##d'],\n",
       " 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n",
       " 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V2yUgBYRndp"
   },
   "source": [
    "Now, we're ready for training. Below, we'll write two functions:\n",
    "\n",
    "1.   `compute_pair_scores()` can be used to compute the score for each pair of subwords given the `splits` dictionary we generated above.\n",
    "2.   `merge_pair()` can be used to apply the appropriate merge rule to two subwords given the two subwords and the `splits` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxyxKS7RSUX9"
   },
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "  letter_freqs = defaultdict(int)\n",
    "  pair_freqs = defaultdict(int)\n",
    "\n",
    "  for word, freq in word_freqs.items():\n",
    "    split = splits[word]\n",
    "\n",
    "    # If the word only contains a single character, we only need to update letter_freqs:\n",
    "    if len(split) == 1:\n",
    "        letter_freqs[split[0]] += freq\n",
    "        continue\n",
    "\n",
    "    # Otherwise, we'll iterate through each pair of subsequent characters to update both pair_freqs & letter_freqs:\n",
    "    for i in range(len(split) - 1):\n",
    "        pair = (split[i], split[i + 1])\n",
    "        letter_freqs[split[i]] += freq\n",
    "        pair_freqs[pair] += freq\n",
    "    letter_freqs[split[-1]] += freq     # The for loop above doesn't capture the letter frequency of the last character in each multi-character word.\n",
    "\n",
    "  # Compute WordPiece algorithm scores\n",
    "  scores = {\n",
    "      pair : freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "      for pair, freq in pair_freqs.items()\n",
    "  }\n",
    "\n",
    "  return scores\n",
    "\n",
    "def merge_pair(a, b, splits):\n",
    "  for word in word_freqs:\n",
    "    split = splits[word]\n",
    "\n",
    "    # No merge rule can be applied to a single-character word\n",
    "    if len(split) == 1:\n",
    "      continue\n",
    "\n",
    "    # Iterate through every multi-character word\n",
    "    i = 0\n",
    "    while i < len(split) - 1:\n",
    "      if split[i] == a and split[i + 1] == b:\n",
    "        merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "        split = split[:i] + [merge] + split[i + 2:]     # Replace the subwords a & b with the merged word merge in the relevant key of our splits dictionary\n",
    "      else:\n",
    "        i += 1\n",
    "    splits[word] = split\n",
    "  return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OphKh7kxT7C_"
   },
   "source": [
    "Now, we're ready to write our tokenization training loop. We're going to aim for a vocabulary size of 70:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcJXKSCHUAry",
    "outputId": "3f3e4673-992b-4bed-c8b9-301fd4d9c29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut', '##ta']\n"
     ]
    }
   ],
   "source": [
    "# Training algorithm\n",
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "\n",
    "  # Compute each score for each split in our splits dictionary & capture the pair (\"best_pair\") with the highest associated score (\"max_score\")\n",
    "  scores = compute_pair_scores(splits)\n",
    "  best_pair, max_score = \"\", None\n",
    "  for pair, score in scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "      best_pair = pair\n",
    "      max_score = score\n",
    "\n",
    "  # Update our splits dictionary\n",
    "  splits = merge_pair(*best_pair, splits)\n",
    "\n",
    "  # We don't return the new token in our merge_pair() function, so we extract it from best_pair here:\n",
    "  new_token = (\n",
    "      best_pair[0] + best_pair[1][2:]\n",
    "      if best_pair[1].startswith(\"##\")\n",
    "      else best_pair[0] + best_pair[1]\n",
    "  )\n",
    "\n",
    "  vocab.append(new_token)\n",
    "\n",
    "# Final result\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8a7CqQoU5qO"
   },
   "source": [
    "Having finished and applied our WordPiece training algorithm, we can now write up our WordPiece tokenization algorithm.\n",
    "\n",
    "In particular, we'll write up 2 functions:\n",
    "\n",
    "1.   Given a word as input (and, implicitly, our vocabulary in `vocab`), `encode_word()` will tokenize this word.\n",
    "2.   Given an input text (e.g., a sentence), `tokenize_text()` will tokenize the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BfR8680fWbSW",
    "outputId": "21d68336-a991-47f6-e97e-c994be858317"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " '##i',\n",
       " '##s',\n",
       " 'is',\n",
       " 'th',\n",
       " '##e',\n",
       " 'Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'Fac',\n",
       " '##e',\n",
       " 'c',\n",
       " '##o',\n",
       " '##u',\n",
       " '##r',\n",
       " '##s',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions\n",
    "def encode_word(word):\n",
    "  tokens = []\n",
    "  while len(word) > 0:\n",
    "    i = len(word)\n",
    "    while i > 0 and word[:i] not in vocab:\n",
    "      i -= 1\n",
    "    if i == 0:\n",
    "      return [\"[UNK]\"]\n",
    "\n",
    "    tokens.append(word[:i])\n",
    "    word = word[i:]\n",
    "    if len(word) > 0:\n",
    "      word = f\"##{word}\"\n",
    "\n",
    "  return tokens\n",
    "\n",
    "def tokenize_text(text):\n",
    "  pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "  pre_tokenized_text = [word for word, offset in pre_tokenize_result]     # Remove offsets from our pre-tokenized text, since we don't require them\n",
    "  encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "\n",
    "  return sum(encoded_words, [])\n",
    "\n",
    "# Let's try out our WordPiece tokenization algorithm on an example piece of text:\n",
    "tokenize_text(\"This is the Hugging Face course!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdWg_FCr4KoQ"
   },
   "source": [
    "### **6.2.2: Understanding fast tokenizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMhqkykevmLY"
   },
   "source": [
    "$\\textbf{Introduction:}$\n",
    "\n",
    "**Source:** [Fast tokenizers’ special powers](https://huggingface.co/learn/nlp-course/chapter6/3?fw=pt)\n",
    "\n",
    "🤗 provides access to two types of tokenizers: \"slow\" tokenizers -- tokenizers written in Python in the HF Transformers library -- and \"fast\" tokenizers -- tokenizers written in Rust which are provided through the HF Tokenizers library.\n",
    "\n",
    "The performance difference between \"slow\" and \"fast\" tokenizers is oftentimes substantial. For example, going back to the Drug Review Dataset introduced in Section 6.1.2, HF researchers recorded the following times taken when tokenizing the entire aforementioned dataset:\n",
    "\n",
    "\\begin{array}{ccc}\n",
    "  & \\textbf{Fast tokenizer} & \\textbf{Slow tokenizer} \\\\\n",
    "  \\hline\n",
    "  \\scriptsize{batched = True} & \\small{10.8s} & \\small{4min \\; 41s} \\\\\n",
    "  \\scriptsize{batched = False} & \\small{59.2s} & \\small{5min \\; 3s}\n",
    "\\end{array}\n",
    "\n",
    "Beyond speed benefits (which often only come into play when tokenizing or otherwise performing operations on tons of text in parallel), \"fast\" tokenizers provide a variety of additional functionality which proves useful when pre-processing datasets and training LLMs for various NLP tasks.\n",
    "\n",
    "In this section, we'll do a brief tour of `BatchEncoding` objects and the variety of functionality that \"fast\" tokenizers backed by the 🤗 Tokenizers library provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8QF3AOJycQb"
   },
   "source": [
    "$\\textbf{What are BatchEncoding objects?:}$\n",
    "\n",
    "Instead of a simple Python dictionary object, the output of a \"fast\" tokenizer is a `BatchEncoding` object. A `BatchEncoding` object is a subclass of a dictionary which provides access to a variety of additional methods which \"fast\" tokenizers can take advantage of; you can find an exhaustive list of these various methods [here](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/tokenizer#transformers.BatchEncoding).\n",
    "\n",
    "Beyond parallelization, the key functionality of fast tokenizers is their generated *offset mappings* -- mappings between the original span of text and the tokens generated from them. Going beyond the basic example given in Section 6.2.1, \"offset mappings\" refers to a whole class of mappings, including:\n",
    "\n",
    "*   mappings from each word to the token it generated, and vice versa; or\n",
    "*   mappings from each character of the input text to the token it's inside, and vice versa.\n",
    "\n",
    "Now, we'll take some time to look at a brief example in order to explore all of these different mappings and methods accessible to \"fast\" tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344,
     "referenced_widgets": [
      "434e4fda170b4ba9a9e14ec53f61fe99",
      "8b01aeea165b4f5092befde9989b1c56",
      "e21dd2d2aff34deca884dfbfca79a7c9",
      "1196139b4b89423f97793d9f1f2bb05e",
      "ae5e22e7dfee45268ff8cf59a4f1a2ec",
      "e0638426c0aa4bf5865e5a1829d25185",
      "163a39bb22674315b204c3d2a162a41e",
      "343770b1af0c4a459faa6d7986ff1cbd",
      "920b77f4a07a4f8e84b7d6c86376577b",
      "91193ef51ae24dbba36b728d95f467ba",
      "ccd4e0583dcf46a790d737e9334f3a87",
      "6ad679a7c47b4a8b9514d7bb6c24c844",
      "ec37fa5f86b04fae828e8be5d0ac5464",
      "9ef42d2728f548148c3d99cb5a4b6f70",
      "7721c4ef4e4f494baa6309fb3644f0ab",
      "054f3548655946d49141216bc0f6f582",
      "7d034f40ba37490c83d31f319a3e6c62",
      "f01a38546421480a8b394ef45cece28f",
      "c63672ce4ea34dc9bce5260d3afceede",
      "b3a5269da4564ae08d63da81042fdf55",
      "3907f393c47e43eba7485a8ad5247749",
      "d1eb0702795149c6872864f78663e41b",
      "2a58265a2f394027969c5ce1547e6dab",
      "81feced71fcc4c91819201dc87235ed6",
      "59951e42eaa04dd89072b4d16b933161",
      "7398f357b3924154bedf843ce4df9590",
      "4da62ee7b1a94869ac7ebc73ef66164c",
      "c2aed26a7b57417f947407f96fa65ce0",
      "e71ad00026bc43edbe40d8ec57d8e14a",
      "2f9b753d471a46d58746b50182f441f3",
      "83cb915a9a544d8eae25f756fb2e7f67",
      "214e39c8810d40a49cf74d92a4cd51af",
      "d16071d5093746b3b2d209e22e5e141d",
      "e112ad0a6771430d96679ddcda76ae27",
      "992829b131b54a44b434eeb987bf819c",
      "55bb965847bb40cf8fe34369dffb445b",
      "a1b76ac915714de8a51d22d38672465f",
      "eaf1c6af302349159979055648497909",
      "3c11da15415e44cb8d7c03ca24bd506f",
      "52f39ad672c44385a3f4ba76114e1cd6",
      "d8837afb6d0c44b2bebfe79569f679d3",
      "b4998e6c7f77406ebadd044d3e148306",
      "bbdfa1b23403452da4a8d479d7f782a9",
      "690e3285954643f1b7488ce6f3ec0e2b"
     ]
    },
    "id": "x6nmKPLGBcpy",
    "outputId": "3844fab0-6d19-4837-9e35-860900796b05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434e4fda170b4ba9a9e14ec53f61fe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad679a7c47b4a8b9514d7bb6c24c844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a58265a2f394027969c5ce1547e6dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e112ad0a6771430d96679ddcda76ae27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 6010, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Instantiating the tokenizer used in the bert-base-cased model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Example text\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "\n",
    "# Generating tokens\n",
    "encoding = tokenizer(example)\n",
    "\n",
    "# Outputs\n",
    "print(encoding)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q05UAqA1B3oh"
   },
   "source": [
    "First, and foremost, let's explore the output of our tokenizer. We see the following outputs generated:\n",
    "\n",
    "1.   `input_ids`: These are the actual tokens generated from the input text provided to the tokenizer.\n",
    "2.   `token_type_ids`: These IDs are only relevant for certain NLP tasks. For example, if one wanted to supervised fine-tune a model for generative question-answering, the input sequences provided to the model's tokenizer would contain both the question and desired answer. In this case, the `token_type_ids` would be used to differentiate between tokens belonging to the question text input (e.g., `0`) and the answer text input (e.g., `1`).\n",
    "3.   `attention_mask`: As the name suggests, the value associated with this `BatchEncoding` key is the attention mask applied to the provided input sequence. Our model will attend to tokens whose `attention_mask` value is `1`, whereas our model won't attend to tokens assigned a value of `0`.\n",
    "\n",
    "Note that the following output format isn't necessarily standard (excluding the `input_ids`, which are always generated). Certain models expect more or less ouputs from the tokenizer, so make sure that when you are instantiating a tokenizer using `AutoTokenizer.from_pretrained()`, the checkpoint you provide as input is the same as the checkpoint used to generate a model using `AutoModel.from_pretrained()` (we'll cover the latter function in Section 6.3).\n",
    "\n",
    "Second, notice that our encodings are a `BatchEncoding` object, not a regular `dict` object (although the representations of both types of objects are quite similar).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmmcH4wZEhX3"
   },
   "source": [
    "As a quick note, it's always best to check the type of your instantiated tokenizer before attempting to acess any methods only available to \"fast\" tokenizers. In the case of the code above, we could check the type of the tokenizer by checking the tokenizer directly:\n",
    "\n",
    "<pre><code class=\"python\"><b>Input:</b>\n",
    "tokenizer.is_fast\n",
    "\n",
    "<b>Output:</b>\n",
    "True\n",
    "</code></pre>\n",
    "\n",
    "or by checking the tokenizer's output, as follows:\n",
    "\n",
    "<pre><code class=\"python\"><b>Input:</b>\n",
    "encoding.is_fast\n",
    "\n",
    "<b>Output:</b>\n",
    "True\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjLQeggeFRwe"
   },
   "source": [
    "$\\textbf{Important Methods of \"Fast\" Tokenizers}$\n",
    "\n",
    "Now, let's explore some of the various methods accessible for use with a \"fast\" tokenizer. We list descriptions of the important methods in this text box, and then show their outputs below:\n",
    "\n",
    "*   `tokens()`: This method returns the list of tokens at a given batch index; if only a single batch is provided, you don't need to specify any batch index. The list of tokens generated are the sub-parts of the input string generated after word/subword splitting but before conversion to integer indices (e.g., the final output of the WordPiece tokenization algorithm deployed in Section 6.2.1).\n",
    "\n",
    "*   `word_ids()`: A list indicating the word corresponding to each token. Special tokens added by the tokenizer are mapped to `None` while other tokens are mapped to the index of their corresponding word; in relevant cases, several tokens will be mapped to the same word index.\n",
    "\n",
    "*   `sequence_ids()`: A method which can be used to map a token to the sentence it came from. In our case, the `token_type_ids` returned by the tokenizer provides the same information.\n",
    "\n",
    "*   `word_to_chars()`: Give as input `batch_index` (if necessary) and `word_index`. To understand what `word_index` is, imagine for the sake of simplicitly that you've provided only a single input sequence to the tokenizer. Now, imagine generating a list from this input sequence of text where each list entry is a single word (or punctuation mark) from this input text; `word_index` is an integer you could use to index a single entry from this list. The output to this method is a `CharSpan` `NamedTuple` object of format: `(<index_of_first_character_in_original_string>, <index_of_last_character_in_original_string>)`. See the example below for further clarification.\n",
    "\n",
    "*   `token_to_chars()`: Given as input `batch_index` (if necessary) and `token_index` (i.e., given the list generated by the `tokens()` method, `token_index` as in `tokens_list[token_index]`), this method returns the span of characters in the original string, or None, if the token doesn’t correspond to any characters in the original string. In particular, the output is a `CharSpan` object of the same `(<start>, <end>)` format as the output to the `word_to_chars()` method.\n",
    "\n",
    "*   `char_to_word()`: Given as input `batch_index` (if necessary) and `char_index` (the index of a character in the original string), this method returns the index or indices of the associated encoded token(s). For example, suppose you provide as input the index associated with the character 'y' as in 'Sylvian' in the original string (as is done below). Now, pre-tokenize your string via the `backend_tokenizer.pre_tokenizer.pre_tokenize_str()` method; if necessary, remove any offsets or miscellaneous information returned so that you only have a list of strings. If you index this list by the integer returned by `char_to_word()`, you'll get the word 'Sylvian' (and hence \"character to word\").\n",
    "\n",
    "*   `char_to_token()`: Give as input `batch_index` (if necessary) and `char_index` (the index of a character in the original string). To understand the output, entertain the following example. Suppose you give as input the index associated with the character 'v' as in 'Sylvian' in the original string (as is done below). If you take the integer output and use it to index the list generated by the `tokens()` method, you'll get the token that character whose index you provided to this method is associated with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NX7Vc9AKssu",
    "outputId": "102b7fe1-b902-4a8c-91f2-09e8acf5e0f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtokens():\u001b[0m  ['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n",
      "\u001b[1mword_ids():\u001b[0m  [None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n",
      "\u001b[1msequence_ids():\u001b[0m  [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None]\n",
      "\u001b[1mword_to_chars():\u001b[0m  Sylvain\n",
      "\u001b[1mtoken_to_chars():\u001b[0m  work\n",
      "\u001b[1mchar_to_word():\u001b[0m \n",
      " \t \u001b[1mCharacter:\u001b[0m  y \n",
      " \t (Not in output:) as in 'Sylvian'. \n",
      "\n",
      " \t \u001b[1mIndex:\u001b[0m  3\n",
      "\n",
      "\t \u001b[1mpre_tokenization (pre-tokenized example):\n",
      "\t\u001b[0m [('My', (0, 2)), ('name', (3, 7)), ('is', (8, 10)), ('Sylvain', (11, 18)), ('and', (19, 22)), ('I', (23, 24)), ('work', (25, 29)), ('at', (30, 32)), ('Hugging', (33, 40)), ('Face', (41, 45)), ('in', (46, 48)), ('Brooklyn', (49, 57)), ('.', (57, 58))]\n",
      "\n",
      "\t \u001b[1mpre_tokenization_modified:\n",
      "\t\u001b[0m ['My', 'name', 'is', 'Sylvain', 'and', 'I', 'work', 'at', 'Hugging', 'Face', 'in', 'Brooklyn', '.']\n",
      "\n",
      "\t \u001b[1mIndexing pre_tokenization_modified By Output:\u001b[0m  Sylvain\n",
      "\u001b[1mchar_to_token():\u001b[0m \n",
      " \t \u001b[1mCharacter:\u001b[0m  v \n",
      "\t (Not in output:) as in 'Sylvian'. \n",
      "\n",
      "\t \u001b[1mOutput:\u001b[0m 6 \n",
      "\n",
      "\t \u001b[1mIndexing encoding.tokens() By Output:\u001b[0m  ##va\n"
     ]
    }
   ],
   "source": [
    "# tokens()\n",
    "print(\"\\033[1mtokens():\\033[0m \", encoding.tokens())\n",
    "\n",
    "# word_ids()\n",
    "print(\"\\033[1mword_ids():\\033[0m \", encoding.word_ids())\n",
    "\n",
    "# sequence_ids()\n",
    "print(\"\\033[1msequence_ids():\\033[0m \", encoding.sequence_ids())\n",
    "\n",
    "# word_to_chars()\n",
    "start, end = encoding.word_to_chars(3)\n",
    "print(\"\\033[1mword_to_chars():\\033[0m \", example[start:end])\n",
    "\n",
    "# token_to_chars()\n",
    "start, end = encoding.token_to_chars(10)\n",
    "print(\"\\033[1mtoken_to_chars():\\033[0m \", example[start:end])\n",
    "\n",
    "# char_to_word()\n",
    "print(\"\\033[1mchar_to_word():\\033[0m \\n \\t \\033[1mCharacter:\\033[0m \", example[12], \"\\n \\t (Not in output:) as in 'Sylvian'. \\n\\n \\t \\033[1mIndex:\\033[0m \", encoding.char_to_word(12))\n",
    "\n",
    "pre_tokenization = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(example)\n",
    "pre_tokenization_modified = [input[0] for input in pre_tokenization]\n",
    "\n",
    "print(\"\\n\\t \\033[1mpre_tokenization (pre-tokenized example):\\n\\t\\033[0m\", pre_tokenization)\n",
    "print(\"\\n\\t \\033[1mpre_tokenization_modified:\\n\\t\\033[0m\", pre_tokenization_modified)\n",
    "print(\"\\n\\t \\033[1mIndexing pre_tokenization_modified By Output:\\033[0m \", pre_tokenization_modified[encoding.char_to_word(12)])\n",
    "\n",
    "# char_to_token()\n",
    "print(\"\\033[1mchar_to_token():\\033[0m \\n \\t \\033[1mCharacter:\\033[0m \", example[14], \"\\n\\t (Not in output:) as in 'Sylvian'. \\n\\n\\t \\033[1mOutput:\\033[0m\", encoding.char_to_token(14),\"\\n\\n\\t \\033[1mIndexing encoding.tokens() By Output:\\033[0m \", encoding.tokens()[encoding.char_to_token(14)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mF7NbIqw4Sny"
   },
   "source": [
    "### **6.2.3: Walk-through of tokenization process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORnCMfRYk_Ik"
   },
   "source": [
    "$\\textbf{Introduction:}$\n",
    "\n",
    "**Source:** [Building a tokenizer, block by block](https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt)\n",
    "\n",
    "Overall, the entire process of tokenization is composed of the following steps:\n",
    "\n",
    "*   **Normalization:** During this step, we conduct any necessary cleanup, including removing spaces or accents, Unicode normalization, and the such. This step varies depending upon what type of input the particular model you're working with expects. We've haven't mentioned this process up until now, but as you can see, it's pretty self-explanatory.\n",
    "*   **Pre-Tokenization:** Splitting the input into a list of words (and punctuation marks).\n",
    "*   **Tokenization:** Running your pre-tokens through the (already-trained) tokenizer to generate batches of sequences of tokens.\n",
    "*   **Post-Processing:** This process covers a variety of things, namely: adding special tokens to the tokenizer, and generating attention masks and token type IDs. We've seen bits of this step in what we've covered so far.\n",
    "\n",
    "In this section, we're going to build up a tokenizer from scratch. This section will work as a good review of what we've discussed so far, and function as a good place to mention any of the finer details surrounding the tokenization process that we've neglected up to this point.\n",
    "\n",
    "In order to construct our new tokenizer, we're going to use HF's `Tokenizer` class. This class contains the following submodules which we will use to build our new tokenizer:\n",
    "\n",
    "*   `normalizers` => Contains all possible types of `Normalizers` you can use. A complete list of all `Normalizers` can be found [here](https://huggingface.co/docs/tokenizers/api/normalizers).\n",
    "*   `pre_tokenizers` => Contains all the possible types of `PreTokenizer` you can use. A complete list can be found [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).\n",
    "*   `models` => Contains the various types of models you can use -- namely, `WordPiece`, which we'll be using. A complete list can be found [here](https://huggingface.co/docs/tokenizers/api/models).\n",
    "*   `trainers` => Contains all the different types of `Trainer` you can use to train your model on a corpus. A complete list can be found [here](https://huggingface.co/docs/tokenizers/api/trainers).\n",
    "*   `post_processors` => Contains the various types of `PostProcessor` you can use. A complete list can be found [here](https://huggingface.co/docs/tokenizers/api/post-processors).\n",
    "*   `decoders` => Contains the various types of `Decoder` you can use to decode the outputs of tokenization (convert a list/array/tensor of integer token indices into words). A complete list can be found [here](https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCaTlDx3op5l"
   },
   "source": [
    "$\\textbf{Generating our Corpus of Text:}$\n",
    "\n",
    "In order to train our tokenizer, we'll be utilizing the [WikiText-2](https://huggingface.co/datasets/Salesforce/wikitext) dataset. In order to work with this dataset, we need to first instantiate the dataset before writing a [generator](https://www.programiz.com/python-programming/generator) function which can be used to iterate through the entire dataset; in particular, our generator will yield 1000 text sequences at a time. For future reference, HF Tokenizers tokenizers can be trained on text files directly; still, the generator approach will for our purposes be easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "b70e921e993e4830b0aedef86cdfc0d7",
      "d6ce33e6dcce4e819be66a950c8c4c1d",
      "4ef45d2ed3c44e1c9760ab23de89a038",
      "7a61b3f758d4499ca72ffeccb7a70b26",
      "db358963f48a4288899ad1930cb2967c",
      "b0c206994e3348188b0aa08401a19a09",
      "89277bc8ea0f48d998eaa49fdda9dd7f",
      "423c7f39e4ca47ffb1b465ce39d88eab",
      "678f1fddb3394620bf90637ef4d25d9c",
      "9c6cc9fc242b49f5bb91340b8da77e2f",
      "ba3afcda287643cdaced9fe0eb35e8cb",
      "4693acb64c3b4e228e4c4ed8b772cc93",
      "042260a712e54a3eb38f00c42a994d5e",
      "7f2fd9d6257f4fb9bbdb0a87a9be32f1",
      "48d85096b58c4e4e81faf5644c93aad6",
      "2cde16c6d360491d8674533f68c5c85e",
      "3cda42c2775844cda73d359dcdedc20a",
      "50a22c2756a340ab8ed00e2f72456850",
      "b914a25b7c764f44a3de1aee02abf694",
      "25eeeba6b21945efb9d7b19b90043b9e",
      "fb7ad543dab643b28837816b4fc2b034",
      "d1b1d35df99846be9e866625f117d5a2",
      "f9b52e8fbbab4a13885fc558608ba5ba",
      "3c0beda0df3b4375a7e2a7b8603c36ad",
      "0766872b57a14ec4bb5316859aad2c5b",
      "85939cb993ec4ef1944cb81b6e09dff4",
      "8a4eb63f40dd49f59f914b5319b85086",
      "84ccb3f0a53a411fb04b14ca6e132b74",
      "ebd2d26b06e54f829376f845ddfa43f2",
      "4af173c4fcef47ceb0027e941642b92e",
      "7eb16027a0d04840adf095e5890603de",
      "a4f6381a08904bf3903a12ef1ba630e4",
      "a05814ad7f704efa8f5f4b6618603641",
      "13e47d5367dd468dbe441721bf49878e",
      "bd110b489eea456d8a73b58bf9b32a99",
      "a16258fcfcef4dab89a724ff0602dfc0",
      "de6a8fe620b54169ab9a7ab2ee9a19eb",
      "160a3daa38724e24958c03d223b92715",
      "10217de50c4746ffbf997802e19e0cad",
      "2f5f59a138874738a2c861b32bc85f9c",
      "629bf7b955a747868845ff14f8d13bb1",
      "c427df0b55ff47469e70a9fd0030445a",
      "ed5e56aed57241f0abdbb0bda00775a8",
      "bd40d8b38b0c4ae8a681c7c674e7096f",
      "8f5e352e4c074868b8ce76d877e2a371",
      "9550b80584bd49acb4c38012fc76ae9a",
      "4989ff1d16f944ab91f2604cc1bc25ca",
      "7341450f5259434980fa878091b45c37",
      "564deacee3754a8f87cd51c71c85a5c5",
      "3b3d5529b8ff4525a96940b7bb0827cc",
      "af36f4d12af74d988c14ee98b6a5efdb",
      "78d93a0414c94998b3c44ff809948275",
      "a9226658ccbd421fb0453111b9231d27",
      "b3c4cdd09f6f49f79347774767d5864a",
      "ab616db6fc2148748457b99c772a62dd",
      "0d7c78f67f85437da428d5633794a5f1",
      "12604e96d3db4d0b956f69e25931b88e",
      "4e6beca49bda462ba96cf97cdef5dda7",
      "e53ce1e1d9024d06a41624395ba1a02e",
      "0329dd1e1a72495eb395a687767bcd5b",
      "992423da68ac4dec9e303ee2289b495c",
      "2a993814a96f45cfb267c72667d62b45",
      "65d578e8666c4a7ea76668e4ae2a3785",
      "51a6b5f1167647b79051e18a4e374cc2",
      "7b774c116dac4507b56778fce5a63ccf",
      "b2fc4151915c4f1da4c43503bb8ee3c2",
      "acaeea97a25d4c48b012b16c093a541f",
      "c14c5065a9b74a2c9ac2b99edaf85b27",
      "0fa9b0913465492a8c22f49968ac705a",
      "d0fdce6e63fb444ab12cfafe8cbb9cfc",
      "a7058991c7f14b0cb1a83b57deb78b34",
      "0783cd39a8514340aaef7f1f697d1350",
      "a93eb32601e24264861fd678c5c1891e",
      "a98c390af18349288701b439ee0c0449",
      "b95929b0b65645c28dcd2c854149f2f3",
      "8afa55b867a24cc7b11b6c02d8ce1a2c",
      "d2bd0ecd772f445fb85dc03062097a89"
     ]
    },
    "id": "vZeuwpGC6kbs",
    "outputId": "ab82e809-5bcd-44b2-9859-cde511286b73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70e921e993e4830b0aedef86cdfc0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4693acb64c3b4e228e4c4ed8b772cc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b52e8fbbab4a13885fc558608ba5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e47d5367dd468dbe441721bf49878e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5e352e4c074868b8ce76d877e2a371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7c78f67f85437da428d5633794a5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acaeea97a25d4c48b012b16c093a541f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load in dataset\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split = \"train\")\n",
    "\n",
    "# Generator\n",
    "def get_training_corpus():\n",
    "  for i in range(0, len(dataset), 1000):\n",
    "    yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0a2Ygkf69Wd"
   },
   "source": [
    "$\\textbf{Building a WordPiece tokenizer from scratch:}$\n",
    "\n",
    "In line with the tokenization algorithm we demonstrated back in Section 6.2.1, here we practically implement a WordPiece tokenizer from scratch. In particular, we'll be building up the WordPiece tokenizer utilized in the BERT Transformer model.\n",
    "\n",
    "Our general process will involve instantiating a `Tokenizer` object with a `model`, before setting its `normalizer`, `pre_tokenizer`, `post_processor`, and `decoder` attributes to the desired values.\n",
    "\n",
    "---\n",
    "\n",
    "*Note: When we instantiate our `Tokenizer` object with our `model`, we need to specify the `unk_token` so the model knows what to return when it encounters characters it hasn’t seen before.*\n",
    "\n",
    "*Some additional arguments we could've passed to the model include:*\n",
    "*   *`vocab` - Pass in the vocabulary of our model. Since we’re going to train the model, we don’t need to set this. For future reference, you can call the `get_vocabulary()` method on a 🤗 Tokenizers tokenizer (such as one instantiated via the `load_pretrained()` method) to extract a tokenizer's vocabulary.*\n",
    "*   *`max_input_chars_per_word` - This argument can be used to specify a maximum length for each word; words longer than this value are split.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e063yJEV8c_H"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRRV5pgV8fGz"
   },
   "outputs": [],
   "source": [
    "# Instantiating Tokenizer object with model\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dR7KA9sE9o8O"
   },
   "source": [
    "Now, let's replicate a BERT tokenizer (in particular, the tokenizer utilized in the training of the `bert-base-uncased` model). Composing multiple normalizers via `normalizers.Sequence()`, we'll pass in the following normalizers: `NFD`, `Lowercase`, and `StripAccents`.\n",
    "\n",
    "For reference, `normalizers.NFD()` is a NFD Unicode normalizer; without this normalizer, `normalizers.StripAccents()` won’t be able to properly recognize the accented characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQ4ZXVeH9okD",
    "outputId": "b3b06411-5591-41f9-c145-bcbff2725159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest String Output:\u001b[0m  hello how are u?\n"
     ]
    }
   ],
   "source": [
    "# Adding a normalizer module to our tokenizer\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "\n",
    "# Testing our normalizer by calling the normalize_str() method of the normalizer module:\n",
    "print(\"\\033[1mTest String Output:\\033[0m \", tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TadwSVDl-jgQ"
   },
   "source": [
    "Next, we build our BERT `pre_tokenizer` module from scratch. We use the `pre_tokenizer.WhitespaceSplit()` and `pre_tokenizer.Punctuation()` objects in sequence to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8Zf5kJk-2iY",
    "outputId": "97e595a3-8347-4b7d-e7a5-016ec28b07ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest String Output:\u001b[0m  [('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)), ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]\n"
     ]
    }
   ],
   "source": [
    "# Adding a pre-tokenizer module to our tokenizer\n",
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "# Test our pre-tokenizer by calling the pre_tokenize_str() method of our tokenizer's pre_tokenizer module:\n",
    "print(\"\\033[1mTest String Output:\\033[0m \", tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8HA9EDM_Y0g"
   },
   "source": [
    "Before we can train our tokenizer, we need to instantiate our tokenization trainer. In our particular case, we will be using `WordPieceTrainer`. The main thing to remember when instantiating a HF Tokenizers `Trainer` is to pass in all the special tokens you intend to use -- otherwise it won’t add them to the vocabulary, since they are not in the training corpus.\n",
    "\n",
    "---\n",
    "\n",
    "*Note: Beyond `vocab_size` and `special_tokens`, we could have also passed in the following arguments:*\n",
    "*   *`min_frequency` - Set the minimum number of times a token must appear to be included in the vocabulary.*\n",
    "*   *`continuing_subword_prefix` - If we want to utilize something different from `##`, we can set the desired string here.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHH6E9qd_Yc_"
   },
   "outputs": [],
   "source": [
    "# Loading in our tokenizer's Trainer\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LwUoYLHAWA_"
   },
   "source": [
    "With our `Trainer` object instantiated, we can train our `tokenizer` by simply calling the `train_from_iterator()` method, passing in our generator function as well as our `Trainer` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeYq3o9x_5vs"
   },
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naZVtuAyA0aT"
   },
   "source": [
    "If we wanted to, we could now test our tokenizer by calling its `encode()` method. For example:\n",
    "\n",
    "<pre><code class=\"python\"><b>Input: </b>\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)\n",
    "</code></pre>\n",
    "\n",
    "<pre><code class=\"python\"><b>Output: </b>\n",
    "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n",
    "</code></pre>\n",
    "\n",
    "Note that the `encoding` we've generated contains all the necessary outputs of the tokenizer in its various attributes: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_token_masks`, and `overflowing` (`special_token_masks` is self-explanatory, and `overflowing` is something we'll discuss later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwkV2emwCGER"
   },
   "source": [
    "Now, we'll add a `post_processor` module to our tokenizer. In order to create our post-processor, we need to use a `TemplateProcessor` object.\n",
    "\n",
    "For our `TemplateProcessor` object, we need to specify how to treat a single sentence and a pair of sentences. For our post-processor, we want to add the `[CLS]` token at the beginning and `[SEP]` token at the end of each sentence, or after each sentence (in the case of pairs of sentences being included in a single input sequence). To do this, note the following:\n",
    "*   The first (or single) sentence is represented by `$A`, while the second sentence is represetned by `$B`.\n",
    "*   For each of thes special tokens and sentences, we also specify the corresponding token type ID after a colon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKXPav6VDA9n"
   },
   "outputs": [],
   "source": [
    "# Get the token IDs associated with our [CLS] and [SEP] tokens\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "# Add a post_processor module to our tokenizer\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zYtqE-kDhjN"
   },
   "source": [
    "Now, let's add a `decoder` module to our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qY-a1ovDQN2"
   },
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_RgqABdDnTN"
   },
   "source": [
    "We could stop here. If you wanted to do so, you can save your tokenizer in a single JSON file as follows:\n",
    "\n",
    "<pre><code class=\"python\">tokenizer.save(\"tokenizer.json\")\n",
    "</code></pre>\n",
    "\n",
    "Now, you can reload that file in a `Tokenizer` object with the `from_file()` method:\n",
    "\n",
    "<pre><code class=\"python\">new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2o7UU8CD9c8"
   },
   "source": [
    "If you want to be able to use this tokenizer in 🤗 Transformers (the topic of our next section), our actual last step will be wrapping our tokenizer in a `PreTrainedTokenizerFast`. To do so, we can either pass the tokenizer we built as a `tokenizer_object` or pass the tokenizer file we saved as `tokenizer_file`. The key thing to remember is that we have to manually set all the special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123,
     "referenced_widgets": [
      "07525973e06c49d689a949cfd9001213",
      "52f7ed83c91d43c6a66d127139202725",
      "2c6567ce451243a8be84e8276c57de15",
      "975bdc8c1b60487da4c1b8566872d524",
      "f1e9472fbefb43c6bda9674d3813964a",
      "b3773112e5504c8984c02c3ce14505b2",
      "6d51cfba1b4d46b0ba45bfdcf39de540",
      "86d9a86ee8bb4bcbb2296deae79c2327",
      "cf19b167f29143bb860ccb80034df1d5",
      "9ebff9d7000b4857a9dd8a542ea1b8fd",
      "1142548591c54b588d9620cb57528191"
     ]
    },
    "id": "GIwrg9bHEPxx",
    "outputId": "96584342-b553-4435-ed12-f9737f558128"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07525973e06c49d689a949cfd9001213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\",      # If you want to pass in a tokenizer from a file\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-2lLPeBETIh"
   },
   "source": [
    "And that's it! Now, you can use your tokenizer like any other 🤗 Transformers tokenizer. If you want, you can save this tokenizer using the [`save_pretrained()`](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qvrlsuby-yg"
   },
   "source": [
    "## **6.3: 🤗 Transformers: Behind the Pipeline API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm4Eq_eeFrCz"
   },
   "source": [
    "$\\textbf{Section Introduction: }$\n",
    "\n",
    "**Source:** [Transformers, what can they do?](https://huggingface.co/learn/nlp-course/chapter1/3?fw=pt)\n",
    "\n",
    "While the 🤗 Datasets and 🤗 Tokenizers are going to be crucial libraries for preparing your datasets and input text for any variety of NLP tasks, the 🤗 Transformers library will be your main go-to library when working with Transformers and LLMs. HF hosts over [900,000 models](https://huggingface.co/docs/hub/en/index) (including Transformer-based models, CNNs, RNNs, etc.), which are accessible via the [Model Hub](https://huggingface.co/models). In order to utilize and create your own 🤗 Transformers models, you will be using the 🤗 Transformers library.\n",
    "\n",
    "The most basic object in the 🤗 Transformers library is the `pipeline()` function. This function connects a model with its necessary preprocessing and postprocessing steps, allowing you to directly input any text & receive an intelligble answer. For example, let's use the `pipeline()` function for the following simple extractive QA task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393,
     "referenced_widgets": [
      "fc7a1d8713cf4087b402885e7e9b05b9",
      "52f581b7293a491280a12400f65e0241",
      "50d9c3c3636c4e5790985c122c6d7939",
      "35fcec816fed4504b2c0f49ce2e847c1",
      "e2bdefd6561b472b8ce2495cd6c28be3",
      "911b67b85fb049cebb6e8e010f870fc2",
      "296be13c832f4180a6e4f9588d87bda9",
      "583b130711f742329c0139b326076239",
      "b905211caa2b489ba7cbf0c5d0299294",
      "ddae23e8fdc642ccbc90b5c875546cf1",
      "50552047366d4218be960664916ab46d",
      "eeb7326598ce45ab805862f38f22999e",
      "60f9e038a8a642b6b7bfb53bbc4e50a9",
      "6e1c4ad928c446378fc38f3d720a967e",
      "a18ede446565406f88958a79be922bcc",
      "4e61267bd3f6411a8141e337dd06feca",
      "a8a634f9997b42329cab44f4bb2a7b4e",
      "9932e45450994ec498a09512736c3dab",
      "fd3c5d204c75498c8f655469cdeb1eb5",
      "792fad0d63fe43acb6bbbad8aacf7f62",
      "06084b9d71454f14a150b0e4eeb1f13b",
      "b10549d54c694d5e8780e6347aadc3c8",
      "edea1cb764c94cd09e34f2900f504786",
      "a54f3b41d8dc404291227e3b083c534d",
      "facbcad61a6c4c3ead97cef9e48fdfac",
      "b4e42704e3ea41e38e10824a431500b9",
      "03bed342efd64befa9a1d1463027f587",
      "bd45e4a7296c4354831076f0ded9f308",
      "35ac8b4ca88545109d059e840be7d1a7",
      "04cda3288465464b91f24e1fcf92a2a3",
      "b93f594ad0074675870c1932282926c8",
      "6deac91ca05a489bb3a5991ac8f8997e",
      "086d6247cf284b34a232f3cec1ee51a9",
      "88b95ba9e9ca4dd08d0dcac1d63d5f15",
      "64962148617249fbbb066062d6019e88",
      "9bcc8e8c428a4fed9d476763ea04b632",
      "3ccfd3ec1bd840b8a24ab2264565c991",
      "5b78547495764842aa4fb87cd5bbcac8",
      "5503de6ab43947dcbf6a2c39034e2466",
      "17734ca0d2604f58baf5ae40d3e9283b",
      "32d2f014c6c343c786dd96c979da90bd",
      "45756c32abe749209e6f160bbe7f5328",
      "a958923c50384acdb91e801a59a54a6b",
      "d04e5cabed4e4617adc0641b9889803e",
      "84035eecf9684924a17e098960cbfeb7",
      "b7679c1ac022446083e1a780b70bd39b",
      "845224c3e7e54734b1384bbf94540111",
      "46fb614c85214e82a0a6c24e8e7e6cd3",
      "1a85ab1f66184d0b8cedb2ffac0566ab",
      "d1748c8b756b49aeb6dc89d9af6ce78a",
      "2cf95d6eb1a24c9f821309e6079615c3",
      "4ee386b6b08f4ce4af4dca6dd9edb11f",
      "452aa34eaa16419eac603657d209cb05",
      "5d4420cb889448e0b1e437bcef34d012",
      "adaec476be664c689f1087102ce96533"
     ]
    },
    "id": "k4_GqiVPe9be",
    "outputId": "5fece17b-e1f8-4d41-f416-9244712299f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7a1d8713cf4087b402885e7e9b05b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb7326598ce45ab805862f38f22999e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edea1cb764c94cd09e34f2900f504786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b95ba9e9ca4dd08d0dcac1d63d5f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84035eecf9684924a17e098960cbfeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949766278266907, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yra2WTHfdIV"
   },
   "source": [
    "You can customize the basic `pipeline()` function in a variety of ways; for example, you can pass in a string to the `model` argument to specify (in the above case) a specific NLP model fine-tuned for QA tasks, or pass a Boolean value to `use_fast` to specify whether or not you want to use a \"fast\" tokenizer. A complete documentation can be found [here](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/pipelines#transformers.pipeline) (scroll to the bottom of the linked-to section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-iL7EFfgJZr"
   },
   "source": [
    "In this section, we're going to be looking at how to use the 🤗 Transformers library in order to essentially re-create the results we can get from the baseline `pipeline()` function. In our progress towards this task we'll learn how to integrate the 🤗 Datasets and 🤗 Tokenizers libraries with the 🤗 Transformers library in order to utilize 🤗's NLP capabilities to its fullest extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17QmIu4z5WS5"
   },
   "source": [
    "### **6.3.1: Using models with 🤗 Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hS1R4ROhLyf"
   },
   "source": [
    "$\\textbf{Introduction: }$\n",
    "\n",
    "**Source:** [Models](https://huggingface.co/learn/nlp-course/chapter2/3?fw=pt)\n",
    "\n",
    "In this section, we're going to look at the variety of different methods available to us for loading in and saving our HF models.\n",
    "\n",
    "In general, when loading in 🤗 models you'll want to utilize the `AutoModel` class. In particular, you'll oftentimes be using a particular \"variation\"/version of this class depending upon the particular task you are trying to instantiate a model for (e.g., `AutoModelForQuestionAnswering` if you want to load in a NLP model trained for QA tasks); these particular variations essentially amount to adding different \"heads\"/re-configuring the final layer of the model depending upon what task you want to train the model for (e.g., a binary sentiment analysis model only requires that the final layer of the model contain two logit-producing neurons).  \n",
    "\n",
    "The `AutoModel` class and all of its relatives are simple wrappers over the wide variety of pre-trained models available in the library. It’s a clever wrapper as it can automatically guess the appropriate model architecture configuration for your checkpoint, and then instantiates a model with this architecture; all you'll have to do is pass in a string specifying the particular model checkpoint you want (e.g., `bert-base-uncased`).\n",
    "\n",
    "The important thing to understand about the `AutoModel` class is that it's just a shortcut command. Behind the scenes, the library has one model class per combination of architecture plus class/\"variation\". For example, if you call the `AutoModelForSequenceClassification` class using the model `\"distilbert-base-uncased-finetuned-sst-2-english\"`, this class will actually instantiate a `DistilBertModel` class -- in particular, the `DistilBertForSequenceClassification` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae_UoMG33Nbe"
   },
   "source": [
    "$\\textbf{Loading in a Model: }$\n",
    "\n",
    "Now, let's initialize a BERT model. To do so, we must instantiate its configuration before passing this configuration in as a model argument into the appropriate model class (in this case, we want `BertConfig` and `BertModel`). Using this approach, we automatically randomly initialize the weight and bias neurons of our model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TOSpaMD3YU8",
    "outputId": "c5ca94ac-dc5d-4c52-cd53-12d571cb4e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "print(config)\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-X4EclvF3g3Z"
   },
   "source": [
    "In order to load in a pre-trained BERT model, we will instead want to use the `from_pretrained()` method. Instead of passing in a `BertConfig` object, we'll instead pass in the `bert-base-cased` identifier.\n",
    "\n",
    "By utilizing the `from_pretrained()` method, we've now loaded a BERT model with the weights associated with the `bert-base-cased` model checkpoint. As a pre-trained model, we can use this model for inference purposes for the tasks it was trained on, or fine-tune this model for a separate task. By training with pretrained weights rather than from scratch, we can quickly achieve good results without significant data or computational cost. You can find out more about the specifics of the `bert-base-cased` model checkpoint by reading its [model card](https://huggingface.co/google-bert/bert-base-cased).\n",
    "\n",
    "Note that the identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. To find a complete list of all such models, see [here](https://huggingface.co/models?other=bert&sort=trending).\n",
    "\n",
    "---\n",
    "\n",
    "*Note: The URL linked to \"here\" is as follows:*\n",
    "\n",
    "<pre>\n",
    "https://huggingface.co/models?other=bert&sort=trending\n",
    "</pre>\n",
    "\n",
    "*To search for all models that are compatible with, say, the DistilBERT architecture, modify the above url as follows:*\n",
    "\n",
    "<pre>\n",
    "https://huggingface.co/models?<b>other=distilbert</b>&sort=trending\n",
    "</pre>\n",
    "\n",
    "*To see a complete list of all models you could modify the above \"other\" tag with, go to the [🤗 Transformers documentation](https://huggingface.co/docs/transformers/index) and search under \"Models\" > \"Text Models\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "144cec0143164fa68353c0dde3a962c0",
      "52c72eb0564541b186121ea3893854f3",
      "26aacfd269124b129a20a8f2b7db731a",
      "3bd92424c32a4ad192ecc5e0207cfcd9",
      "1162a5640f944c558c247a02097cae1d",
      "87c928be2a264d05a5b737c19b8fc904",
      "6424ab851f5741bba9e022affda482ba",
      "217154c39a594309bcb7c7f0baf692fd",
      "233fdf122ae34967965fe87640789bb7",
      "09dc178160bc4aa5a960026035e9c8e5",
      "b44e9926372242baaa84f1f849dc099c",
      "c854e62ad40f421881a846985bf376ae",
      "1254a9b6bcc94a2e8ec1b9697360442f",
      "85d3c70ecd8b454e950a03bb41d4e5f2",
      "f688e36b40b442ba9b5d7b3997f38c4d",
      "4177dcb8e62648febecd498135b4f382",
      "0693d03620834d04b130dfb83d820749",
      "473180932d7a4e81bdb8e5a08c4795c4",
      "9532815a965a440bb9701ccfa8d2cab3",
      "278b0abe9f7b4279a0f4d9a261b49518",
      "cf0461188898401aa3b30cfcecf56522",
      "4209b0b538a74ac0ade98cb40caeaec8"
     ]
    },
    "id": "YnuhZzdV7NAT",
    "outputId": "c2a137e1-e989-4107-aedc-b71129eb80fc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144cec0143164fa68353c0dde3a962c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c854e62ad40f421881a846985bf376ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQ-Yvhws7lpi"
   },
   "source": [
    "$\\textbf{Saving Our Model: }$\n",
    "\n",
    "In order to save our above model, we simply use the `save_pretrained()` method in a manner such as the following:\n",
    "\n",
    "<pre><code>model.save_pretrained(\"directory_on_my_computer\")\n",
    "</code></pre>\n",
    "\n",
    "Doing so will save the following files into the specified directory:\n",
    "\n",
    "<pre>(1) config.json\n",
    "(2) model.safetensors</pre>\n",
    "\n",
    "The *config.json* file will include all attributes necessary to build the model architecture. This file will also contain some additional metadata, such as where the checkpoint originated and what 🤗 Transformers version you were using when you last saved the checkpoint\n",
    "\n",
    "The *model.safetensors* file is our state dict -- that is, it contains all of our model weights. If you're interested in learning more about the Safetensors file format, you can start reading up about it [here](https://huggingface.co/docs/safetensors/en/index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1XTWv8g5fLc"
   },
   "source": [
    "### **6.3.2: Using tokenizers with 🤗 Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD998FmL9RLT"
   },
   "source": [
    "$\\textbf{Introduction: }$\n",
    "\n",
    "**Source:** [Tokenizers](https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt#tokenizers)\n",
    "\n",
    "In this section, we'll be looking at how to work with tokenizers in the 🤗 Transformers library.\n",
    "\n",
    "We've already worked with tokenizers extensively in Section 6.2 (in fact, Section 6.2.3 was all about building a tokenizer from scratch which could *also* be integrated with the 🤗 Transformers library). The main difference between Section 6.2 and this section is that while we only covered methods available to \"fast\" tokenizers in Section 6.2, the methods covered here are available to both \"fast\" *and* \"slow\" tokenizers. Most of time you are (and should be) working with \"fast\" tokenizers; still, it's useful to know the following general methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CXTYg72-scH"
   },
   "source": [
    "$\\textbf{Loading and Saving a Tokenizer: }$\n",
    "\n",
    "We can easily load and save our tokenizers with the same two methods we use for our models: `from_pretrained()` and `save_pretrained()`. When saving our tokenizer, we're essentially saving both the algorithm used (similar to the *architecture* of a model) as well as the vocabulary (similar to the *weights* of a model).\n",
    "\n",
    "Let's load in below a BERT tokenizer trained with the same checkpoint as the BERT model we looked at the in the previous section. Similar to the `AutoModel` and `BertModel` classes, there are the `AutoTokenizer` and `BertTokenizer` classes, respectively. Unlike the former classes, though, there is relatively less variety in the number of tokenizer classes available; whereas you might be inclined to use `AutoModelForSequenceClassification` or `AutoModelForQuestionAnswering` depending upon which task you are trying to fine-tune a pre-trained model on, you generally just use the `AutoTokenizer` (or, as in our case, `BertTokenizer`) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204,
     "referenced_widgets": [
      "0d303729254f4374b1eaac34ec067979",
      "810af4928d5247a1b4725db29d6d8af5",
      "8d6caad5bc1849c99d4469abfd39157d",
      "ebc72bdf05de4fdabee331a8aaaae973",
      "f9419f144fcd4fccadb67d1b1dc62684",
      "bec6eef5e5e2438c9ce6bb28cbd6d649",
      "50e3c6f2b0824fa1b069e0a2640a81f2",
      "b6a029b9e86f4ec0b3af8507da1a6998",
      "06e94dc0533b4320b56616292a1a3022",
      "1224dee8a9e0426caacc4ae43a19227f",
      "43af094091cb4d40a7652d3491bb3d66",
      "a0dd6290f35c4a10ba987c4cd66d0896",
      "5bbdabe6bfe44592ac28e0564b6c2395",
      "78b231431b114d55875f75948ecd8810",
      "445193d3516f468fa9dfb0757b62cc3e",
      "e28a8c838e214fada832de7988275842",
      "8f80bdd560cf4ebd9bd30519aa30ca9d",
      "b354317b2def4db88d7cc1866cbaad1a",
      "c4d67daa8ec145a7a973a6ab6a877c34",
      "c84c3f9c828547fe8c1c9b5b520c4028",
      "9253504af4864098989ec584a700e219",
      "3d992c7686ca4343bb65e9cd6d995b1c",
      "d9cdeb1b9c6c438aa61c4c4954958796",
      "f04d8bf670cd4ede960ecf61cd13a979",
      "b0a52b52613f404ead5bfd8fcee4fe20",
      "da31dede7b9b4940b46a39a394612e5b",
      "f91345863f8c4c5982ef2f64ff5a886a",
      "223a167c14444cdd8d1cb956b1accc1d",
      "ce6a9119c4814f40a8609a5f3202b278",
      "4b49fb19b1b54c9cb514642a9a1c9873",
      "761aa90a3580408ba9bb159c0f9a4be2",
      "dd596e23627f44c3a3f9fb154c25df42",
      "5b3a7f3ffdc2423a9f497598e448e6b8"
     ]
    },
    "id": "xsekbvkOAJk6",
    "outputId": "bc25cfcd-f712-484d-ac7a-cb62a3b681ec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d303729254f4374b1eaac34ec067979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dd6290f35c4a10ba987c4cd66d0896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cdeb1b9c6c438aa61c4c4954958796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Loading our tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenizer inference on an example input sequence\n",
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7H4s0gnAWrA"
   },
   "source": [
    "To save our tokenizer, we'd run something like the following:\n",
    "\n",
    "<pre><code>tokenizer.save_pretrained(\"directory_on_my_computer\")\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1ynQ-AZAdap"
   },
   "source": [
    "$\\textbf{Common Tokenization Methods Available in the HF Transformers Library:}$\n",
    "\n",
    "In general, you'll simply be calling the tokenizer directly on your inputs. Still, we dedicate this section to a brief review of some common tokenization methods applicable to 🤗 Transformers-supported tokenizers which you may encounter outside of this notebook (and, in fact, have already encountered in previous sections).\n",
    "\n",
    "Generally speaking, the *encoding* process (i.e., the process of encoding input text into a sequence of numbers) is two-fold. First, you translate your input sequence into a list of tokens via the tokenizer's `tokenize()` method. Finally, you'll convert your list of tokens into a sequence of numbers/input IDs via the tokenizer's `convert_tokens_to_ids()` method. Let's quickly try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWFUBDWmCAyi",
    "outputId": "f1a48632-5a0c-49bd-c724-f8d8eeaa3113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTokens:\u001b[0m  ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
      "\u001b[1mNumber IDs:\u001b[0m  [7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load in the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize an input sequence\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(\"\\033[1mTokens:\\033[0m \", tokens)\n",
    "\n",
    "# Convert our tokens to numbers/input IDs\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\033[1mNumber IDs:\\033[0m \", ids)\n",
    "  # These IDs are currently stored in a list. You'll want to convert this list of IDs into a torch.Tensor in order to\n",
    "  # actually work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgfNhAd2CqB2"
   },
   "source": [
    "In order to decode a sequence of vocabulary indices back into the original string, we can simply call the tokenizer's `decode()` method.\n",
    "\n",
    "Note that the `decode()` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be useful when we use models that predict new text.\n",
    "\n",
    "Let's look at a quick example to wrap up this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwVLZZu4C_a1",
    "outputId": "bb85bf79-8195-4099-94a5-99c435370ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dw3L3LH5h_Z"
   },
   "source": [
    "### **6.3.3: Additional details: padding, attention masks, and more**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66eKIGFbDOfj"
   },
   "source": [
    "$\\textbf{Introduction: }$\n",
    "\n",
    "**Sources:**\n",
    "1.   [Handling multiple sequences\n",
    "](https://huggingface.co/learn/nlp-course/chapter2/5?fw=pt#handling-multiple-sequences)\n",
    "2.   [Putting it all together](https://huggingface.co/learn/nlp-course/chapter2/6?fw=pt)\n",
    "3.   [Padding and truncation](https://huggingface.co/docs/transformers/en/pad_truncation#padding-and-truncation)\n",
    "\n",
    "In this section, we're going to cover some additional details related to utilizing models and tokenizers within the 🤗 Transformers library. In particular, we'll cover the following topics:\n",
    "\n",
    "*   Batching  \n",
    "*   Padding\n",
    "*   Attention Masks\n",
    "*   Truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVCY3Nkuy8qb"
   },
   "source": [
    "$\\textbf{Batching: }$\n",
    "\n",
    "Suppose you run the following code:\n",
    "\n",
    "<pre><code><b>Input: </b>\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "input_ids = torch.tensor(tokenizer.convert_tokens_to_ids(tokens))\n",
    "\n",
    "model(input_ids)   # This line will fail\n",
    "</code></pre>\n",
    "\n",
    "You'll end up getting the following `IndexError`.\n",
    "\n",
    "<pre><code><b>Output: </b>\n",
    "IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
    "</code></pre>\n",
    "\n",
    "Why? 🤗 Transformers models expect multiple sentences by default. This doesn't mean you can't only send in a single input sequence; rather, you need to make sure you add a \"batch\" dimension to your `input_ids` before passing them into your model (the model will automatically add the dimension corresponding to the hidden states of the model, so you only need to provide a \"batch\" dimension and \"input sequence\" dimension to your model input). This can easily be accomplished by modifying the input to `torch.tensor` as follows:\n",
    "\n",
    "<pre><code>input_ids = torch.tensor(<b>[</b>tokenizer.convert_tokens_to_ids(tokens)<b>]</b>)\n",
    "</code></pre>\n",
    "\n",
    "Now, before wrapping this subsection up, you should know that 🤗 Transformers-backed tokenizers can handle the conversion of inputs to specific framework tensors -- that is, you don't have to manually convert the `tokenizer.convert_tokens_to_ids()` output to a `torch.tensor`. You can accomplish this by passing different inputs to the `return_tensors` keyword argument: `\"pt\"` returns PyTorch tensors, `\"tf\"` returns TensorFlow tensors, and `\"np\"` returns NumPy arrays. In order to be able to actually convert the input into a specific framework tensor, you'll need to pass in an additional argument: `padding = True`. See the subsection below for a discussion of padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc5kRn73BZCz"
   },
   "source": [
    "$\\textbf{Padding}$\n",
    "\n",
    "If you're familiar with tensors from a mathematical perspective, you know that in the case of a 2D tensor, all columns *have* to have the same number of entries & all the rows *have* to have the same number of entries. So how are we able to pass in a batch of input sequences to our model where different inputs have different sequence lengths? Padding.\n",
    "\n",
    "*Padding* refers to the process of repeatedly appending tokens -- in particular, special tokens known as *padding tokens* -- to relevant input sequences within a single batch until all sequences within a batch have the same number of tokens as the longest (non-padded) input sequence (or until all input sequences are of some specified token length). Different batches can be padded to different lengths depending upon the size of the longest non-padded input sequence (*dynamic padding*, which we'll see later on in Section 6), or all input sequences in all batches can be padded to some pre-determined token size.\n",
    "\n",
    "If you want to manually pad your input sequences, you can access the padding token index for your tokenizer's vocabulary using `tokenizer.pad_token_id`. If you want your tokenizer to automatically handle padding for you, you can specify one of 3 (technically 5) options as input to the `padding` keyword argument:\n",
    "\n",
    "*   `True`/`\"longest\"` => Pad to the longest sequence in the batch.\n",
    "*   `\"max_length\"` => Pad to a length specified by the `max_length` argument (another keyword argument you can pass to your tokenizer) or the maximum length accepted by the model if no `max_length` is provided (make sure the checkpoint you use for your model and your tokenizer are the same!).\n",
    "*   `False`/`\"do_not_pad\"` => Do not apply padding. This is the default behavior.\n",
    "\n",
    "With the above in mind, entertain the following example. `distilbert-base-uncased-finetuned-sst-2-english` is a fine-tuned Transformer-based model capable of binary sentiment analysis inference (e.g., a binary sentiment analysis model could be asked to describe a provided movie review as either a \"positive\" or a \"negative\" review). Notice that we get some strange results in the model's logits:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "3fd72d47e4c542ef8b810fbff30b1bcc",
      "7ee619e13bdc4568b3a85b776f5ff7da",
      "698d27cd92474c96a02e7023987606d2",
      "6c72b7382ccc48e0a3873b3a432b0ad0",
      "8f2cd5d2c7f14c899c48aa177eb3e914",
      "c20b194d27bc48ca955cdda7f3c13702",
      "8857a4bff8c44f1c87f4ec7010d1bd7e",
      "3ecf9cdf0c4845e69758ae65fd7e104c",
      "172d80737b2e4512bc7427aaa3a5e057",
      "274ec6bd5bf04b949ca0c338e28c65c6",
      "62d5ab7d20324592a9ce46f7e0ec1625",
      "1ded5ce470354b0b845a1b2bca68b5b9",
      "e98b917004b74822a23aff186d8f020b",
      "9a968f2c80bc403e895289c2346f55cb",
      "38d8ee3660724df98946a46f08845cc1",
      "6157bcd5010e47879de36249caea407a",
      "fe7d7025dac24a559c24f0c7596ee339",
      "f95558c3cf5344e8a695e2c076d9d0e7",
      "b61ca30b077845199c8854b208d9cca7",
      "0cf1876934984ef8ab10b3ec9c32c2b9",
      "ba27778ad27d48889d430c3ae8258952",
      "a23d09e78c824f8fae47f5f5697b92b6",
      "9e9674392349422f82c4e37074b7b3c4",
      "0049ab3f84c748fa8e6ef049d57e8130",
      "bce8e7a8b6ee4d2896ece6f71e818a20",
      "20007169b0f4430aabbb80caf8734d0c",
      "35d9863239d54821a9b107f8ee5344c9",
      "e0cbc92685634dfbb88533b725f5c87c",
      "ba6c8f2b9c454f09af10db85d01df9b1",
      "1a7d3e37da2b4878a99ece80128ee9e0",
      "10c53cd7624040cd987c7b4ba7a72f2d",
      "62d0bd77814246d9a6168dd44f68578f",
      "4e98645baafc4c6389d51de198171280",
      "54c6eea657f045ca998c4fce04f3bdaf",
      "223793b3c3334219aff419de60409d35",
      "d8f34a8087284bf082322875a3196aab",
      "579580e048e345a3872b0cd36bfbdf72",
      "d8a68eb7af8b4304975713847297b73e",
      "6377a069ec84466eae3a4cf5190c0a2b",
      "421225aea3e74cbab41a0a4533c0a6d3",
      "fb46ad1bc12247709bb8a7a2184ae09c",
      "baf4ba941a984c17b0eff7f36095e9be",
      "027092f47d5f4d19b5a2d026f1c85feb",
      "c4e6d95e74394c95918f1e45e93f2504"
     ]
    },
    "id": "sbPX8sbyRU1h",
    "outputId": "e8502aae-0e82-4f89-93d3-588256047936"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd72d47e4c542ef8b810fbff30b1bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ded5ce470354b0b845a1b2bca68b5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9674392349422f82c4e37074b7b3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c6eea657f045ca998c4fce04f3bdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msequence1_ids Logits:\u001b[0m  tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "\u001b[1msequence2_ids Logits:\u001b[0m  tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "\u001b[1mbatched_ids Logits:\u001b[0m  tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load in the model and the tokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Random sequences. Notice that batched_ids is a combination of sequence1_ids and sequence2_ids (with the appropriate padding)\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "# Logits\n",
    "print(\"\\033[1msequence1_ids Logits:\\033[0m \",model(torch.tensor(sequence1_ids)).logits)\n",
    "print(\"\\033[1msequence2_ids Logits:\\033[0m \",model(torch.tensor(sequence2_ids)).logits)\n",
    "print(\"\\033[1mbatched_ids Logits:\\033[0m \",model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b6LskiySKiL"
   },
   "source": [
    "Why are the logits associated with `sequence2_ids` different from the logits associated with the second input sequence to `batched_ids`? Simply put, because the attention mechanism within the DistilBERT model is attending to *every* token within its input, including the padding token. While the model has likely learned a representation for the padding token (which differs it in the model's \"mind\" from any other token), the fact that it is still being attended to by the model means that the model's logits for `sequence2_ids` and for the second row of `batched_ids` are going to be different.\n",
    "\n",
    "If we instead wanted (as we should) the model to produce the same logits in both instances, then we need to *mask* the padding token in `batched_ids`. This naturally leads us to our next subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLuG2xB7UcKd"
   },
   "source": [
    "$\\textbf{Attention Masks: }$\n",
    "\n",
    "Pulling from the HuggingFace NLP course:\n",
    "\n",
    ">\"*Attention masks* are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\"\n",
    "\n",
    "You can provide your own attention mask to the model via the `attention_mask` argument. Make sure that your `attention_mask` is of the same framework tensor as the model's inputs.\n",
    "\n",
    "Let's now \"fix\" the above example so that we get the same logits in both instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3-Xm3UVVGF7",
    "outputId": "40b5a8a5-a428-411c-de76-2349b96f11a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msequence1_ids Logits:\u001b[0m  tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "\u001b[1msequence2_ids Logits:\u001b[0m  tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "\u001b[1mbatched_ids Logits:\u001b[0m  tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load in the model and the tokenizer\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# Random sequences. Notice that batched_ids is a combination of sequence1_ids and sequence2_ids (with the appropriate padding)\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "# Attention mask (only for batched_ids)\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "# Logits\n",
    "print(\"\\033[1msequence1_ids Logits:\\033[0m \",model(torch.tensor(sequence1_ids)).logits)\n",
    "print(\"\\033[1msequence2_ids Logits:\\033[0m \",model(torch.tensor(sequence2_ids)).logits)\n",
    "print(\"\\033[1mbatched_ids Logits:\\033[0m \",model(torch.tensor(batched_ids), attention_mask = torch.tensor(attention_mask)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkVMfg9pVY31"
   },
   "source": [
    "$\\textbf{Truncation: }$\n",
    "\n",
    "In essence, *truncation* is a solution to the inverse problem which padding deals with. For Transformer-based models, there is a limit to the lengths of the sequences we can pass to them before their performance starts breaking down. In order to deal with this problem, there are essentially 3 possible solutions we can entertain:\n",
    "\n",
    "*   Truncate all of the relevant sequences -- that is, curtail any sequences longer than the largest supported input token sequence length.\n",
    "*   Use a different Transformer-based model which can support longer input sequences.\n",
    "*   Split an input text sequence across multiple input token sequences.\n",
    "\n",
    "The first and third options are most commonly utilized (depending upon *how* long the longest input sequences you want to pass to your model are).\n",
    "\n",
    "For example, suppose we wanted to pre-train a causal language model. On the one hand, the model would likely be just fine if we outright truncated any relevant input token sequences; as long as our model has some sort of `[SEP]` token, the model shouldn't get confused & assume that a bunch of cut-off sentences are actually complete sentences. On the other hand, we could just as easily split each input text sequence across multiple input token sequences/entries in our tokenized dataset. Especially if we're just pre-training a CLM, all we really need is the text; we can easily remove most other columns from the original, pre-tokenized dataset, and we can easily duplicate across multiple entries any other info that's still crucial post-tokenization.\n",
    "\n",
    "Now, let's look at some truncation options that 🤗 Transformers-backed tokenizers provide us:\n",
    "\n",
    "*   `True`/`\"longest_first\"` => Truncate to a maximum length specified by the `max_length` argument or the maximum length accepted by the model if no `max_length` is provided. The \"first\" in `longest_first` refers to the fact that the longest possible sequence starting from the *first* token in any input token sequence will be returned.\n",
    "*   `\"only_second\"` => Truncate to a maximum length specified by the `max_length` argument or the maximum length accepted by the model if no `max_length` is provided. This will only truncate the second sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided. For example, if we're fine-tuning a model for an extractive QA task, oftentimes only the \"context\" input sequence (versus the \"question\" input sequence, which typically comes before the \"context\" input sequence in a pair of input sequences) needs to be truncated.  \n",
    "*   `\"only_first\"` => Similar to `\"only_second\"`, but this will only truncate the first sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.  \n",
    "*   `False`/`\"do_not_truncate\"` => Do not apply truncation. This is the default behavior.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g73Ju-3r535h"
   },
   "source": [
    "### **6.3.4: Putting everything together: A simple example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KprX2KEqjwno"
   },
   "source": [
    "$\\textbf{Introduction: }$\n",
    "\n",
    "**Source:** [Fast tokenizers in the QA pipeline](https://huggingface.co/learn/nlp-course/chapter6/3b?fw=pt)\n",
    "\n",
    "At the beginning of this section, we saw an example utilizing the 🤗 Pipeline API in order to solve an extractive QA task. In Section 6.3.4, we're going to use what we've learned throughout the rest of Section 6 in order to recreate the output of the 🤗 Pipeline API when applied to 2 QA tasks:\n",
    "*   A QA task involving a regular-length context input sequence.\n",
    "*   A QA task involving a long context input sequence -- that is, a context input sequence which needs to be truncated.\n",
    "\n",
    "The question for each of the above two QA tasks is the same: `\"Which deep learning libraries back 🤗 Transformers?\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM7n1Ul5olFJ"
   },
   "source": [
    "First, let's recreate the output of the 🤗 Pipeline API when applied to our first of two QA tasks.\n",
    "\n",
    "To start off, we load in our 🤗 Transformers-backed model and tokenizer with the provided checkpoint. `distilbert-base-cased-distilled-squad` is the default model for the `question-answering` pipeline (note that \"squad\" refers to the the SQuAD dataset this model was fine-tuned on).\n",
    "\n",
    "From here, we compute and store our tokenzed inputs and offset mappings. For each token, these offset mappings store a pair of indices (strictly speaking, a 1D tensor of length 2) which map to the starting character & ending character of that particular token in the original input string; the way our particular tokenizer works is that it concatenates the question and context tokenized input sequences one after the other, separated by a `[SEP]` token (and with a `[CLS]` token at the beginning of each tokenized input sequence). For example, the offset mapping associated with `\"deep\"` (as in `\"Which deep learning libraries back...\"`) (assuming `\"deep\"` is only mapped to a single token) would be, `[6, 10]`, as `tokenized_input_sequence[6:10] == \"deep\"` is `True`.\n",
    "\n",
    "Finally, we pass in our tokenized input to our model. Since our input is a `BatchEncoding` object, we need to unpack our input before passing it to the model. The `outputs` of our model are the model's final layer logits. In particular, `outputs.start_logits` are the model's logits as it pertains to our model's prediction of the question's answer's first token within our tokenized input sequence (remember, this is an *extractive* QA task), whereas `outputs.end_logits` are the model's logits as it pertains to our model's prediction of the question's answer's last token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulsBf45KdmbP",
    "outputId": "085cd786-11c0-454c-dbbf-7e752c4fb9d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOffsets:\u001b[0m  tensor([[[  0,   0],\n",
      "         [  0,   5],\n",
      "         [  6,  10],\n",
      "         [ 11,  19],\n",
      "         [ 20,  29],\n",
      "         [ 30,  34],\n",
      "         [ 35,  36],\n",
      "         [ 37,  49],\n",
      "         [ 49,  50],\n",
      "         [  0,   0],\n",
      "         [  1,   2],\n",
      "         [  3,  15],\n",
      "         [ 16,  18],\n",
      "         [ 19,  25],\n",
      "         [ 26,  28],\n",
      "         [ 29,  32],\n",
      "         [ 33,  38],\n",
      "         [ 39,  43],\n",
      "         [ 44,  51],\n",
      "         [ 52,  56],\n",
      "         [ 57,  65],\n",
      "         [ 66,  75],\n",
      "         [ 76,  77],\n",
      "         [ 78,  81],\n",
      "         [ 81,  82],\n",
      "         [ 83,  84],\n",
      "         [ 84,  85],\n",
      "         [ 85,  86],\n",
      "         [ 86,  88],\n",
      "         [ 88,  90],\n",
      "         [ 90,  91],\n",
      "         [ 92,  95],\n",
      "         [ 96,  99],\n",
      "         [ 99, 102],\n",
      "         [102, 103],\n",
      "         [103, 106],\n",
      "         [107, 108],\n",
      "         [109, 113],\n",
      "         [114, 115],\n",
      "         [116, 119],\n",
      "         [119, 120],\n",
      "         [120, 124],\n",
      "         [125, 136],\n",
      "         [137, 144],\n",
      "         [145, 149],\n",
      "         [149, 150],\n",
      "         [151, 153],\n",
      "         [153, 154],\n",
      "         [154, 155],\n",
      "         [156, 171],\n",
      "         [172, 174],\n",
      "         [175, 180],\n",
      "         [181, 185],\n",
      "         [186, 192],\n",
      "         [193, 197],\n",
      "         [198, 201],\n",
      "         [202, 208],\n",
      "         [209, 216],\n",
      "         [217, 221],\n",
      "         [222, 225],\n",
      "         [226, 228],\n",
      "         [228, 235],\n",
      "         [236, 240],\n",
      "         [241, 244],\n",
      "         [245, 250],\n",
      "         [250, 251],\n",
      "         [  0,   0]]])\n",
      "\u001b[1mModel Outputs:\u001b[0m  QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-4.4952, -6.4454, -4.7115, -7.0968, -7.0726, -7.4981, -5.5397, -4.1368,\n",
      "         -5.9199, -5.4193, -1.5920, -1.0857, -5.0981, -2.9331, -3.4070,  2.2467,\n",
      "          5.1563, -1.3602, -2.2209, -0.9686, -4.8112, -2.2527,  1.4383, 10.1211,\n",
      "         -1.5311,  2.2685, -1.8951, -2.2108, -4.2142, -2.5571, -2.3252, -2.6046,\n",
      "          1.7047, -1.9867, -1.7211, -0.5415, -2.0239, -4.4246, -5.1012, -4.4966,\n",
      "         -7.8940, -6.7200, -4.6759, -6.3279, -4.8339, -5.1839, -3.3724, -7.4120,\n",
      "         -8.1542, -4.4871, -7.4659, -4.3293, -4.2293, -3.1903, -7.9467, -5.2665,\n",
      "         -7.5902, -5.0570, -7.4476, -7.9083, -6.5951, -7.4061, -8.8821, -7.6749,\n",
      "         -6.9879, -7.0466, -5.4193]], grad_fn=<CloneBackward0>), end_logits=tensor([[-2.3958e+00, -7.0978e+00, -7.0745e+00, -6.3676e+00, -5.9532e+00,\n",
      "         -7.9585e+00, -7.1869e+00, -3.6494e+00, -6.9677e+00, -5.1421e+00,\n",
      "         -3.1757e+00, -1.1649e+00, -7.0748e+00, -5.2875e+00, -6.8611e+00,\n",
      "         -5.1769e+00,  3.7892e+00, -4.4408e+00, -7.6688e-01, -3.9180e+00,\n",
      "         -2.1634e+00,  1.8116e+00, -1.4678e+00,  2.0508e+00,  1.5417e-03,\n",
      "         -1.5531e+00, -6.9470e-01, -1.3466e+00, -1.6879e+00,  4.0826e+00,\n",
      "          1.1467e+00, -3.7881e-01,  6.0774e-01,  1.2281e+00,  5.8202e-01,\n",
      "          1.0657e+01,  5.8794e+00, -5.7342e+00, -7.0719e+00, -6.8077e+00,\n",
      "         -7.1513e+00, -5.3228e+00, -3.4305e+00, -4.2575e+00,  2.2268e+00,\n",
      "         -4.1297e-01, -6.8944e+00, -7.9381e+00, -8.3298e+00, -5.6078e+00,\n",
      "         -8.9589e+00, -5.5772e+00, -5.7309e+00, -1.9592e+00, -7.8078e+00,\n",
      "         -2.3823e+00, -7.2457e+00, -6.1642e+00, -4.2830e+00, -8.0948e+00,\n",
      "         -8.0364e+00, -4.5566e+00, -7.6585e+00, -7.3241e+00, -2.2402e+00,\n",
      "         -1.8463e+00, -5.1421e+00]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
      "\u001b[1mStart Logits Shape:\u001b[0m  torch.Size([1, 67])\n",
      "\u001b[1mEnd Logits Shape:\u001b[0m  torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Loading in the model and the tokenizer from the checkpoint\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Our provided context & question\n",
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "\n",
    "# Tokenized inputs + offset mappings\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "offsets = inputs.pop(\"offset_mapping\")    # Our model doesn't expect the offest_mapping entry in our inputs, so we pop it out here & store it in offsets\n",
    "print(\"\\033[1mOffsets:\\033[0m \", offsets)\n",
    "\n",
    "# Model outputs\n",
    "outputs = model(**inputs)\n",
    "print(\"\\033[1mModel Outputs:\\033[0m \", outputs)\n",
    "print(\"\\033[1mStart Logits Shape:\\033[0m \", outputs.start_logits.shape)\n",
    "print(\"\\033[1mEnd Logits Shape:\\033[0m \", outputs.end_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHNxQ-vitfWA"
   },
   "source": [
    "Before we can compute our softmax probabilities associated with the output's `start_logits` and `end_logits`, we need to first mask our tokenized input sequence. As mentioned previously, our tokenized input sequence is of the form `[CLS] question [SEP] context [SEP]`, so we need to mask all of the tokens associated with our tokenized question sequence as well as our `[SEP]` tokens. Note that we don't mask our `[CLS]` token, since some models use this token to indicate that the answer is *not* in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FU3QYUaYe5u7",
    "outputId": "bf4374cf-ffd9-40cb-ab65-9712c8de4764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1msequence_ids:\u001b[0m  [None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "## Masking the question tokens & the [SEP] tokens before taking a softmax over our output probabilities\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "print(\"\\033[1msequence_ids:\\033[0m \", sequence_ids)\n",
    "\n",
    "# Preparing mask\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "mask[0] = False     # Unmask our [CLS] token\n",
    "mask = torch.tensor(mask).unsqueeze(0)    # Unsqueeze our mask from dim. [67] to dim. [1, 67]\n",
    "\n",
    "# Storing & masking start_logits and end_logits\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpXyNAi0uA19"
   },
   "source": [
    "Now, we calculate the softmax probabilities associated with our `start_logits` and `end_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGWKxrKqggm1"
   },
   "outputs": [],
   "source": [
    "## Calculating and storing our softmax probabilities\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1).squeeze(0)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1).squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qzrgvr2BuOnI"
   },
   "source": [
    "With these probabilities, we're now ready to compute the `(start_token_index, end_token_index)` pair with the highest associated probability -- i.e., our model's prediction. In order to compute these pair probabilities, we make the following two assumptions:\n",
    "\n",
    "1.   The events “The answer starts at `start_token_index`\" and “The answer ends at `end_token_index`” are two independent events. With this independence assumption, the pair probabilities for each `start_token_index` and `end_token_index` is simply:\n",
    "\n",
    "$$start\\_probabilities[start\\_token\\_index] \\times end\\_probabilities[end\\_token\\_index]$$\n",
    "\n",
    "2.   The correct answer, if it can be found within the context, is such that the `start_token_index` index is *less than or equal to* the `end_token_index` index. This is a rather natural assumption to make.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0APesXm3gnbl",
    "outputId": "6793cef0-9884-43ce-b1e7-bdd96fa83285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShape of Our Scores Tensor:\u001b[0m  torch.Size([67, 67])\n",
      "\n",
      "\u001b[1mScore Associated With Our Model's Prediction:\u001b[0m  0.9802603125572205\n"
     ]
    }
   ],
   "source": [
    "## Calculating the models' predicted (start_token_index, end_token_index) pair & extracting its associated score\n",
    "# Calculating our scores\n",
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "print(\"\\033[1mShape of Our Scores Tensor:\\033[0m \", scores.shape)\n",
    "scores = torch.triu(scores)     # We want to find the (start_token_index, end_token_index) pair with the highest score which ALSO satisfies the following condition: start_token_index <= end_token_index\n",
    "\n",
    "# Getting the model's predicted (start_token_index, end_token_index) pair\n",
    "max_index = scores.argmax().item()\n",
    "\n",
    "# Extracting the score associated with our model's predction\n",
    "# Since PyTorch returns the index in the flattened tensor, we need to do the following manipulations to get the start_token_index & end_token_index\n",
    "start_token_index = max_index // scores.shape[0]      # Finding the appropriate row index of our scores matrix\n",
    "end_token_index = max_index % scores.shape[1]       # Finding the appropriate column index of our scores matrix\n",
    "print(\"\\n\\033[1mScore Associated With Our Model's Prediction:\\033[0m \", scores[start_token_index, end_token_index].data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AnZ3qPAv7Uy"
   },
   "source": [
    "With our `offsets` as well as our model's predicted `start_token_index` & `end_token_index`, we're now ready to extract (in text format) our model's predicted answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kwatu1VfitWe",
    "outputId": "953da36b-591c-418f-dc62-2284ec434740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOur Model's Answer:\u001b[0m  Jax, PyTorch, and TensorFlow\n"
     ]
    }
   ],
   "source": [
    "## Extracting our model's answer in text format\n",
    "start_char, _ = offsets.squeeze(0)[start_token_index]\n",
    "_, end_char = offsets.squeeze(0)[end_token_index]\n",
    "answer = context[start_char:end_char]\n",
    "print(\"\\033[1mOur Model's Answer:\\033[0m \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-8hGh4xwSBS"
   },
   "source": [
    "Finally, we format our answer so that our `result` is in the same format as the output to the 🤗 `question-answering` Pipeline API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eu00nuKjgQw",
    "outputId": "906992d9-ef28-4848-e58e-d24338d26edf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 78, 'end': 106, 'score': 0.9802603125572205}\n"
     ]
    }
   ],
   "source": [
    "## Reacreating QA pipline API output\n",
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char.item(),\n",
    "    \"end\": end_char.item(),\n",
    "    \"score\": scores[start_token_index, end_token_index].data.item(),\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBGyK6SSj0oE"
   },
   "source": [
    "$\\textbf{Long Contexts: }$\n",
    "\n",
    "The maximum tokenized input sequence length accepted by the 🤗 `question-answering` pipeline is 384 tokens. If we attempt to tokenize our input sequence with `long_context` (see below) as our context, though, our tokenized input sequence will contain 461 tokens.\n",
    "\n",
    "Naturally, we'll need to truncate our input to the maximum sequence length. Since we only need to truncate our context sequence, we'll make sure to pass in `\"only_second\"` to our `truncation` tokenizer keyword argument. Doing only this, though, means we encounter a critical issue: what if the answer to our question isn't contained within our truncated context?\n",
    "\n",
    "How do we address the above problem? There are two things we'll want to do:\n",
    "\n",
    "1.   We'll want to \"truncate\" our tokenized input sequence by actually splitting it across multiple input sequences. It would also be nice to have a mapping between each text sequence and each of the tokenized input sequences it produces; since we're only passing in a single question-answer sequence, though, it's not strictly necessary.\n",
    "2.   In the instance that our model splits the context at exactly the wrong place (that is, our starting token ends up being split into one input sequence while our ending token ends up being split into another input sequence), we'll want to include some overlap between the chunks.\n",
    "\n",
    "Fortunately, 🤗 Transformers-backed tokenizers allow us to achieve the above two objectives with relative ease. In order to split our text input sequence across multiple tokenized input sequence, we need to pass in `return_overflowing_tokens=True`. Not only will this argument split our text input sequence; it will also return `overflow_to_sample_mapping`, a mapping which tells us which sentence each of the resulting tokenized input sequences corresponds to. In order to specify the number of tokens of overlap we desire, we pass in an integer to the `stride` keyword argument.\n",
    "\n",
    "Let's look at a quick example:\n",
    "\n",
    "<pre><code><b>Input: </b>\n",
    "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "inputs = tokenizer(\n",
    "    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))\n",
    "</code></pre>\n",
    "\n",
    "<pre><code><b>Output: </b>\n",
    "'[CLS] This sentence is not [SEP]'\n",
    "'[CLS] is not too long [SEP]'\n",
    "'[CLS] too long but we [SEP]'\n",
    "'[CLS] but we are going [SEP]'\n",
    "'[CLS] are going to split [SEP]'\n",
    "'[CLS] to split it anyway [SEP]'\n",
    "'[CLS] it anyway. [SEP]'\n",
    "</code></pre>\n",
    "\n",
    "Now, let's look at our `overflow_to_sample_mapping`:\n",
    "\n",
    "<pre><code><b>Input: </b>\n",
    "print(inputs[\"overflow_to_sample_mapping\"])\n",
    "</code></pre>\n",
    "\n",
    "<pre><code><b>Output: </b>\n",
    "[0, 0, 0, 0, 0, 0, 0]\n",
    "</code></pre>\n",
    "\n",
    "Since all 7 tokenized input sequences are generated from 1 text input sequence, all of the entries in `overflow_to_sample_mapping` are `0`.\n",
    "\n",
    "With the above example out of our way, let's get back to our second QA task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bB0DPn_Hj23A"
   },
   "outputs": [],
   "source": [
    "## Instantiate our long_context\n",
    "long_context = \"\"\"\n",
    "🤗 Transformers: State of the Art NLP\n",
    "\n",
    "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mhe7VXSD0_8l"
   },
   "source": [
    "First, let's extract our tokenized input sequence. Let's quickly walk through all of the keyword arguments provided to our tokenizer:\n",
    "\n",
    "*   We feed in our `question` and `long_context` string.\n",
    "*   We specify a `stride` of `128` -- that is, there will be 128 tokens of overlap between subsequent tokenized input sequences generated from the same text input sequence.\n",
    "*   We pass in a `max_length` of `384` -- the maximum tokenized input sequence length accepted by the 🤗 `question-answering` pipeline's default model.\n",
    "*   We pass in `padding=\"longest\"` so that every tokenized input sequence is 384 tokens long. This will allow our inputs to be formatted as PyTorch tensors, as specified by the `return_tensors = \"pt\"` argument.\n",
    "*   We pass in `truncation=\"only_second\"` since we only want to truncate the context text sequence.\n",
    "*   We pass in `return_overflowing_tokens=True` so that all of the splits of our original text input sequence when tokenized are returned.\n",
    "*   We pass in `return_offsets_mapping=True` in order to have returned our offsets.\n",
    "\n",
    "After getting our inputs, we remove the `overflow_to_sample_mapping` and `offset_mapping` `BatchEncoding` key-value pairs before passing our input to the model (the model doesn't expect them in our input) & extracting our logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HID6GnuwkqPc",
    "outputId": "459e9384-fa33-4321-ee10-e42b5a841b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInputs Shape:\u001b[0m  torch.Size([2, 384])\n",
      "As we can see, our long context has been split into two input sequences.\n"
     ]
    }
   ],
   "source": [
    "## Extracting and modifying tokenized inputs\n",
    "# Extracting tokenized inputs\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "\n",
    "# Modifying tokenized inputs\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")    # This mapping isn't useful to us, so we remove it from our BatchEncoding object\n",
    "offsets = inputs.pop(\"offset_mapping\")      # We'll store our offsets for later\n",
    "\n",
    "# Looking at our tokenized inputs\n",
    "print(\"\\033[1mInputs Shape:\\033[0m \", inputs[\"input_ids\"].shape)\n",
    "print(\"As we can see, our long context has been split into two input sequences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5e7YEmnli2K"
   },
   "outputs": [],
   "source": [
    "## Calculating our model output logits\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz9q37t6CyvG"
   },
   "source": [
    "Next, we prepare our softmax mask. The only difference between the mask here and our previous mask (besides the fact that our mask now extends across two input sequences) is that we also want to mask any padding tokens added to the second tokenized input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omco6qoJlnd8"
   },
   "outputs": [],
   "source": [
    "## Creating our mask\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "mask[0] = False\n",
    "\n",
    "# We want to mask our [SEP] tokens, our question tokens, and our padding tokens (i.e., where inputs[\"attention_mask\"] == 0)\n",
    "mask = torch.logical_or(torch.tensor(mask).unsqueeze(0), (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C93z8b6FmINE"
   },
   "outputs": [],
   "source": [
    "## Calculating our probabilities\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndFKhsDXDiJn"
   },
   "source": [
    "As done before, we extract our model's predicted `(start_token_index, end_token_index)` pair, as well as its associated `score`. The only difference here is that we engage in this process for both of the tokenized input sequences (since they correspond to the same original text input sequence). While we don't do this here, one could easily generate a `final_candidate` by comparing the scores of the two entries in the `candidates` list generated below before selecting the candidate with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KB4gkO7JmRxw",
    "outputId": "04c3cee8-b153-4ff1-98f1-3a3a06c54907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOur Candidates:\u001b[0m  [(0, 18, 0.33867061138153076), (173, 184, 0.9714869856834412)]\n"
     ]
    }
   ],
   "source": [
    "## Within each input token sequence, calculate the model's predicted start_idx & end_idx, as well as the pair's associated score\n",
    "\n",
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "  scores = start_probs[:, None] * end_probs[None, :]\n",
    "  max_idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "  start_token_index = max_idx // scores.shape[0]\n",
    "  end_token_index = max_idx % scores.shape[1]\n",
    "  score = scores[start_token_index, end_token_index].data.item()\n",
    "  candidates.append((start_token_index, end_token_index, score))\n",
    "\n",
    "# Final results\n",
    "print(\"\\033[1mOur Candidates:\\033[0m \", candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlVUts9rEPPL"
   },
   "source": [
    "Finally, we format each of our candidates in line with the 🤗 Transformers Pipeline API output format. And now we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vpQpRMwnJh9",
    "outputId": "b932bb47-1d1a-44ad-fb17-c290eaa6ae0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\n🤗 Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867061138153076}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714869856834412}\n"
     ]
    }
   ],
   "source": [
    "## Format our candidates\n",
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset.squeeze(0)[start_token]\n",
    "    _, end_char = offset.squeeze(0)[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char.item(), \"end\": end_char.item(), \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0s7rMem2zCe_"
   },
   "source": [
    "## **6.4: An Example: 🤗 Accelerate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTcxQiOpPExE"
   },
   "source": [
    "$\\textbf{Section Introduction: }$\n",
    "\n",
    "**Sources:**\n",
    "1.   [Fine-tuning a masked language model](https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt)\n",
    "2.   [Token classification](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt)\n",
    "3.   [Supercharge your training loop with 🤗 Accelerate](https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt#supercharge-your-training-loop-with-accelerate)\n",
    "\n",
    "The process of fine-tuning a pretrained language model on in-domain data *before* fine-tuning the model for a task-specific head is usually called *domain adaptation*. Transfer learning (taking a pre-trained model & going straight into supervised fine-tuning for a task-specific head) will generally produce good results *if* the corpus utilized during pre-training doesn't diverge significantly from the corpus utilized during supervised fine-tuning. If there is a significant divergence, though, then without self-supervised fine-tuning on in-domain data first, your pretrained model will treat many domain-specific words in your corpus as rare tokens -- resulting in less than satisfactory performance. Domain adapation would be necessary if you took a generalist pre-trained model and are seeking to fine-tune its capabilites for specifically dealing with legal contracts, for example.\n",
    "\n",
    "In Section 6.4, we are going to domain adapt a generalist autoencoder Transformer model to a dataset containing movie reviews (& the relevant film review terminology that accompanies it). In particular, we'll be [autoassociative self-supervised](https://en.wikipedia.org/wiki/Self-supervised_learning#Autoassociative_self-supervised_learning) fine-tuning our model for masked language modeling (MLM) tasks (put another way, we're utilizing a MLM task here as a domain adaptation-based [pre-training technique](https://docs.pieces.app/build/glossary/terms/masked-language-modeling#:~:text=Masked%20Language%20Modeling%2C%20or%20%E2%80%9CMLM,tokens%20within%20an%20input%20sequence.)). In practice, this means our model inputs & model labels will be the same *except* for the fact that a random percentage of the tokens in our model inputs will be masked; thus, we have a clear loss metric which we can use to optimize our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGH4AwxEO62I"
   },
   "source": [
    "$\\textbf{Selecting a Pre-Trained Model Trained on MLM for Domain Adaptation: }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbAzBNjc89nU"
   },
   "source": [
    "The BERT and [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta) family of models are naturally the most commonly-utilized models for working with MLM tasks, since these models were pre-trained using MLM as the sole training objective (BERT was also trained on a next-sentence training objective). In this section, though, we'll be a model from the DistilBERT family of models, a faster and light weight varient of the BERT and RoBERTa families. The DistilBERT models have been pre-trained using a technique known as [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation), wherein a larger \"teacher model\" (e.g., BERT) is used to train a much smaller (in terms of number of paremeters) \"student model\". In particular, we'll be working with `distilbert-base-uncased`, the distilled version of the uncased variant of the BERT base model. By choosing a model from a family of light-weight models, we can significantly cut back on data, computational, and time requirements for our process of domain adapting an autoencoder Transformer model.\n",
    "\n",
    "With the above out of the way, let's load in our model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169,
     "referenced_widgets": [
      "7bb915d50c5e4f0da325cf557418e509",
      "8926554ca8464d12949150c7d4ad6ded",
      "65c2e11aa83249e6a65cb3697f44e462",
      "c633115f53ed4188ab12e2bcf22f403e",
      "76ab054392324b08b6b5c64d23579d92",
      "b3d5dc7ac4f243b1bf2b752e11d458ec",
      "14cf735ff1774b4aab4ec78e71486053",
      "2c225ba8ceba44578e833f40011f6052",
      "dd4df8d80e1a4b8e891ee0835bdf0941",
      "e7bcee156ebe492f803069135073ae33",
      "a2efb8d6aea84b2dbc61567ed3b4693a",
      "0d212e7f731a4f50a3bb68fa6331ef60",
      "6cc505fb9cd241568847073efa0185ab",
      "3df839ae7fee4967973a47cc41946e7f",
      "f56039474d6643439b79eb4ab94f37fb",
      "41aae30d10ff41eaa32393fb531b2ee2",
      "26a9be967df5499088252b7deb4f8d97",
      "4a19e90611554d5c85e91edb31e7de14",
      "e163a133e493499692d26e0ab3ab2318",
      "804dd56580394d0b819ba2ef6f4bae89",
      "bfed79430429431a91310cbd9df796c2",
      "a0353e798fd84483b9fed9cb462e2ac7",
      "7fb695d5bba44916ac605a1cb01e2352",
      "a777d553fd144477b24ec9540bf0acd6",
      "d43016d8718b4b459af68f373718a170",
      "96d737edb1f44b9cbd508a3de09f5f48",
      "5738b3b6e30d4f5087b65e18b0bb1896",
      "de3c2d9ab5c64402be02ccb68dc72151",
      "33a69f0fcaca466481b3337ebbc5ace6",
      "8103c5256a254ab69a5756fb18eaeb14",
      "36764ded34e54c8c99b786bc4a47390f",
      "1a0e22bc28a5415f917ef4ece23d7473",
      "9f14106a8592425090ac2372b79c1f14"
     ]
    },
    "id": "M4RUlfSHLBV7",
    "outputId": "12442340-9d36-4098-d9f6-36014a910951"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb915d50c5e4f0da325cf557418e509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d212e7f731a4f50a3bb68fa6331ef60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb695d5bba44916ac605a1cb01e2352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Loadng in model & tokenizer\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAjcqpHh_S-3"
   },
   "source": [
    "In order to conduct inference, our `distilbert-base-uncased` model expects one or more input sequences of text, each with one or more `[MASK]` tokens interspersed throughout the sequence. Below, we'll be asking our model to generate its top 5 predictions for what should replace the `[MASK]` token in the following sentence:\n",
    "\n",
    "<pre>\"This is a great [MASK].\"\n",
    "</pre>\n",
    "\n",
    "Naturally, our pre-trained DistilBERT model's predictions will depend on the corpus the model was trained on; since `distilbert-base-uncased` was trained on the [English Wikipedia](https://huggingface.co/datasets/legacy-datasets/wikipedia) and [BookCorpus](https://huggingface.co/datasets/bookcorpus/bookcorpus) datasets, we expect a set of rather generic responses. Let's test out this hypothesis below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gu3ZnoCLalV",
    "outputId": "ee4b2db0-4dbb-4db7-cfc4-4a577faa70a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great deal.\n",
      "This is a great success.\n",
      "This is a great adventure.\n",
      "This is a great idea.\n",
      "This is a great feat.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a great [MASK].\"\n",
    "\n",
    "# Extract our model logits\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]    # torch.where() produces the tuple (tensor([0]), tensor([5])); since the MASK token index is associated with tensor([5]), we index this entry with \"[1]\"\n",
    "mask_token_logits = token_logits.squeeze(0)[mask_token_index, :]      # squeezing out the batch dimension\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "'''\n",
    "  torch.topk() returns an object containing the top k values (in the \"values\" key) as well as the indices associated\n",
    "  with these values (in the \"indices\" key). In our case, we want the indices -- since we can decode these vocabulary indices\n",
    "  into the model's predicted text -- so we access the indices attribute. Additionally, indices is stored as a 2D tensor\n",
    "  of size [1,5], so we take [0] to essentially squeeze the first dimension before converting the final result into a list.\n",
    "'''\n",
    "\n",
    "# Print out our model's top 5 predictions\n",
    "for token in top_5_tokens:\n",
    "  print(f\"{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOgd5P3ICJGS"
   },
   "source": [
    "As we can see, our suspicions were correct. In order to domain-adapted fine-tune our DistilBERT model, we first need to load in and quickly preprocess a 🤗 Datasets dataset containing movie reviews. Fortunately, there's one famous dataset which will be just right for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "METS4VTQPIgh"
   },
   "source": [
    "$\\textbf{Selecting and Pre-Processing our Dataset for Domain Adaptation: }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6RGQbi1CtEp"
   },
   "source": [
    "In order to domain adapt our DistilBERT model, we'll be using the large corpus of text which can be found in the [Large Movie Review Dataset](https://huggingface.co/datasets/stanfordnlp/imdb) (IMDb, for short), a corpus of movie reviews often used to benchmark (binary) sentiment analysis models.\n",
    "\n",
    "The IMDb dataset contains three training splits: `train`, `test`, and `unsupervised`; we'll be using the `train` and `test` splits for the training and validation loops of our model, which each contain 25,000 entries. Each training split contains two columns: the `text` column which contains the actual movie reviews, and the `label` column which assigns each movie review a \"negative\" (0) or \"positive\" (1) rating.\n",
    "\n",
    "First, let's load in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "52ae1efe476b423d97b0a574ce218562",
      "25b0e7025f9444aea65ef390dcba7aa2",
      "a2f07ef9dcab4504ab0fc795d717862c",
      "05297369987b4ea591a2e2562a2df30e",
      "2fa8ccd237614379861154fc01f34b47",
      "02a854d841e04f2199f98efe37e1cf59",
      "689f6c49563e45de9811c90be5d0751f",
      "c9396475ce1f4adc919b104c9460cd5c",
      "6e7950f4b45c4f5d90feafd19ff01a98",
      "2b2b5f3d41fb4d1a8f8b88071b0d97e4",
      "a5eef3b0cf0f4826860a96f329273089",
      "0d351ba8240843199d37a3128f54f6f6",
      "3ae80da3205446bba93d59bed7cad5a8",
      "7ce213ae654b4a9cbe82b38ddf85c54d",
      "99bf812835d54972b05703982a732bd4",
      "86e1c4698d51425b8387a49cd8ca6998",
      "1e33a39312054748ba0d5e775da816bf",
      "152508d8e8294b959ee55f8d9ff97283",
      "40458d25adc74b09af3ea663d50be646",
      "078779f168884de1afef7e12bb1aaa04",
      "e487242029f842308015e2e09d1f7d39",
      "c36aedcff1724aa2afb68e2c0ed5de3c",
      "233448c09ca54601a3b3d5c1a92a0145",
      "8a94b98a478c4edeabb1dbe63f5b49b6",
      "04a61c458a0c4dff93ea33ea3d7a8805",
      "64f11b6f3be04f3eb35561b968ed0878",
      "bf169d31c22646bc921a7ed0a66e6451",
      "0fba709ffd624782bb565f63c5de151b",
      "55a174182f3e4785b078a038c4a0fbd9",
      "389181fe2ee74fca846834e1e6b4dca3",
      "a57400f09b1345d0b29d82d3e4608a60",
      "49b522f1d8c24567a9ec5c6016117cae",
      "c5c3e3b874894a179615c64bb14003a1",
      "e0979eeba02b4c5089b4242a268202f6",
      "dc0dc5f0647945db8f0da0f9448ea1e7",
      "2178e635696e4c8e98f2b8b399c75eb0",
      "4832da64e80248b7b5a7cce791b16a83",
      "30ff13d2e4e24253afeaee6d3e4882be",
      "969a9ed638a94e3fb90448f776fc13b4",
      "4a4a0299d461402e961d3391b074c934",
      "212ac8f6dbcd4c6e85bda71d94f1554f",
      "230660d97f0c4ab893062421762a5121",
      "b738b10ac9264e8d9d44c1916385aaec",
      "db11285b941849b0aa73cd93735e8c88",
      "29556c1246be429b84bfc2f944d4f6f0",
      "97786715b0ab420aa44f1fdda4c3f045",
      "355af90071914365a7f9449e7c49eeb9",
      "bcb361563da3477581123f3712c8386e",
      "2ac3d92d2797492998a17b7a6ee98a6b",
      "ade680ff95cb4c5bbfea0f2b92ab9f8a",
      "df1be52dadbc4865a70db5c80ab19cc8",
      "ee9163d962b948f6b8cca9bf38e47e04",
      "e297325c09ae46d9ac561ac8f111e7d2",
      "69690ff5bda54276b0b88eaa1989bd7a",
      "93e2d1b2f1a7456b9d3a4909705a641d",
      "574eff63fa4641c3b9e60a2243473880",
      "83e274310d4c4514899d04dd7eedc07e",
      "20311e8acdce4764a7d4b716e328b867",
      "aa488ed642d74f029a0fa40c915c2fe0",
      "635e308e49c24b299db6c81a1383ca4f",
      "12ff0ba3ca234aa4b1ffff6f295c1f0b",
      "85c40063d5f24713b591863edf1f1b8f",
      "9bfec57868ff4f40aff248253d26b958",
      "183c46e57d7c44c9af5dce1fcbf90f0a",
      "e7873ff7616d4f29af9c6f8c2a7427aa",
      "df0124f124894e11ac4901737b901b05",
      "1fa2255da0c24ae88ca57d264f7d0305",
      "6a5b406e87a84322ac5adc58ed4846a7",
      "a77b65e4e9b2487aab0e6beb7756d5a0",
      "c41f21a0cc634b7abba698274d7a2146",
      "b698ba384aa5450198784b79e69c7a48",
      "4bde9c4343a04c629e83b7a9b6663148",
      "5c9da4e6c79246ac8c311bdfa7dfca21",
      "d57961f9dbc44fc4aabed21edbb1f6ee",
      "d0c267ab47c04db18eab0476c9743d09",
      "a5c4d826479746b7ad475670a4ec0c60",
      "ff797f575c55461599083f9c94ad0b9a"
     ]
    },
    "id": "lEr01RLkPQ0L",
    "outputId": "10127c37-c9db-4a92-c705-3ff23db0c3e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ae1efe476b423d97b0a574ce218562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d351ba8240843199d37a3128f54f6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233448c09ca54601a3b3d5c1a92a0145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0979eeba02b4c5089b4242a268202f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29556c1246be429b84bfc2f944d4f6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574eff63fa4641c3b9e60a2243473880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa2255da0c24ae88ca57d264f7d0305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load in dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCahz6QcECPa"
   },
   "source": [
    "Now, let's take a random sample of 5 entries from the training split of our dataset and inspect each column entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5HJDALYpJZg",
    "outputId": "7a2b10ed-b401-49d5-9b00-41e024b9b5c2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mReview:\u001b[0m There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      "\u001b[1mLabel:\u001b[0m 1\n",
      "\n",
      "\u001b[1mReview:\u001b[0m This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      "\u001b[1mLabel:\u001b[0m 1\n",
      "\n",
      "\u001b[1mReview:\u001b[0m George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      "\u001b[1mLabel:\u001b[0m 0\n",
      "\n",
      "\u001b[1mReview:\u001b[0m In the process of trying to establish the audiences' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never withdrew from the Union and the Union Army was not an invading force. The Southerners fought for State's Rights: the right to own slaves, elect crooked legislatures and judges, and employ a political spoils system. There's nothing noble in that. The Missourians could have easily traveled east and joined the Confederate Army.<br /><br />It seems to me that the story has nothing to do with ambiguity. When Jake leaves the Bushwhackers, it's not because he saw error in his way, he certainly doesn't give himself over to the virtue of the cause of abolition.\n",
      "\u001b[1mLabel:\u001b[0m 1\n",
      "\n",
      "\u001b[1mReview:\u001b[0m Yeh, I know -- you're quivering with excitement. Well, *The Secret Lives of Dentists* will not upset your expectations: it's solidly made but essentially unimaginative, truthful but dull. It concerns the story of a married couple who happen to be dentists and who share the same practice (already a recipe for trouble: if it wasn't for our separate work-lives, we'd all ditch our spouses out of sheer irritation). Campbell Scott, whose mustache and demeanor don't recall Everyman so much as Ned Flanders from *The Simpsons*, is the mild-mannered, uber-Dad husband, and Hope Davis is the bored-stiff housewife who channels her frustrations into amateur opera. One night, as Dad & the daughters attend one of Davis' performances, he discovers that his wife is channeling her frustrations into more than just singing: he witnesses his wife kissing and flirting with the director of opera. (One nice touch: we never see the opera-director's face.) Dreading the prospect of instituting the proceedings for separation, divorce, and custody hearings -- profitable only to the lawyers -- Scott chooses to pretend ignorance of his wife's indiscretions.<br /><br />Already, the literate among you are starting to yawn: ho-hum, another story about the Pathetic, Sniveling Little Cuckold. But Rudolph, who took the story from a Jane Smiley novella, hopes that the wellworn-ness of the material will be compensated for by a series of flashy, postmodern touches. For instance, one of Scott's belligerent patients (Denis Leary, kept relatively -- and blessedly -- in check) will later become a sort of construction of the dentist's imagination, emerging as a Devil-on-the-shoulder advocate for the old-fashioned masculine virtues (\"Dump the b---h!\", etc.). When not egged-on by his imaginary new buddy, Scott is otherwise tormented by fantasies that include his wife engaged in a three-way with two of the male dental-assistants who work in their practice. It's not going too far to say that this movie is *Eyes Wide Shut* for Real People (or Grown-Ups, at least). Along those lines, Campbell Scott and Hope Davis are certainly recognizable human beings as compared to the glamourpuss pair of Cruise and Kidman. Further, the script for *Secret Lives* is clearly more relevant than Kubrick's. As proof, I offer the depiction of the dentists' children, particularly the youngest one who is about 3 or 4 years old, and whose main utterance is \"Dad! Dad! Dad! Dad! Dad! DAD!!!\" This is Family Life, all right, with all its charms.<br /><br />The movie would make an interesting double-bill with *Kramer vs. Kramer*, as well. One can easily trace the Feminization of the American Male from 1979 to 2003. In this movie, Dad is the housewife as in *Kramer*, but he is in no way flustered by the domestic role, unlike Dustin Hoffman, who was too manly to make toast. Here, Scott gets all the plumb chores, such as wiping up the children's vomit, cooking, cleaning, taking the kids to whatever inane after-school activity is on the docket. And all without complaint. (And without directorial commentary. It's just taken for granted.)<br /><br />The film has virtues, mostly having to do with verisimilitude. However, it's dragged down from greatness by its insistence on trendy distractions, which culminate in a long scene where a horrible five-day stomach flu makes the rounds in the household. We must endure pointless fantasy sequences, initiated by the imaginary ringleader Leary. Whose existence, by the way, is finally reminiscent of the Brad Pitt character in *Fight Club*. And this finally drives home the film's other big flaw: lack of originality. In this review, I realize it's been far too easy to reference many other films. Granted, this film is an improvement on most of them, but still. *The Secret Lives of Dentists* is worth seeing, but don't get too excited about it. (Not that you were all that excited, anyway. I guess.)\n",
      "\u001b[1mLabel:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# Take a random sample of our dataset from the training split to inspect further\n",
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(5))\n",
    "\n",
    "for row in sample:\n",
    "  print(f\"\\n\\033[1mReview:\\033[0m {row['text']}\")\n",
    "  print(f\"\\033[1mLabel:\\033[0m {row['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeUi0BV-ESF4"
   },
   "source": [
    "Having looked at a few of the entries in our dataset, we're now ready for dataset pre-processing.\n",
    "\n",
    "A common dataset pre-processing practice when preparing to train a model for a MLM objective is to concatenate all the examples together before splitting the whole corpus into chunks of equal size. Why take this approach? If we don't take this approach, there's a good chance that quite a few individual dataset entries will be truncated to the model's maximum token input sequence length, meaning we're *losing* information which might be useful for the language modeling task. As long as our model contains a `[SEP]` token for separating individual sentences and a `[CLS]` token for separating individual paragraphs/movie reviews (which is true in our case), then the model won't accidentally learn to generate incomplete sentences and/or movie reviews.\n",
    "\n",
    "Before we can do this, though, we need to tokenize our entire dataset. We'll make sure to set `truncation=False` (the default entry to the `truncation` keyword argument), as we don't want to lose any tokens from any of our dataset entries. Additionally, we'll remove the `text` and `label` columns from our dataset, as they are no longer necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "5a7980253ecd48399d1c91dec16a033a",
      "d264cadb2c344cf7b18e7913007ffa6f",
      "5bec6fe30a8f46c3adc172718b2f06bf",
      "4676533ffd90467f842bd5585974e97d",
      "69691aaa3cce455aa878aaf9bebd58e4",
      "fdc0c135f0914569be352d7b754d3165",
      "344b7056b144494d9f3b0372f879b159",
      "865033e8c65d4accb6bbbcfec89ecfad",
      "36e132a876ba4b1aa3f16f29fbf915f1",
      "0e79906ef28644358169a0233dc066c2",
      "677387456aec40debfb4d7be4bc89150",
      "401c24bf215b4debb667932dc01c0c6b",
      "e15b3b28abf0430699328164e09f3d95",
      "1811358daa3c4b1ea44265d16f4a7b47",
      "93762380d128406280c076509061eca7",
      "db16e9411b6341f89794c476433e5de3",
      "4204f661614f46cc8c1ebc3e4ee5b6ea",
      "a7c398c00fa94f64a24c4077639e7bfb",
      "28c92c4c69a44305ae28d2d844d22d46",
      "8f090fd9ff7c4a8dafd31700ce31e7ff",
      "2c054dedc6c049a5bf8f3191f83ff525",
      "cc6d82b9738d4ef1ad0c922fc2c181f5",
      "b044ea84145b4d60b9a5517d29fbf7b3",
      "29f6fd6594834637a40101fc8473d993",
      "695b2de9477d4b8887bbb155aba782e9",
      "fd04e9e124e849c2b1b0477d2f81e224",
      "aaab49e3ba9c453d8aad64fa73c6f4a2",
      "c27b9ce6b111491eac4dbcc7a4049a1a",
      "ac39af92f5f143ebbe2a05b7757c5a50",
      "2646245430bc42c9a85a5f02e2f08457",
      "19573e8304e44a2086c38f456793e813",
      "8bb45d7aa9aa49d08110d29adb67506d",
      "0fa2699fb38f4dcba0960ea1b53d7f0c"
     ]
    },
    "id": "KxYF0TYfpoOy",
    "outputId": "ff3e9621-edef-4e80-a34d-e29527523c5d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7980253ecd48399d1c91dec16a033a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401c24bf215b4debb667932dc01c0c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b044ea84145b4d60b9a5517d29fbf7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Tokenize our dataset\n",
    "# Tokenizing our dataset using Dataset.map() & batched = True.\n",
    "# We remove the columns \"text\" and \"label\" as we no longer need them post-tokenization.\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"]), batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "# Final result\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2R_HXgw8Gpku"
   },
   "source": [
    "As we can see above, our tokenized dataset contains the same `num_rows` within each training split, but with `'input_ids'` and `'attention_mask'` as our new & only dataset columns.\n",
    "\n",
    "We're now ready to group together and chunk our tokenized dataset. How big should these chunks be? First, let's get our model's maximum context size:\n",
    "\n",
    "<pre><code><b>Input: </b>\n",
    "tokenizer.model_max_length\n",
    "</code></pre>\n",
    "\n",
    "<pre><code><b>Output: </b>\n",
    "512\n",
    "</code></pre>\n",
    "\n",
    "This value is derived from the *tokenizer_config.json* file associated with our checkpoint. We'll choose to work with a chunk size which is 25% of the model's maximum context size: 128 tokens. Note that in real-world scenarios, small chunk sizes can be detrimental to overall model performance; if you have access to a more robust and powerful network of GPUs outside of Google Colab, it's recommended you consider a larger chunk size.\n",
    "\n",
    "With this in mind, let's chunk our text! Below, we write a `group_texts()` function which (given a batch of examples) concatenates all of the examples together before proceeding to chunk the tokenized text. Notice that our `group_texts()` function creates a new `labels` column which is (for now) a copy of the `input_ids` column; later, when we randomly mask tokens within our `input_ids` column, we'll use our `labels` column as the model's ground truth for each batch of examples. After writing the `group_texts()` function, we map it across the entire `tokenized_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "8a2f1e79d80742b498428b0e75c29593",
      "99fdb5fa0fce4e038973093c318d36e3",
      "ccaff96a294145768b3f2852a5f5a5f1",
      "fec3ea4d5143463b869127e5aad158cf",
      "cbee94616f4f46469fcb81e8c1053cfd",
      "6ae7e72a52724da6925fb4088b6240a2",
      "2fd52c68acee4e93a21944e609b83390",
      "b95f783a5b27416a947b09555dfabf1e",
      "05fa364a5b6f4a669b12bc36268ba184",
      "933e531367f24366940b5ff8a9b2065b",
      "d5515ce4d44d402b99abeb4f5ab5bcf0",
      "989c1d65b1cd4b2d8adf07aefe33c605",
      "c9b33077684a4e7a92792ef5005d8c11",
      "7a5fde5acbeb4189b29f77846ad1db0c",
      "db70f80e51384d0693585e8389783ada",
      "224cd733449d4137832ce897369fae0a",
      "3cd920717bb44fea83e181c7214f01f8",
      "d02784f33216477396925fe67193f3cf",
      "efbef54503d446acbf94b0013e583585",
      "e3a93ccff8794194a5c2c6d80056e824",
      "9cc299510d9e447b824f5ef6e6c74993",
      "20823f0305954a1b9d9f04c246192e2f",
      "3c60edfef89b47aea3e804f4bc421c72",
      "bb3b54dafad54ad9ad8d7d9bb83a08ba",
      "b52d9faf9d1e4666a5e79128c2182647",
      "8317adfbeb0840fbb00058eecab98568",
      "eaf46b77498249f9b2087a096489a78b",
      "71f4da197225426cab596ee80d568650",
      "e37de55e6dc5401ab46be3d0fa4e80a8",
      "4efad02ffc9d44569362501ffbcd70c5",
      "487f3371a33c477b8f988eb55cba5fd3",
      "2eafa6c92e864bbf8bd299cff48c062a",
      "ecd14faefbba496792ec9075bd365a2c"
     ]
    },
    "id": "y_JFEktssFwB",
    "outputId": "bea33a29-fb44-42cd-dab0-bdfdfe500afd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2f1e79d80742b498428b0e75c29593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989c1d65b1cd4b2d8adf07aefe33c605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c60edfef89b47aea3e804f4bc421c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Chunk our dataset\n",
    "# Instantiating our chunk size\n",
    "chunk_size = 128\n",
    "\n",
    "# Chunking function\n",
    "def group_texts(examples):\n",
    "  # Concatenating our examples. Each of the keys/columns in examples (\"input_ids\", \"attention_mask\") has as its value\n",
    "  # a list containing batch_size lists, where each of the batch_size lists contains the relevant row & column entry.\n",
    "  # Here, we use the sum() function to concatenate all batch_size lists into a single list for each key/column in our\n",
    "  # examples batch.\n",
    "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "\n",
    "  # Inside the len() term, we compute the total length of the concatenated examples. Since we just need the length\n",
    "  # of the \"input_ids\" column (or any other single column for that matter), we take '[0]', as in 'list(examples.keys())[0]'.\n",
    "\n",
    "  # Afterwards, we adjust total_length so that we drop the last chunk if it's smaller than chunk_size.\n",
    "  total_length = (len(concatenated_examples[list(examples.keys())[0]]) // chunk_size) * chunk_size\n",
    "\n",
    "  # Chunking our examples\n",
    "  result = {\n",
    "      k: [t[i:i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "      for k, t in concatenated_examples.items()\n",
    "  }\n",
    "\n",
    "  # Create a new \"labels\" column\n",
    "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "  # Function output\n",
    "  return result\n",
    "\n",
    "# Chunking our dataset using Dataset.map() & batched = True\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kCXpqv9LI7t"
   },
   "source": [
    "As we can see, chunking our texts has produced many more examples than our original 25,000 for the `train` and `test` splits -- an already significant corpus of text. Due to the prohibitively restrictive computational costs, we'll actually be downsampling the size of our dataset to only a couple thousand examples. In order to accomplish this, we'll be using the 🤗 Datasets [`Dataset.test_train_split()`](https://huggingface.co/docs/datasets/v3.0.0/en/package_reference/main_classes#datasets.Dataset.train_test_split) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EGteMum1SEI"
   },
   "outputs": [],
   "source": [
    "## For computational reasons, we downsample our original dataset\n",
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG7bN-tcgEUz"
   },
   "source": [
    "Working with our `downsampled_dataset`, we're now ready to begin randomly masking tokens within our training and validation Datasets. The general approach we'll take is as follows:\n",
    "\n",
    "For our `train` split, we'll be using a special [data collator](https://huggingface.co/docs/transformers/en/main_classes/data_collator), which we'll pass into our [DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), in order to randomly mask a pre-determined percentage of tokens across all input sequences in every batch. The data collator will randomly mask different tokens every batch *every time*, meaning that across epochs there is still some variation in terms of which tokens are and aren't masked; this is ideal for our model training, as we don't want the model learning to only expect certain tokens at certain places to be masked at all times.\n",
    "\n",
    "We'll be taking a different approach as it pertains to our `test` split; while the aforementioned randomness is ideal for model training, it isn't when it comes to consistently measuring performance *across epochs* on our validation/test set. For this reason, we'll apply masking *once* on the whole set before using the default default data collator in 🤗 Transformers (again as input to our DataLoader) to collect the batches during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PnHxAquCM2v"
   },
   "source": [
    "With this overall strategy in mind, let's load in our special data collator. While we won't be using this data collator in our `test` DataLoader, we'll still use this data collator in preparing our `test` split, so we decide to load it in here. In order to use `DataCollatorForLanguageModeling`, we need to pass in our `tokenizer` as well as a floating point number between 0 and 1 to `mlm_probability` to specify the percentage of tokens we'd like randomly masked within each batch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tWivc0EuLZm"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Instantiating our data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_JHT8a1BsBQ"
   },
   "source": [
    "As the final step of our dataset pre-processing, we'll be applying random masking *once* throughout the entire test split of our `downsampled_dataset`. First, we write the function `insert_random_mask()`, which takes as input a batch of examples. We then map this function to the test split of our `downsampled_dataset` before renaming the columns; we call this new dataset `eval_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuHw2CswvvQS"
   },
   "outputs": [],
   "source": [
    "## Apply random masking to our dataset's test split\n",
    "\n",
    "# Random masking function\n",
    "def insert_random_mask(batch):\n",
    "    # features is a list of dicts, where each dict is a single example from our batch. Read the comments in the\n",
    "    # group_texts() function (in particular, the comment *above* the concatenated_examples object) for an in-depth\n",
    "    # explanation of the structure of batch -- and hence why we have to manipulate it as follows in order to get our\n",
    "    # desired features object. We re-structure our batch object as follows as this is the particular format our\n",
    "    # data_collator object expects to be passed in.\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "\n",
    "    # The output of our data collator will be a dict with the keys being the columns of our dataset while the values\n",
    "    # are a single PyTorch tensor object wrapping a single list containing <batch size> entries, each a list\n",
    "    # containing the relevant column input sequence from the relevant row/dataset entry.\n",
    "\n",
    "    # Here, we apply the special DataCollatorForLanguageModeling in order to randomly apply masks throughout the\n",
    "    # \"input_ids\" column of each input sequence within each batch.\n",
    "    masked_inputs = data_collator(features)\n",
    "\n",
    "    # Return a new \"masked\" column for each column in the dataset. We don't want the values in our dictionary\n",
    "    # to be PyTorch tensors, so we convert them to NumPy arrays with the numpy() method.\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "# Applying our random masking function\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "\n",
    "# Renaming our columns back to their original names\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykCtYY_YvhtE"
   },
   "source": [
    "$\\textbf{Fine-tuning DistilBERT with 🤗 Accelerate: }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvrYOwcCEqyj"
   },
   "source": [
    "With dataset pre-processing out of the way, let's move on to fine-tuning our DistilBERT model.\n",
    "\n",
    "In order to do so, we'll be using a special 🤗 library known as 🤗 Accelerate. The 🤗 Accelerate library represents a simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, including with automatic mixed precision (including [fp8](https://en.wikipedia.org/wiki/Minifloat)) as well as easy-to-configure [FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://github.com/microsoft/DeepSpeed) support. 🤗 Accelerate was created for PyTorch users who like to write the training loop of PyTorch models but are reluctant to write and maintain the boilerplate code needed to use multi-GPUs/[TPU](https://lightning.ai/docs/pytorch/1.6.3/accelerators/tpu.html#:~:text=A%20TPU%20is%20a%20Tensor,hosts%20many%20TPUs%20on%20it.)/[fp16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format). Throughout this portion of Section 6.4, we'll interject to introduce any relevant 🤗 Accelerate objects, functions, and methods where necessary.\n",
    "\n",
    "Of particular note is the 🤗 Accelerate `notebook_launcher()`, which you can use in a notebook to launch a distributed training; just define your training loop in a function which you pass to the `training_function` keyword argument, and you're done. If you want to conduct your training in a Google Colab notebook with a TPU backend, you'll likely want to take this approach; reference [this notebook](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb) for more details on how to set up your training loop. While we won't be putting our entire training set-up in a training function in this section, you'll see that you can easily re-organize everything below into a single `training_function` if you so desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUorChgtYM_e"
   },
   "source": [
    "With everything mentioned above out of the way, let's get into preparing our DistilBERT model for fine-tuning. First, we build the appropriate `DataLoaders` from our datasets. Note that we use the `default_data_collator` for the evaluation dataset, as we've already applied the random `\"input_ids\"` masking we desired in the section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzKVWG-h1ZEC"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "## Loading in our dataloaders\n",
    "# Establishing the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Loading in our training and evaluation dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,       # Commonly applied to training dataset DataLoaders\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,     # Notice we use DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator       # We've already applied the random masking to the evaluation dataset, so we don't want to use DataCollatorForLanguageModeling, just the default data collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhDXEOAHY28n"
   },
   "source": [
    "Now, we'll load in our optimizer from the `torch.optim` library. `AdamW` is similar to the classic `Adam` optimizer, but with a modification in how weight decay is applied. You can read about it [here](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpqalkyoCqSh"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Establishing our optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlHzFFr9B1an"
   },
   "source": [
    "Having prepared our model, optimizer, and relevant dataloaders, we're now ready to instantiate our `Accelerator` object and pass the aforementioned modules to the `Accelerator.prepare()` method. The `Accelerator` object is what looks at your environment & uses that information to initialize the proper distributed setup. Passing your model, optimizer, and dataloaders to `accelerator.prepare()` wraps those objects in the proper container to make sure your distributed training works as intended.\n",
    "\n",
    "---\n",
    "\n",
    "*Note: An additional benefit of 🤗 Accelerate not shown below is that the library handles the device placement for you. As such, you don't need to worry about pushing the model and other relevant modules to your GPU cluster (or whatever distributed training set-up you've configured). If you want to specify which devices your model, batches, etc., are being pushed to, you can do so by accessing `accelerator.device`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tB3p1zLCuga"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Instantiating our Accelerator() object\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Preparing our model, optimizer, and the relevant dataloaders for HF Accelerate\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwSdW-qYEOT1"
   },
   "source": [
    "We're now ready to prepare our LR scheduler. We'll use the `transformers.get_scheduler` API in order to load in one of the many LR schedulers avaiable in the 🤗 Transformers library. You can read about `transformers.get_scheduler` [here](https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.get_scheduler).\n",
    "\n",
    "Note that you should wait to set up your LR scheduler until *after* you've prepared your dataloaders. 🤗 Accelerate will [add samples](https://github.com/huggingface/accelerate/issues/226) to make sure each process gets the same batch size (otherwise, you will get weird errors) (see `sgugger`'s first comment); the library will do this by essentially looping back to the beginning of your dataset. This whole process modifies the length of your dataloaders, thus modifying the final integer value passed to `transformers.get_scheduler`'s `num_training_steps` keyword argument.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i4v0wAbC3rn"
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "## Loading in our learning rate scheduler\n",
    "# Establishing some important values\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)      # Note: We can only take the len() of train_dataloader *after* preparing it for HF Accelerate\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# Loading in our LR scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFSL-_XaGaJ_"
   },
   "source": [
    "We're now finally ready to write our training and validation/evaluation loops in order to fine-tune our DistilBERT model. If you're completely unfamiliar with how PyTorch training & validation loops are generally structured, then I'd recommend checking out [this section](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation) in the official PyTorch tutorial. Otherwise, I've left plenty of comments below explaining exactly what we're doing at each step of the training process.\n",
    "\n",
    "Before letting you read onwards, I wanted to touch briefly on a metric we measure below: perplexity. I'll quote a description of perplexity from the relevant [🤗 tutorial](https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt#perplexity-for-language-models):\n",
    "\n",
    ">\"Unlike other tasks like text classification or question answering where we’re given a labeled corpus to train on, with language modeling we don’t have any explicit labels. So how do we determine what makes a good language model? Like with the autocorrect feature in your phone, a good language model is one that assigns high probabilities to sentences that are grammatically correct, and low probabilities to nonsense sentences... Assuming our test set consists mostly of sentences that are grammatically correct, then one way to measure the quality of our language model is to calculate the probabilities it assigns to the next word in all the sentences of the test set. High probabilities indicates that the model is not “surprised” or “perplexed” by the unseen examples, and suggests it has learned the basic patterns of grammar in the language. There are various mathematical definitions of perplexity, but the one we’ll use defines it as the exponential of the cross-entropy loss. [🤗 Transformers models automatically calculate this loss for us; we can access it via the model's `loss` attribute].\"\n",
    "\n",
    "The above paragraph sums up perplexity pretty well. If you're interested in reading more about it, then I'd recommend reading Section 6.1.5 of Kevin Murphy's, [\"Probabilistic Machine Learning: An Introduction\"](https://probml.github.io/pml-book/book1.html). Note that based upon the way we've defined perplexity, we want our perplexity to *decrease* as we continue to train our model, not increase.\n",
    "\n",
    "With that being said, let's get to training!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2EjGAx9FMBH"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "### Our training and evaluation loop\n",
    "# Initializing our progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "  ## Training\n",
    "  model.train()   # Putting model in training mode\n",
    "\n",
    "  # Iterating through each batch in our train_dataloader\n",
    "  for batch in train_dataloader:\n",
    "    outputs = model(**batch)   # Gathering outputs\n",
    "    loss = outputs.loss     # Gathering the model's loss (HF Transformers-backed models automatically compute & store the model's loss in the loss attribute)\n",
    "    accelerator.backward(loss)    # Backpropagating the loss via HF Accelerate. If we weren't running a distributed training set-up, we could simply call 'loss.backward()'.\n",
    "\n",
    "    optimizer.step()    # Taking an optimizer step\n",
    "    lr_scheduler.step()     # Taking a LR scheduler step\n",
    "    optimizer.zero_grad()     # Zeroing out the gradients stored within our optimizer\n",
    "    progress_bar.update(1)      # Updating our progress bar\n",
    "\n",
    "  ## Evaluation\n",
    "  model.eval()    # Setting our model to evaluation/test mode\n",
    "  losses = []      # We'll append all of our evaluation losses to this list\n",
    "\n",
    "  # Iterating through each batch in our eval_dataloader\n",
    "  for batch in eval_dataloader:\n",
    "\n",
    "    # We don't need to compute gradients for the outputs for our evaluation loop\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Gather the values in tensors across all processes and concatenate them on the first dimension.\n",
    "\n",
    "    # The first dimension of the result is the number of processes (num_processes) multiplied by the first dimension\n",
    "    # of the input tensors (i.e., the batch dimension). Thus, we apply 'repeat(batch_size)' to each loss\n",
    "    # we compute so that each entry in our dataset has an associated loss.\n",
    "    losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "  # torch.cat() here will convert our list of lists of losses into a single list containing all losses\n",
    "  losses = torch.cat(losses)\n",
    "\n",
    "  # HF Accelerate will add samples to make sure each process gets the same batch size (as discussed above).\n",
    "  # As such, here we truncate our losses to the length of our evaluation dataset.\n",
    "  losses = losses[: len(eval_dataset)]\n",
    "\n",
    "  # Calculating perplexity\n",
    "  try:\n",
    "      perplexity = math.exp(torch.mean(losses))\n",
    "  except OverflowError:\n",
    "      perplexity = float(\"inf\")\n",
    "\n",
    "  # Print-out after each epoch\n",
    "  print(\"\\033[1mEpoch:\\033[0m \", epoch, \"\\n\\033[1mPerplexity:\\033[0m \", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZLFo5P_JxZa"
   },
   "source": [
    "Great! Assuming you've trained your model using some configuration of the above code (again, if you want to train your model directly in a Google Colab notebook with a TPU backend, I'd recommend checking out the approach discussed [here](https://colab.research.google.com/drive/1eJE1aebfF60y6paWPAzttN3wcpvdlsBM?authuser=5#scrollTo=PvrYOwcCEqyj&line=5&uniqifier=1)), you have now successfully domain-adapted fine-tuned a DistilBERT model utilizing a MLM objective!\n",
    "\n",
    "That wraps everything up for Section 6. We are now ready to move on to our final section: Section 7."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
